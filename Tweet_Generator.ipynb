{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Generator.ipynb",
      "provenance": [],
      "mount_file_id": "12MHNh7F8wxZkVWHEuDkvARP1SNU4bpcX",
      "authorship_tag": "ABX9TyOPQV4sz0XmrAgLHvSQolC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Tweet_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMqVaBwSPA",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of **Generating sentences from a continuous space paper**\n",
        "Here's a link to the [paper](https://arxiv.org/pdf/1511.06349v4.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCPxyfX9r53",
        "colab_type": "text"
      },
      "source": [
        "> ### The goal \n",
        " What I want to explore here is the expression of sentiment in generative models. <br>\n",
        " The dataset consists of two different samples of tweets, one with positive sentiment and one with negative sentiment. <br>\n",
        " The goal is to train two generators on the two sets separetly and analyse the qualitative differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwkLCb83wngS",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSehtnM7dGh",
        "colab_type": "code",
        "outputId": "94b4b155-d2b3-40e9-80c0-c4f3a3d01abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3kXO6vvwm9f",
        "colab_type": "code",
        "outputId": "50067bc1-3c70-4df0-ddfe-eb4eaa930d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RnhXHWxKGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxiSqdJQxPvj",
        "colab_type": "text"
      },
      "source": [
        "An example of the raw data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR4lq8bJxKNx",
        "colab_type": "code",
        "outputId": "dac2019d-4226-4171-bb67-cbf66c4e6c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW_iOL4O78mD",
        "colab_type": "code",
        "outputId": "f86ab784-1e6b-4e27-8d2e-8c69fcaf7798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzumvsqowr8M",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di8j0afxw3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJp435CzVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from collections import Counter\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StWI7_EYwbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Transforms the specified files in tokens using the Twitter tokenizer.\n",
        "    @:params: str\n",
        "        Input text to tokenize\n",
        "    @:return: list(str)\n",
        "        Returns the tokens as a list of strings.\n",
        "    \"\"\"\n",
        "    tokenizer = TweetTokenizer()\n",
        "    # tokenizing the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [w.lower() for w in tokens]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2SswBFb7ut3",
        "colab_type": "text"
      },
      "source": [
        "Now we build a vocabulary with the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Gwr0Ex7zeP",
        "colab_type": "code",
        "outputId": "930681f5-aa1a-44b5-8598-ec0383c07d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "frequency_treshold = 100000*0.0001 # 0.1% of the tweets should contain the \"frequent words\"\n",
        "frequency_treshold"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-eecHHzzQJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(file_name, output_file_name):\n",
        "  \"\"\"\n",
        "  Builds the vocabulary for the specified file.\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  print(\"Reading \",file_name)\n",
        "  raw = open(file_name,  \"r\").read()\n",
        "  more_words = tokenize_text(raw)\n",
        "  words.extend(more_words)\n",
        "  # counting the words\n",
        "  counter = Counter(words)\n",
        "  words_count = dict(counter)\n",
        "  # filtering \n",
        "  filtered_words = [k for k, v in words_count.items() if v >= frequency_treshold]\n",
        "  # building voabulary \n",
        "  # index starts at 1 so that the 0 can be left for the empty spaces\n",
        "  vocab = {k:i+1 for i,k in enumerate(filtered_words)}\n",
        "  # saving the vocabulary \n",
        "  with open(output_file_name, 'wb') as f: \n",
        "      pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2XSsFiVECtj",
        "colab_type": "code",
        "outputId": "29c0b20d-3389-4cad-fb7d-0e31a3266259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_pos = build_vocabulary(positive_location, \"vocab_pos.pkl\")\n",
        "vocab_neg = build_vocabulary(negative_location, \"vocab_neg.pkl\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading  twitter-datasets/train_pos.txt\n",
            "Reading  twitter-datasets/train_neg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bpgXYLg9IkZ",
        "colab_type": "code",
        "outputId": "4120226e-a882-4d9d-83fc-e328c35cc3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_pos.keys())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8Sh0RXESLv",
        "colab_type": "code",
        "outputId": "423ccd3c-bab5-4043-955d-29a65e8493cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_neg.keys())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgAWLg6-4o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_sentence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  Filters the given sentence with the given vocabulary. \n",
        "    The words that are not in the vocabulary will be filtered out \n",
        "    of the sentence. \n",
        "  \"\"\"\n",
        "  return [word for word in sentence if word in vocab.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kRaA30GKkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_sequence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  (Filtering included)\n",
        "  Transforms the given sentence into a sequence of ints, \n",
        "  corresponding to the index of the word.\n",
        "  \"\"\"\n",
        "  filtered_sentence = filter_sentence(sentence, vocab)\n",
        "  return [vocab[word] for word in filtered_sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc5EBhCqwtuW",
        "colab_type": "text"
      },
      "source": [
        "# Sentences handler\n",
        "During training we want to be able to load the tweets from the file sequentially for memory efficiency. <br>\n",
        "Here I implement the helper functions that will do that\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKxJMIYlio29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_new_chunk(start, chunksize, file):\n",
        "  \"\"\"\n",
        "  Loads -chunksize- tweets from the specified file, starting from \n",
        "  -start- tweet (excluded). \n",
        "  \"\"\"\n",
        "  f=open(file)\n",
        "  lines=f.readlines()\n",
        "  selected = lines[start:start+chunksize]\n",
        "  return selected "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyDeGyj-q8D",
        "colab_type": "code",
        "outputId": "51150bf9-d1d5-4a63-8488-7dac12640ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "load_new_chunk(99997, 5, positive_location)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<user> <user> um gord ... i just read your profile . i'm not sure i can have lunch with a riders fan\\n\",\n",
              " \"<user> i'm so excited for tomorrow ! look out for two leprechauns ! xx\\n\",\n",
              " 'i always wondered what the job application is like at hooters .. do they just give you a bra and say , \" here fill this out \" .. ? \"\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dL5Zn1yG2Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5dsfNPguQfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_size = int(VALIDATION_SPLIT*100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9IcOlwA-3YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_generator(file, chunksize, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  The output of the generator must be a tuple (inputs, targets)\n",
        "  This tuple (a single output of the generator) makes a single batch\n",
        "  Different batches may have different sizes. For example, \n",
        "  the last batch of the epoch is commonly smaller than the others, \n",
        "  if the size of the dataset is not divisible by the batch size. \n",
        "  \"\"\"\n",
        "  start = 0 \n",
        "  while start < 100000 - validation_size:\n",
        "    sentences = load_new_chunk(start,chunksize,file)\n",
        "    sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "    start +=chunksize\n",
        "    yield padded_sequences, padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16vV70rNuPo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_validation_data(file, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  start = 100000 - validation_size\n",
        "  sentences = load_new_chunk(start,validation_size,file)\n",
        "  sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "  return padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUuiwa_eul94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_pos = get_validation_data(positive_location,MAX_SEQUENCE_LENGTH,vocab_pos)\n",
        "validation_neg = get_validation_data(negative_location,MAX_SEQUENCE_LENGTH,vocab_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NIhn9VUwzmP",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHrAfaCJmA",
        "colab_type": "text"
      },
      "source": [
        "To embed the words we are going to use a pre-trained GloVe model as a *prior* on the embedding that we'll be inserted as a first layer to the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTmWWpn7w1Jc",
        "colab_type": "code",
        "outputId": "2c5439ef-ce2a-4349-c9e3-5b756b688b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-16 15:15:47--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-16 15:15:47--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-16 15:15:48--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.91MB/s    in 6m 29s  \n",
            "\n",
            "2020-04-16 15:22:18 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgHIKzsEbWG",
        "colab_type": "code",
        "outputId": "0063ccfa-ffce-4554-9c07-d3bf01afc279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ECvY5mFBSD",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the *200d* embedding file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxnFtKDoFAtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_location = 'glove.6B.200d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txw8HQPfFO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOa0_99hFAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove_location, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S10AVtAqAs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_emb_matrix(glove, vocab):\n",
        "  \"\"\"\n",
        "  Build the pre-trained GloVe embedding on the given vocabulary. \n",
        "  The GloVe embedding should be presented as a vocabulary. \n",
        "  \"\"\"\n",
        "  print(\"Building the GloVe embedding.\")\n",
        "  N = len(vocab.keys()) + 1 # number of words in the vocabulary \n",
        "  D = 200 # embedding dimension\n",
        "  emb_matrix = np.zeros((N,D))\n",
        "  for idx, word in enumerate(vocab.keys()):\n",
        "    embedding_vector = glove.get(word)\n",
        "    if embedding_vector is not None: emb_matrix[idx+1] = embedding_vector\n",
        "    else: emb_matrix[idx+1] = embeddings_index.get('unk')\n",
        "  return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8_oEHkGjm8",
        "colab_type": "code",
        "outputId": "4457ae04-c70a-4ff5-a9a3-9e9031dce25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "emb_matrix_pos = build_emb_matrix(embeddings_index, vocab_pos)\n",
        "emb_matrix_neg = build_emb_matrix(embeddings_index, vocab_neg)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the GloVe embedding.\n",
            "Building the GloVe embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idGVoWe7G64B",
        "colab_type": "code",
        "outputId": "4ad625a5-d893-4b13-ddfa-3ed695ad1f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb_matrix_pos.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5712, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSIyMZstbdiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez(\"emb_matrix_pos\",emb_matrix_pos)\n",
        "np.savez(\"emb_matrix_neg\",emb_matrix_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXDvDo5I7Jab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_matrix_pos = np.load(\"emb_matrix_pos.npz\")[\"arr_0\"]\n",
        "emb_matrix_neg = np.load(\"emb_matrix_neg.npz\")[\"arr_0\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlfhxfAw19g",
        "colab_type": "text"
      },
      "source": [
        "# VAE model\n",
        "To make my life simpler I chose to use Keras to build the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv37tgi0HOYj",
        "colab_type": "code",
        "outputId": "e87353b5-bdfe-4925-86e5-f1a03c8821ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.layers.advanced_activations import ELU"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2vCT4_HCXN",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameters first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-eFaAIHx_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LATENT_DIM = 64\n",
        "HIDDEN_DIM = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYMmGPNd7a-i",
        "colab_type": "text"
      },
      "source": [
        "#### text VAE class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcN1Jf7v-px",
        "colab_type": "text"
      },
      "source": [
        "In the following cells we'll build a **Model** class and its **Layers**' classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGCQIt3u6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlzVlTJ_vi4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  \"\"\"\n",
        "  Sampling layer. \n",
        "  The sampling will be from the posterior on the latent vector, \n",
        "  whose prior is a standard Gaussian. \n",
        "  \"\"\"\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    # batch and latent dimensionality\n",
        "    batch_size = tf.shape(z_mean)[0] \n",
        "    latent_dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch_size, latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zREAGsAxkTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(layers.Layer):\n",
        "  \"\"\"\n",
        "  Embedding layer. \n",
        "  Responsible for training the embedding. \n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               emb_matrix= None, # pre-trained embeddings\n",
        "               name='embedding',\n",
        "               to_train= True, # wether to train the embeddings\n",
        "               **kwargs):\n",
        "    super(Embedding, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.embedding = layers.Embedding(N, D, weights=[emb_matrix],\n",
        "                                      input_length=seq_length, trainable=to_train)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyZfu7BwlTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Encoder layer. \n",
        "  This layer parametrizes the mapping from the input  \n",
        "  to a latent space distribution. \n",
        "  Concretely it will map sentences to the triple (z_mean, z_log_var, z).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               latent_dim=LATENT_DIM,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               name='encoder',\n",
        "               dropout_rate=0.2,\n",
        "               activation_fun='elu',\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.recurrent_layer = layers.Bidirectional(layers.LSTM(hidden_dim, \n",
        "                            return_sequences=False, recurrent_dropout=dropout_rate,\n",
        "                            input_shape=(None,seq_length,D)), merge_mode='concat')\n",
        "    self.dense_proj = layers.Dense(hidden_dim, activation=activation_fun)\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Input must be of shape (None, max_len)\n",
        "    h = self.recurrent_layer(inputs)\n",
        "    x = self.dense_proj(h)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8io-a-Xzy6S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Decoder layer. \n",
        "  This layer parametrizes the mapping from the latent space to the \n",
        "  output dimension. \n",
        "  Concretely it will map a z to a readable sentence.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               name='decoder',\n",
        "               dropout_rate=0.2,\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.latent2hidden = layers.Dense(hidden_dim,activation='linear',\n",
        "                                      input_shape=(None,latent_dim))\n",
        "    self.recurrent_layer = layers.LSTM(hidden_dim, return_sequences=True, \n",
        "                                       recurrent_dropout=dropout_rate, \n",
        "                                       input_shape=(None,seq_length,D))\n",
        "    self.mean = layers.TimeDistributed(layers.Dense(N, activation='linear'))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs[0]\n",
        "    z = inputs[1]\n",
        "    mask = inputs[2]\n",
        "    hidden = self.latent2hidden(z)\n",
        "    h_decoded = self.recurrent_layer(x, initial_state=[hidden,hidden], mask=mask)\n",
        "    x_decoded_mean = self.mean(h_decoded)\n",
        "    return h_decoded, x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPooKv_05bSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxln6VbuudUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textVAE(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  Model class for VAE.\n",
        "  Combines the embedding, encoder and decoder into an \n",
        "  end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               unk_embedding, # embedding for unknown word\n",
        "               emb_matrix=None, # pre-trained embeddings - if None, they will be trained\n",
        "               train_emb = True,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               word_dropout_rate=0.75,\n",
        "               dropout_rate=0.2,\n",
        "               name='textVAE',\n",
        "               **kwargs):\n",
        "    super(textVAE, self).__init__(name=name, **kwargs)\n",
        "    self.N, self.D = vocab_dim\n",
        "    self.unk_embedding = unk_embedding\n",
        "    self.seq_length = seq_length\n",
        "    self.word_dropout_rate = word_dropout_rate\n",
        "    self.embedding = Embedding(vocab_dim=vocab_dim,\n",
        "                               seq_length=seq_length,\n",
        "                               emb_matrix=emb_matrix,\n",
        "                               to_train=train_emb)\n",
        "    self.encoder = Encoder(vocab_dim=vocab_dim,\n",
        "                           seq_length=seq_length,\n",
        "                           latent_dim=latent_dim,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           dropout_rate=dropout_rate)\n",
        "    self.decoder = Decoder(vocab_dim=vocab_dim, \n",
        "                           seq_length=seq_length,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           latent_dim=latent_dim,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    \n",
        "  def get_mask(self, input_sequence):\n",
        "    \"\"\"\n",
        "    Automatically drops self.word_dropout_rate% of words \n",
        "    in the input. (To call before forwarding to the decoder.)\n",
        "    \"\"\"\n",
        "    #input shape = batch size x seq length x emb dimension\n",
        "    shape = K.shape(input_sequence)\n",
        "    prob = tf.random.uniform(shape,0,1,tf.float32)\n",
        "    mask = prob < self.word_dropout_rate \n",
        "    return mask\n",
        "\n",
        "  def compute_loss(self, z_mean, z_log_var, z, \n",
        "                   x, x_decoded_mean, target_weights):\n",
        "    \"\"\"\n",
        "    Computes the VAE loss. \n",
        "    \"\"\"\n",
        "    labels = tf.cast(x, tf.int32)\n",
        "    xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, \n",
        "                        labels, weights=target_weights, \n",
        "                        average_across_timesteps=False,\n",
        "                        average_across_batch=False), axis=-1)\n",
        "    kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    z_mean, z_log_var, z = self.encoder(x)\n",
        "    # Word dropout for the decoder, as described in the paper\n",
        "    mask = self.get_mask(inputs)\n",
        "    h_decoded, x_decoded_mean = self.decoder((x,z,mask))\n",
        "    # Add KL divergence regularization loss.\n",
        "    target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "    loss = self.compute_loss(z_mean, z_log_var, z, \n",
        "                             inputs, x_decoded_mean, target_weights) \n",
        "    self.add_loss(loss)\n",
        "    return h_decoded,x_decoded_mean  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAR_g9F7a5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGFY1dn7Den",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_model(emb_matrix, name):\n",
        "  \"\"\"\n",
        "  Constructing the text VAE model.\n",
        "  \"\"\"\n",
        "  N,D = emb_matrix.shape\n",
        "  vae = textVAE(vocab_dim=(N,D),\n",
        "                seq_length=MAX_SEQUENCE_LENGTH,\n",
        "                unk_embedding=embeddings_index.get('unk'), # from GloVe embedding index\n",
        "                emb_matrix=emb_matrix,\n",
        "                name=name)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  vae.compile(optimizer=optimizer)\n",
        "  return vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgNBXBecIw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusLhCTm8Zlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_pos = construct_model(emb_matrix_pos, name=\"pos_vae\")\n",
        "generator_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7JUB_cMj64-",
        "colab_type": "text"
      },
      "source": [
        "### text VAE with functional APIs\n",
        "Here I implement the same model as above, but using the Keras functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "commf2LjknYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xJaL6bkEVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(vocab, emb_matrix, N, D):\n",
        "  x = Input(batch_shape=(None, MAX_SEQUENCE_LENGTH))\n",
        "  x_embedded = Embedding(N, D, weights=[emb_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)(x)\n",
        "  return x, x_embedded\n",
        "\n",
        "\n",
        "def sample(z_mean, z_log_var):\n",
        "  epsilon = K.random_normal(shape=(BATCH_SIZE, LATENT_DIM))\n",
        "  return z_mean + tf.exp(0.5 * z_log_var) * epsilon  \n",
        "\n",
        "def get_encoding(x_embedded, dropout_rate):\n",
        "  hidden = Bidirectional(LSTM(HIDDEN_DIM, \n",
        "                              return_sequences=False, \n",
        "                              recurrent_dropout=dropout_rate),\n",
        "                          merge_mode='concat')(x_embedded)\n",
        "  hidden_dropped = Dropout(dropout_rate)(hidden) # dropping some of the input and output units, as suggested in the paper\n",
        "  hidden = Dense(HIDDEN_DIM, activation='relu')(hidden_dropped)\n",
        "  hidden_dropped = Dropout(dropout_rate)(hidden)\n",
        "  z_mean = Dense(LATENT_DIM)(hidden_dropped)\n",
        "  z_log_var = Dense(LATENT_DIM)(hidden_dropped)\n",
        "  z = sample(z_mean, z_log_var)\n",
        "  return z_mean, z_log_var, z, hidden\n",
        "\n",
        "def get_decoding(z, hidden, dropout_rate, N):\n",
        "  repeated_context = RepeatVector(MAX_SEQUENCE_LENGTH)\n",
        "  sequence_decoded = LSTM(HIDDEN_DIM, \n",
        "                          return_sequences=True, \n",
        "                          recurrent_dropout=dropout_rate)(repeated_context(z), \n",
        "                                                          initial_state=[hidden,hidden])\n",
        "  sequence_decoded_mean = TimeDistributed(Dense(N, activation='linear'))(sequence_decoded)\n",
        "  return sequence_decoded, sequence_decoded_mean\n",
        "\n",
        "\n",
        "class VAEloss(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(VAEloss, self).__init__(**kwargs)\n",
        "\n",
        "    def get_vae_loss(self, z_mean, z_log_var, z, x, inputs, decoded_mean):\n",
        "      labels = tf.cast(inputs, tf.int32)\n",
        "      target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "      xent_loss = K.sum(tfa.seq2seq.sequence_loss(decoded_mean, \n",
        "                                                  labels,\n",
        "                                                  weights=target_weights,\n",
        "                                                  average_across_timesteps=False,\n",
        "                                                  average_across_batch=False), axis=-1)\n",
        "      kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "      return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      # unpack inputs\n",
        "      z_mean = inputs[0]\n",
        "      z_log_var = inputs[1]\n",
        "      z = inputs[2]\n",
        "      x = inputs[3]\n",
        "      inputs_ = inputs[4]\n",
        "      decoded_mean = inputs[5]\n",
        "      # add loss \n",
        "      loss = self.get_vae_loss(z_mean, z_log_var, z, x, inputs_, decoded_mean)\n",
        "      self.add_loss(loss, inputs=inputs)\n",
        "      \n",
        "      return K.ones_like(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1n4ugoard99",
        "colab_type": "text"
      },
      "source": [
        "#### Positive tweets model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVtJzlmvkEka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "09554aa3-742c-474a-bb26-6eba340b9c75"
      },
      "source": [
        "# Nota bene: nothing is really computed here, we are just connecting the separate \n",
        "# computational graphs that we built above.\n",
        "N,D = emb_matrix_pos.shape\n",
        "inputs, embedding = get_embedding(vocab_pos, emb_matrix_pos, N, D)\n",
        "z_mean, z_log_var, latent, encoding = get_encoding(embedding, dropout_rate=0.2)\n",
        "decoding, decoding_mean = get_decoding(latent, encoding, dropout_rate=0.2, N=N)\n",
        "# Now we inster the loss into the graph \n",
        "loss_layer = VAEloss()([z_mean, z_log_var, latent, embedding, inputs, decoding_mean])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9c0GnFvkEt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "e523667d-66a0-48ab-8d7e-77f53b8418e3"
      },
      "source": [
        "VAE_pos = Model(inputs=inputs, outputs=[loss_layer], name=\"positiveVAE\") \n",
        "VAE_pos.compile(optimizer='adam')\n",
        "VAE_pos.summary()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"positiveVAE\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 100, 200)     1142400     input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_9 (Bidirectional) (None, 192)          228096      embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 192)          0           bidirectional_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 96)           18528       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 96)           0           dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 64)           6208        dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_26 (TensorFlowO [(None, 64)]         0           dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Exp_13 (TensorFlowO [(None, 64)]         0           tf_op_layer_Mul_26[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 64)           6208        dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_27 (TensorFlowO [(1000, 64)]         0           tf_op_layer_Exp_13[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_14 (TensorFlo [(1000, 64)]         0           dense_37[0][0]                   \n",
            "                                                                 tf_op_layer_Mul_27[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_7 (RepeatVector)  (1000, 100, 64)      0           tf_op_layer_AddV2_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_19 (LSTM)                  (1000, 100, 96)      61824       repeat_vector_7[0][0]            \n",
            "                                                                 dense_36[0][0]                   \n",
            "                                                                 dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (1000, 100, 5712)    554064      lstm_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "va_eloss_3 (VAEloss)            (None, 100, 200)     0           dense_37[0][0]                   \n",
            "                                                                 dense_38[0][0]                   \n",
            "                                                                 tf_op_layer_AddV2_14[0][0]       \n",
            "                                                                 embedding_9[0][0]                \n",
            "                                                                 input_16[0][0]                   \n",
            "                                                                 time_distributed_7[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 2,017,328\n",
            "Trainable params: 2,017,328\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyibkBEerhA0",
        "colab_type": "text"
      },
      "source": [
        "#### Negative tweets model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppB7jUTBrkmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computational graphs that we built above.\n",
        "N,D = emb_matrix_pos.shape\n",
        "inputs, embedding = get_embedding(vocab_pos, emb_matrix_pos, N, D)\n",
        "z_mean, z_log_var, latent, encoding = get_encoding(embedding, dropout_rate=0.2)\n",
        "decoding, decoding_mean = get_decoding(latent, encoding, dropout_rate=0.2, N=N)\n",
        "# Now we inster the loss into the graph \n",
        "loss_layer = VAEloss()([z_mean, z_log_var, latent, embedding, inputs, decoding_mean])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4nV9OEXrkty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VAE_neg = Model(inputs, [loss]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LJcVQ3w5EB",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBkhZlNxBTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint_file_pos = \"textVAE_pos_checkpt.h5\"\n",
        "model_checkpoint_file_neg = \"textVAE_neg_checkpt.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC27n3V1A8r9",
        "colab_type": "code",
        "outputId": "365531e9-6ee2-4c84-e41a-a8a924e54afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "N_STEPS = int((1-VALIDATION_SPLIT)*100000/BATCH_SIZE) \n",
        "N_STEPS"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXS8bYgBoTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, input_file_location, vocab, model_location, \n",
        "                epochs=EPOCHS, n_steps=N_STEPS, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  Trains the given model for the given number of epochs, \n",
        "  making -n_steps- in each epoch. \n",
        "  \"\"\"\n",
        "  K.clear_session()\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print('-------epoch: ',epoch,'--------')\n",
        "    model.fit(input_generator(positive_location, chunksize=BATCH_SIZE,\n",
        "                                      max_seq_length=MAX_SEQUENCE_LENGTH, vocab=vocab),\n",
        "                        epochs=1, steps_per_epoch=N_STEPS, \n",
        "                        validation_data = (validation_pos,validation_pos))\n",
        "  print(\"Saving the model\")\n",
        "  model.save_weights(model_location)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czHBc4u03qEI",
        "colab_type": "text"
      },
      "source": [
        "#### Positive model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cB4UmvBNEQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_pos_trained = train_model(VAE_pos, \n",
        "                                    positive_location, \n",
        "                                    vocab_pos, \n",
        "                                    model_checkpoint_file_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl6Ljqd1KD5S",
        "colab_type": "text"
      },
      "source": [
        "Now we want to define a separate model for the Encoder and for the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni3sIoCDAvpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b2acbaa4-3102-4519-b29c-dcd4ddc3a91d"
      },
      "source": [
        "#ENCODER\n",
        "N,D = emb_matrix_pos.shape\n",
        "_inputs, _embedding = get_embedding(vocab_pos, emb_matrix_pos, N, D)\n",
        "_, _, _latent, _encoding = get_encoding(_embedding, dropout_rate=0.2)\n",
        "pos_encoder = Model(inputs=_inputs, outputs=[_latent,_encoding])\n",
        "#DECODER \n",
        "_latent = Input(shape=(LATENT_DIM,))\n",
        "_encoding = Input(shape=(HIDDEN_DIM,))\n",
        "_decoding, _decoding_mean = get_decoding(_latent, _encoding, dropout_rate=0.2, N=N)\n",
        "_decoding_mean = (Activation('softmax'))(_decoding_mean)\n",
        "pos_decoder = Model(inputs=[_latent, _encoding], outputs=_decoding_mean)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXVbeBo_xB1G",
        "colab_type": "text"
      },
      "source": [
        "# Tests \n",
        "We are now going to use the trained model to do sentence interpolation, but we need a few helper functions to do so "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sd277FwyN7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word_pos = {v:k for k,v in vocab_pos.items()}\n",
        "index2word_neg = {v:k for k,v in vocab_neg.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBR5t4BI71T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_latent(encoder,inputs):\n",
        "  \"\"\"\n",
        "  Uses the encoder part of the trained generator to go \n",
        "  from the input space to the latent space.\n",
        "  \"\"\"\n",
        "  latent,encoding = encoder(inputs)\n",
        "  return latent,encoding\n",
        "\n",
        "def predict_sentence(decoder,inputs):\n",
        "  \"\"\"\n",
        "  Uses the decoder part of the trained generator to \n",
        "  go from the latent space to a new sentence.\n",
        "  Inputs must be the tuple (x,z)\n",
        "  \"\"\"\n",
        "  latent = model.embedding(inputs[0])\n",
        "  encoding = inputs[1]\n",
        "  decoding_mean = decoder(latent,encoding)\n",
        "  return x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDr6It-xFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_sampled_sentence(sentence_original, sentence_vector, decoder, index2word, latent_dim=LATENT_DIM, max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Uses the trained model\n",
        "  @:param: sentence vector: latent space representation of a sentence\n",
        "      (this should be the output of predict_latent_mean)\n",
        "\n",
        "  \"\"\"\n",
        "  N = len(index2word.keys())+1\n",
        "  sentence_vector = np.reshape(sentence_vector,[1,latent_dim]) # reshaping into 1 x latent space, where 1 is the batch size\n",
        "  generated = tf.keras.activation.softmax(predict_sentence(decoder, (sentence_original, sentence_vector)))\n",
        "  generated = np.reshape(generated,[max_len,N]) # reshaping into sequence length x vocabulary words \n",
        "  generated_indices = np.apply_along_axis(np.argmax, 1, generated)\n",
        "  word_list = list(np.vectorize(index2word.get)(generated_indices))\n",
        "  w_list = [w for w in word_list if w] # filtering out the words\n",
        "  print(' '.join(w_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0oG1Ll0XgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shortest_homologies(point1,point2,n):\n",
        "  \"\"\"\n",
        "  Discover n mid-way  points in the path between point1 and point2.\n",
        "  The name of the functions is due to the fact that the points are in the \n",
        "  latent space.\n",
        "  \"\"\"\n",
        "  dist_vec = point2 - point1\n",
        "  sample = np.linspace(0, 1, n, endpoint = True)\n",
        "  samples = []\n",
        "  for s in sample:\n",
        "      samples.append(point1 + s * dist_vec)\n",
        "  return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQU2-HVzIOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_interpolation(sentence1, sentence2, n, \n",
        "                            vocab, encoder, decoder, index2word, \n",
        "                            max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Interpolating between the two given sentences in n steps.\n",
        "  \"\"\"\n",
        "  sequence1 = sentence_to_sequence(sentence1, vocab)\n",
        "  sequence1 = pad_sequences(sequence1, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  sequence2 = sentence_to_sequence(sentence2, vocab)\n",
        "  sequence2 = pad_sequences(sequence2, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  latent1 = predict_latent(encoder,sequence1)\n",
        "  latent2 = predict_latent(encoder,sequence2)\n",
        "  homologies = shortest_homologies(latent1, latent2, n)\n",
        "  for latent_sentence in homologies:\n",
        "    print_sampled_sentence(sentence1, latent_sentence, decoder, index2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wv3PHMpQs4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = [\"I think machine learning is great\"]\n",
        "sentence2 = [\"Do you want a new book\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYhD3Q_8oeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "b8c9489a-01fe-4663-ce70-15bd5aab44c5"
      },
      "source": [
        "print(\"Positive sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_pos, pos_encoder, pos_decoder, index2word_pos)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive sentence interpolation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-b2bb9c76974a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Positive sentence interpolation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m sentences_interpolation(sentence1,sentence2,10,\n\u001b[0;32m----> 3\u001b[0;31m                         vocab_pos, pos_encoder, pos_decoder, index2word_pos)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-134-294643cb2612>\u001b[0m in \u001b[0;36msentences_interpolation\u001b[0;34m(sentence1, sentence2, n, vocab, encoder, decoder, index2word, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msequence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msequence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mlatent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mlatent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mhomologies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortest_homologies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-94c4f6a5fbb0>\u001b[0m in \u001b[0;36mpredict_latent\u001b[0;34m(encoder, inputs)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mspace\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    713\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defun_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3297\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2874 _defun_call  *\n        return self._make_op(inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2852 _make_op  **\n        c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 0 and 1000 for '{{node Mul_5}} = Mul[T=DT_FLOAT, _cloned=true](inputs, Mul_5/y)' with input shapes: [0,64], [1000,64].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diUSya8k9XyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Negative sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_neg,generator_neg, index2word_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Generator.ipynb",
      "provenance": [],
      "mount_file_id": "12MHNh7F8wxZkVWHEuDkvARP1SNU4bpcX",
      "authorship_tag": "ABX9TyMCOD4a8KG+Z+tkWZCJ1Emz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Tweet_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMqVaBwSPA",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of **Generating sentences from a continuous space paper**\n",
        "Here's a link to the [paper](https://arxiv.org/pdf/1511.06349v4.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCPxyfX9r53",
        "colab_type": "text"
      },
      "source": [
        "> ### The goal \n",
        " What I want to explore here is the expression of sentiment in generative models. <br>\n",
        " The dataset consists of two different samples of tweets, one with positive sentiment and one with negative sentiment. <br>\n",
        " The goal is to train two generators on the two sets separetly and analyse the qualitative differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwkLCb83wngS",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSehtnM7dGh",
        "colab_type": "code",
        "outputId": "1d81f7cc-288f-456a-8f7d-a50ce8ea7af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  twitter-datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3kXO6vvwm9f",
        "colab_type": "code",
        "outputId": "9fd22c1d-989d-4ff9-c6e4-3701f8891da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RnhXHWxKGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxiSqdJQxPvj",
        "colab_type": "text"
      },
      "source": [
        "An example of the raw data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR4lq8bJxKNx",
        "colab_type": "code",
        "outputId": "fecccc00-e537-499b-b9f0-d418d6249a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW_iOL4O78mD",
        "colab_type": "code",
        "outputId": "8b9a33f4-40fa-4b6b-9b51-b66a7caeac85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzumvsqowr8M",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJp435CzVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from collections import Counter\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StWI7_EYwbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Transforms the specified files in tokens using the Twitter tokenizer.\n",
        "    @:params: str\n",
        "        Input text to tokenize\n",
        "    @:return: list(str)\n",
        "        Returns the tokens as a list of strings.\n",
        "    \"\"\"\n",
        "    tokenizer = TweetTokenizer()\n",
        "    # tokenizing the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [w.lower() for w in tokens]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2SswBFb7ut3",
        "colab_type": "text"
      },
      "source": [
        "Now we build a vocabulary with the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Gwr0Ex7zeP",
        "colab_type": "code",
        "outputId": "0c2c8784-43b5-4dff-c583-a1413d8ff0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "frequency_treshold = 100000*0.001 # 0.1% of the tweets should contain the \"frequent words\"\n",
        "frequency_treshold"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-eecHHzzQJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(file_name, output_file_name):\n",
        "  \"\"\"\n",
        "  Builds the vocabulary for the specified file.\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  print(\"Reading \",file_name)\n",
        "  raw = open(file_name,  \"r\").read()\n",
        "  more_words = tokenize_text(raw)\n",
        "  words.extend(more_words)\n",
        "  # counting the words\n",
        "  counter = Counter(words)\n",
        "  words_count = dict(counter)\n",
        "  # filtering \n",
        "  filtered_words = [k for k, v in words_count.items() if v >= frequency_treshold]\n",
        "  # building voabulary \n",
        "  # index starts at 1 so that the 0 can be left for the empty spaces\n",
        "  vocab = {k:i+1 for i,k in enumerate(filtered_words)}\n",
        "  # saving the vocabulary \n",
        "  with open(output_file_name, 'wb') as f: \n",
        "      pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2XSsFiVECtj",
        "colab_type": "code",
        "outputId": "9be8183a-d58e-43b9-e89e-e91b5bc86d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_pos = build_vocabulary(positive_location, \"vocab_pos.pkl\")\n",
        "vocab_neg = build_vocabulary(negative_location, \"vocab_neg.pkl\")"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading  twitter-datasets/train_pos.txt\n",
            "Reading  twitter-datasets/train_neg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bpgXYLg9IkZ",
        "colab_type": "code",
        "outputId": "8eba01c4-3396-4484-bef0-e87c9e0744f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_pos.keys())"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1082"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8Sh0RXESLv",
        "colab_type": "code",
        "outputId": "b0790fcf-95ca-498e-e65b-833cf2a0c5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_neg.keys())"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgAWLg6-4o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_sentence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  Filters the given sentence with the given vocabulary. \n",
        "    The words that are not in the vocabulary will be filtered out \n",
        "    of the sentence. \n",
        "  \"\"\"\n",
        "  return [word for word in sentence if word in vocab.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kRaA30GKkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_sequence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  (Filtering included)\n",
        "  Transforms the given sentence into a sequence of ints, \n",
        "  corresponding to the index of the word.\n",
        "  \"\"\"\n",
        "  filtered_sentence = filter_sentence(sentence, vocab)\n",
        "  return [vocab[word] for word in filtered_sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc5EBhCqwtuW",
        "colab_type": "text"
      },
      "source": [
        "# Sentences handler\n",
        "During training we want to be able to load the tweets from the file sequentially for memory efficiency. <br>\n",
        "Here I implement the helper functions that will do that\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKxJMIYlio29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_new_chunk(start, chunksize, file):\n",
        "  \"\"\"\n",
        "  Loads -chunksize- tweets from the specified file, starting from \n",
        "  -start- tweet (excluded). \n",
        "  \"\"\"\n",
        "  f=open(file)\n",
        "  lines=f.readlines()\n",
        "  selected = lines[start:start+chunksize]\n",
        "  return selected "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyDeGyj-q8D",
        "colab_type": "code",
        "outputId": "fbc13e93-f84e-4020-ba6a-c3e27746ed1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "load_new_chunk(99997, 5, positive_location)"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<user> <user> um gord ... i just read your profile . i'm not sure i can have lunch with a riders fan\\n\",\n",
              " \"<user> i'm so excited for tomorrow ! look out for two leprechauns ! xx\\n\",\n",
              " 'i always wondered what the job application is like at hooters .. do they just give you a bra and say , \" here fill this out \" .. ? \"\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dL5Zn1yG2Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9IcOlwA-3YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_generator(file, start, chunksize, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  The output of the generator must be a tuple (inputs, targets)\n",
        "  This tuple (a single output of the generator) makes a single batch\n",
        "  Different batches may have different sizes. For example, \n",
        "  the last batch of the epoch is commonly smaller than the others, \n",
        "  if the size of the dataset is not divisible by the batch size. \n",
        "  \"\"\"\n",
        "  sentences = load_new_chunk(start,chunksize,file)\n",
        "  sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "  return padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duBVbFlI5sq",
        "colab_type": "code",
        "outputId": "c35fbeaf-9dbf-4b0d-955f-45e7640aee04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "input_generator(positive_location, 0, 5, 100, vocab_pos)"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,  659,  101,  225, 1017,  397,   43,    2,\n",
              "         478,  101,  889,  889,  689,  101,  225,  494,    2,  889,  397,\n",
              "        1017,   38,  478,  885,  795,  885, 1017,  889,  494,    2,  689,\n",
              "         889,  689,  397,  889,  689,  494,    9,  689,  889,  881,  795,\n",
              "         101,  225,  494,    2,  889,   38,  889,  478, 1034,  689,  478,\n",
              "         304,  889,  689,  713,  225,   38,  448,  689,  101,  494,  494,\n",
              "          38,  494,   16,  448,  101,  494,    2,  689,  592, 1017,  795,\n",
              "         689,  101,  713,    2,  881,  881, 1041,  689,  881,  881,  689,\n",
              "         713,  885, 1017, 1006,  448, 1017,  881,    2, 1017, 1017,  151,\n",
              "         610],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,  448, 1017,\n",
              "         348,   38,  101,  225, 1017,  795,  689,  101,  397,  881,  689,\n",
              "        1034,    2,  348,    2,  225,  225,  689,  478,  101,  885,  448,\n",
              "          16,    2,  713,  689,  889,  185,  494, 1017, 1017,  889,  348,\n",
              "         397,  689,  592,  689,  101,  494,  795,  689,  101,  397,  889,\n",
              "          38,  885, 1017,  689,  397,  795,  689,  101,  397,  592,  689,\n",
              "         494,  689,    9,  494,  225,  304,    9,  659,  101,  397,  881,\n",
              "          43],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   34,  659,  101,  225, 1017,  397,\n",
              "          43,  101,  225,  494,  592,  101,  494,  348,   38,  225,  592,\n",
              "        1017,  397,    2,  889,   38,  448,  689,  369,   39,   34,  881,\n",
              "         689,  689, 1017,  478,  494, 1017,  448,   38,  494,  494,  881,\n",
              "        1017,   39, 1006,  348,  397,   38,  304,  304,  448,    2,  494,\n",
              "         348],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,  659,  101,  225,\n",
              "        1017,  397,   43,  659,  101,  225, 1017,  397,   43,  494,   38,\n",
              "         889,  304,  225,  225,    2,  397,   43,   43,  478,  689,  889,\n",
              "         185,  494,  494,  397,    2,  592,  881,    2,  881,  885,   38,\n",
              "         885,   38,    9,    9,    9,  101,  225,  494,  304, 1017, 1017,\n",
              "         592,  478,  689,    2,  889,  795,   38,  494,   38,  889, 1034,\n",
              "          39],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    2,  225,    2,  494,\n",
              "           2,  889, 1034,  885,  795,  448,  397,  689,  494, 1017,  397,\n",
              "         494,  885,  397,    2,  225,  494, 1017,  448, 1017,  225,  494,\n",
              "        1017,  225,  494,  448,    2,  397,  494,  478,   38,  795, 1034,\n",
              "           2, 1041,  494, 1017, 1017, 1017,  397,  397,  397,   39,   39,\n",
              "          39]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NIhn9VUwzmP",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHrAfaCJmA",
        "colab_type": "text"
      },
      "source": [
        "To embed the words we are going to use a pre-trained GloVe model as a *prior* on the embedding that we'll be inserted as a first layer to the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTmWWpn7w1Jc",
        "colab_type": "code",
        "outputId": "8da15544-2c4f-4c1f-aa66-a0b7392fd1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-14 12:44:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-14 12:44:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-14 12:44:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.11MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-14 12:51:08 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgHIKzsEbWG",
        "colab_type": "code",
        "outputId": "46698f22-ea28-4099-a50f-7ae54cb3e152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ECvY5mFBSD",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the *200d* embedding file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxnFtKDoFAtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_location = 'glove.6B.200d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txw8HQPfFO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOa0_99hFAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove_location, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S10AVtAqAs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_emb_matrix(glove, vocab):\n",
        "  \"\"\"\n",
        "  Build the pre-trained GloVe embedding on the given vocabulary. \n",
        "  The GloVe embedding should be presented as a vocabulary. \n",
        "  \"\"\"\n",
        "  print(\"Building the GloVe embedding.\")\n",
        "  N = len(vocab.keys()) + 1 # number of words in the vocabulary \n",
        "  D = 200 # embedding dimension\n",
        "  emb_matrix = np.zeros((N,D))\n",
        "  for idx, word in enumerate(vocab.keys()):\n",
        "    embedding_vector = glove.get(word)\n",
        "    if embedding_vector is not None: emb_matrix[idx+1] = embedding_vector\n",
        "    else: emb_matrix[idx+1] = embeddings_index.get('unk')\n",
        "  return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8_oEHkGjm8",
        "colab_type": "code",
        "outputId": "79aa9bb1-ba95-4b0a-f76f-d37e9bb50332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "emb_matrix_pos = build_emb_matrix(embeddings_index, vocab_pos)\n",
        "emb_matrix_neg = build_emb_matrix(embeddings_index, vocab_neg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the GloVe embedding.\n",
            "Building the GloVe embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idGVoWe7G64B",
        "colab_type": "code",
        "outputId": "7a481118-534c-4813-8a13-142479adb68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb_matrix_pos.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1083, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSIyMZstbdiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez(\"emb_matrix_pos\",emb_matrix_pos)\n",
        "np.savez(\"emb_matrix_neg\",emb_matrix_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXDvDo5I7Jab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_matrix_pos = np.load(\"emb_matrix_pos.npz\")[\"arr_0\"]\n",
        "emb_matrix_neg = np.load(\"emb_matrix_neg.npz\")[\"arr_0\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlfhxfAw19g",
        "colab_type": "text"
      },
      "source": [
        "# VAE model\n",
        "To make my life simpler I chose to use Keras to build the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv37tgi0HOYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.advanced_activations import ELU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2vCT4_HCXN",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameters first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di8j0afxw3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_SPLIT = 0.3\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-eFaAIHx_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LATENT_DIM = 64\n",
        "HIDDEN_DIM = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYMmGPNd7a-i",
        "colab_type": "text"
      },
      "source": [
        "#### text VAE class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcN1Jf7v-px",
        "colab_type": "text"
      },
      "source": [
        "In the following cells we'll build a **Model** class and its **Layers**' classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGCQIt3u6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlzVlTJ_vi4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  \"\"\"\n",
        "  Sampling layer. \n",
        "  The sampling will be from the posterior on the latent vector, \n",
        "  whose prior is a standard Gaussian. \n",
        "  \"\"\"\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    # batch and latent dimensionality\n",
        "    N= tf.shape(z_mean)[0] \n",
        "    D = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(N, D))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zREAGsAxkTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(layers.Layer):\n",
        "  \"\"\"\n",
        "  Embedding layer. \n",
        "  Responsible for training the embedding. \n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               emb_matrix= None, # pre-trained embeddings\n",
        "               name='embedding',\n",
        "               dynamic=True,\n",
        "               to_train= True, # wether to train the embeddings\n",
        "               **kwargs):\n",
        "    super(Embedding, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.embedding = layers.Embedding(N, D, weights=[emb_matrix],\n",
        "                                      input_length=seq_length, trainable=to_train)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyZfu7BwlTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Encoder layer. \n",
        "  This layer parametrizes the mapping from the input  \n",
        "  to a latent space distribution. \n",
        "  Concretely it will map sentences to the triple (z_mean, z_log_var, z).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               latent_dim=LATENT_DIM,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               name='encoder',\n",
        "               dropout_rate=0.2,\n",
        "               dynamic=True,\n",
        "               activation_fun='elu',\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.recurrent_layer = layers.Bidirectional(layers.LSTM(hidden_dim, \n",
        "                            return_sequences=False, recurrent_dropout=dropout_rate,\n",
        "                            input_shape=(None,seq_length,D)), merge_mode='concat')\n",
        "    self.dense_proj = layers.Dense(hidden_dim, activation=activation_fun)\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Input must be of shape (None, max_len)\n",
        "    h = self.recurrent_layer(inputs)\n",
        "    x = self.dense_proj(h)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8io-a-Xzy6S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Decoder layer. \n",
        "  This layer parametrizes the mapping from the latent space to the \n",
        "  output dimension. \n",
        "  Concretely it will map a z to a readable sentence.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               name='decoder',\n",
        "               dynamic=True,\n",
        "               dropout_rate=0.2,\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.repeat_layer = layers.RepeatVector(seq_length)\n",
        "    self.recurrent_layer = layers.LSTM(hidden_dim, return_sequences=True, \n",
        "                                       recurrent_dropout=dropout_rate, \n",
        "                                       input_shape=(None,seq_length,D))\n",
        "    self.mean = layers.TimeDistributed(layers.Dense(N, activation='linear'))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_repeated = self.repeat_layer(inputs)\n",
        "    h_decoded = self.recurrent_layer(z_repeated)\n",
        "    x_decoded_mean = self.mean(h_decoded)\n",
        "    return h_decoded, x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPooKv_05bSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxln6VbuudUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textVAE(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  Model class for VAE.\n",
        "  Combines the embedding, encoder and decoder into an \n",
        "  end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               emb_matrix=None, # pre-trained embeddings - if None, they will be trained\n",
        "               train_emb = True,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               dropout_rate=0.2,\n",
        "               name='textVAE',\n",
        "               **kwargs):\n",
        "    super(textVAE, self).__init__(name=name, **kwargs)\n",
        "    self.N, self.D = vocab_dim\n",
        "    self.seq_length = seq_length\n",
        "    self.embedding = Embedding(vocab_dim=vocab_dim,\n",
        "                               seq_length=seq_length,\n",
        "                               emb_matrix=emb_matrix,\n",
        "                               to_train=train_emb)\n",
        "    self.encoder = Encoder(vocab_dim=vocab_dim,\n",
        "                           seq_length=seq_length,\n",
        "                           latent_dim=latent_dim,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           dropout_rate=dropout_rate)\n",
        "    self.decoder = Decoder(vocab_dim=vocab_dim, \n",
        "                           seq_length=seq_length,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           dropout_rate=dropout_rate)\n",
        "    \n",
        "  def compute_loss(self, z_mean, z_log_var, z, \n",
        "                   x, x_decoded_mean, target_weights):\n",
        "    \"\"\"\n",
        "    Computes the VAE loss. \n",
        "    \"\"\"\n",
        "    labels = tf.cast(x, tf.int32)\n",
        "    xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, \n",
        "                        labels, weights=target_weights, \n",
        "                        average_across_timesteps=False,\n",
        "                        average_across_batch=False), axis=-1)\n",
        "    kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - \n",
        "                                     tf.exp(z_log_var) + 1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    z_mean, z_log_var, z = self.encoder(x)\n",
        "    h_decoded,x_decoded_mean = self.decoder(z)\n",
        "    # Add KL divergence regularization loss.\n",
        "    target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "    loss = self.compute_loss(z_mean, z_log_var, z, \n",
        "                             inputs, x_decoded_mean, target_weights) \n",
        "    self.add_loss(loss)\n",
        "    return h_decoded,x_decoded_mean  \n",
        "\n",
        "  def predict_latent_mean(self,inputs):\n",
        "    \"\"\"\n",
        "    Uses the encoder part of the trained generator to go \n",
        "    from the input space to the latent space.\n",
        "    \"\"\"\n",
        "    x = self.embedding(inputs)\n",
        "    z_mean, _, _ = self.encoder(x)\n",
        "    return z_mean\n",
        "  \n",
        "  def predict_sentence(self,inputs):\n",
        "    \"\"\"\n",
        "    Uses the decoder part of the trained generator to \n",
        "    go from the latent space to a new sentence.\n",
        "    \"\"\"\n",
        "    _,x_decoded_mean = self.decoder(inputs)\n",
        "    return x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAR_g9F7a5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGFY1dn7Den",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_model(emb_matrix, name):\n",
        "  \"\"\"\n",
        "  Constructing the text VAE model.\n",
        "  \"\"\"\n",
        "  N,D = emb_matrix.shape\n",
        "  vae = textVAE(vocab_dim=(N,D),\n",
        "                seq_length=MAX_SEQUENCE_LENGTH,\n",
        "                emb_matrix=emb_matrix, \n",
        "                name=name)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  vae.compile(optimizer=optimizer)\n",
        "  return vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgNBXBecIw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusLhCTm8Zlq",
        "colab_type": "code",
        "outputId": "232927c8-5705-4952-f92c-018ab5903457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generator_pos = construct_model(emb_matrix_pos, name=\"pos_vae\")\n",
        "generator_pos"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.textVAE at 0x7f0838a9add8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LJcVQ3w5EB",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBkhZlNxBTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint_file_pos = \"textVAE_pos_checkpt\"\n",
        "model_checkpoint_file_neg = \"textVAE_neg_checkpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC27n3V1A8r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1\n",
        "N_STEPS = int(100000/BATCH_SIZE) \n",
        "SAVING_STEPS = [300,700,999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXS8bYgBoTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, input_file_location, vocab, model_location, \n",
        "                epochs=EPOCHS, n_steps=N_STEPS, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  Trains the given model for the given number of epochs, \n",
        "  making -n_steps- in each epoch. \n",
        "  \"\"\"\n",
        "  for epoch in range(EPOCHS):\n",
        "    print('-------epoch: ',epoch,'--------')\n",
        "    for step in range(N_STEPS):\n",
        "      start = step*BATCH_SIZE\n",
        "      input_sequences = input_generator(positive_location,start=start, chunksize=BATCH_SIZE,\n",
        "                                        max_seq_length=MAX_SEQUENCE_LENGTH, vocab=vocab)\n",
        "      model.fit(x=input_sequences, y=input_sequences, batch_size = batch_size, epochs=1, \n",
        "                validation_split=VALIDATION_SPLIT, dynamic=True)\n",
        "      if step in SAVING_STEPS: \n",
        "        loss = model.losses\n",
        "        print('----step: ',step,' ----loss:', loss)\n",
        "        model.save(model_location)\n",
        "  \n",
        "  model.save(model_location)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cB4UmvBNEQZ",
        "colab_type": "code",
        "outputId": "8c61c938-10e4-46d1-d5b2-db90e5821b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generator_pos_trained = train_model(generator_pos, positive_location, vocab_pos, \n",
        "                                    model_checkpoint_file_pos)"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 --------\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 698.8077 - val_loss: 693.8820\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 693.9007 - val_loss: 691.0599\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 689.9549 - val_loss: 678.8959\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 683.1779 - val_loss: 674.5922\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 676.1219 - val_loss: 665.6337\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 662.9984 - val_loss: 650.7321\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 647.7756 - val_loss: 634.0820\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 633.5075 - val_loss: 611.5432\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 612.4139 - val_loss: 601.8898\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 590.4075 - val_loss: 563.4077\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 574.2181 - val_loss: 545.9318\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 542.7334 - val_loss: 520.1815\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 520.0309 - val_loss: 494.6841\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 497.1273 - val_loss: 480.5111\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 482.1198 - val_loss: 442.4906\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 452.1064 - val_loss: 432.6648\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 422.2578 - val_loss: 413.9472\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 411.7465 - val_loss: 393.0594\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 367.4001 - val_loss: 350.7354\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 358.3314 - val_loss: 301.1439\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 339.8926 - val_loss: 319.1566\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 344.2596 - val_loss: 325.1105\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 306.4440 - val_loss: 325.5230\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 297.5009 - val_loss: 306.2533\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 288.6124 - val_loss: 282.1977\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 282.9783 - val_loss: 283.4448\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 290.8047 - val_loss: 285.1292\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 284.2160 - val_loss: 254.7630\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 284.1080 - val_loss: 269.9829\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 252.9841 - val_loss: 251.5777\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 312.6880 - val_loss: 346.4076\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 346.3907 - val_loss: 342.6561\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 268.9118 - val_loss: 245.4216\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 258.3502 - val_loss: 244.8209\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 244.6414 - val_loss: 240.7478\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 238.6606 - val_loss: 242.3394\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 246.9892 - val_loss: 250.0274\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 233.9209 - val_loss: 248.0225\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 236.3231 - val_loss: 252.5045\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 224.0676 - val_loss: 265.2090\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 260.5677 - val_loss: 229.8018\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 235.6712 - val_loss: 245.3298\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 247.9422 - val_loss: 235.6520\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 276.2213 - val_loss: 249.8044\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 245.3513 - val_loss: 268.7103\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 260.8088 - val_loss: 270.5049\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 252.0672 - val_loss: 301.5920\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 264.1086 - val_loss: 218.1097\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 240.2035 - val_loss: 253.9196\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 234.9343 - val_loss: 242.2432\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 244.2167 - val_loss: 254.6644\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 211.0326 - val_loss: 221.0772\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 201.3711 - val_loss: 234.2533\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 244.4638 - val_loss: 222.7850\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 237.6735 - val_loss: 229.3997\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 239.6851 - val_loss: 223.5519\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 269.6864 - val_loss: 241.1450\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 233.2129 - val_loss: 282.6026\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 229.0676 - val_loss: 236.7655\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 238.7834 - val_loss: 239.8260\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 213.5670 - val_loss: 225.2363\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 221.3938 - val_loss: 247.7825\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 242.8933 - val_loss: 251.0507\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 230.0834 - val_loss: 239.7016\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 221.0535 - val_loss: 216.4441\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 230.1796 - val_loss: 245.1970\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 218.4088 - val_loss: 242.9353\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 230.0200 - val_loss: 271.1663\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 235.4420 - val_loss: 204.6076\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 250.1484 - val_loss: 211.2518\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 244.1525 - val_loss: 230.2000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 235.4407 - val_loss: 249.9742\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 233.6686 - val_loss: 223.2609\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 236.9043 - val_loss: 235.6202\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 230.2656 - val_loss: 244.0491\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 213.6003 - val_loss: 202.9525\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 235.2395 - val_loss: 217.1980\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 226.5772 - val_loss: 193.2612\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 202.9784 - val_loss: 230.4840\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 245.2519 - val_loss: 235.3107\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 229.5641 - val_loss: 194.4621\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 231.2501 - val_loss: 215.4813\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 236.2823 - val_loss: 236.3363\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 243.3572 - val_loss: 252.5992\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 237.5352 - val_loss: 223.2103\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 223.2885 - val_loss: 244.8528\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 196.5510 - val_loss: 185.8400\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 185.8096 - val_loss: 184.6825\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 184.7287 - val_loss: 183.0557\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 183.1944 - val_loss: 181.1866\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 181.1987 - val_loss: 179.2852\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 179.2826 - val_loss: 177.4464\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 177.3813 - val_loss: 175.6911\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 175.7190 - val_loss: 174.1370\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 174.1682 - val_loss: 172.9218\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 172.8646 - val_loss: 171.7123\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 171.7675 - val_loss: 170.8402\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 170.8572 - val_loss: 170.1688\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 170.1355 - val_loss: 169.6581\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 169.6615 - val_loss: 169.3918\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 169.3801 - val_loss: 169.0269\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 169.1234 - val_loss: 202.0356\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 257.5469 - val_loss: 256.7405\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 250.8111 - val_loss: 274.8634\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 265.1502 - val_loss: 260.2363\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 230.8647 - val_loss: 248.9527\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 245.7855 - val_loss: 291.2664\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 258.4088 - val_loss: 242.5138\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 251.3443 - val_loss: 229.8186\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 282.0905 - val_loss: 222.1643\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 241.0624 - val_loss: 247.4738\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 244.8950 - val_loss: 248.5504\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 255.6831 - val_loss: 242.4956\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 235.0994 - val_loss: 239.4596\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 235.5109 - val_loss: 228.1377\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 231.6615 - val_loss: 248.1152\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 240.5766 - val_loss: 228.1812\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 251.9928 - val_loss: 230.4799\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 223.8066 - val_loss: 236.1996\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 218.2076 - val_loss: 240.7765\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 234.5774 - val_loss: 203.7107\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 230.4201 - val_loss: 241.7250\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 225.3774 - val_loss: 228.6492\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 225.0411 - val_loss: 238.3579\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 226.1793 - val_loss: 221.0917\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 221.5738 - val_loss: 210.8419\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 225.9823 - val_loss: 207.8264\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 245.8233 - val_loss: 241.4473\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 216.4993 - val_loss: 226.7629\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 230.0588 - val_loss: 228.7787\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 232.8841 - val_loss: 228.2437\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 224.2035 - val_loss: 261.6675\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 221.1910 - val_loss: 229.0677\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 218.7838 - val_loss: 235.0462\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 237.8173 - val_loss: 244.4602\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 191.9696 - val_loss: 217.3442\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 219.8522 - val_loss: 220.2858\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 225.0403 - val_loss: 204.5559\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 224.1137 - val_loss: 233.0647\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 223.3438 - val_loss: 241.6813\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 226.3997 - val_loss: 219.1619\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 234.0133 - val_loss: 210.3811\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 246.5893 - val_loss: 238.3787\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 222.5698 - val_loss: 216.5547\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 225.8690 - val_loss: 225.3672\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 233.4020 - val_loss: 240.2308\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 212.9006 - val_loss: 230.3249\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 240.7748 - val_loss: 232.7768\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 254.0011 - val_loss: 219.4274\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 225.0049 - val_loss: 232.7402\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 224.5261 - val_loss: 249.9417\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 227.8842 - val_loss: 216.4617\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 259.3975 - val_loss: 238.1253\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 222.0318 - val_loss: 229.3413\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 228.7905 - val_loss: 215.1086\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 221.6279 - val_loss: 203.4226\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 223.3131 - val_loss: 223.9434\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 237.3842 - val_loss: 198.2118\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 228.7766 - val_loss: 233.0906\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 220.4839 - val_loss: 204.6897\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 233.8464 - val_loss: 230.5527\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 231.3333 - val_loss: 224.8412\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 219.2717 - val_loss: 263.4140\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 237.4434 - val_loss: 230.0726\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 223.7438 - val_loss: 245.9961\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 108.0701 - val_loss: 86.4233\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 205.8186 - val_loss: 191.5674\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 221.4928 - val_loss: 208.8009\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 220.5478 - val_loss: 181.6401\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 227.6165 - val_loss: 223.2828\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 221.3097 - val_loss: 207.4474\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 216.2848 - val_loss: 219.9561\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 225.1426 - val_loss: 210.5123\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 220.4153 - val_loss: 217.7468\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 234.7261 - val_loss: 227.0523\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 214.0456 - val_loss: 219.4849\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 221.9794 - val_loss: 272.7499\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 250.9064 - val_loss: 129.8582\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 220.0964 - val_loss: 219.3778\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 231.7580 - val_loss: 208.0642\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 208.2404 - val_loss: 221.2876\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 214.8284 - val_loss: 219.1846\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 211.9772 - val_loss: 215.1116\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 215.9581 - val_loss: 232.2888\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 232.0330 - val_loss: 201.9744\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 210.3633 - val_loss: 229.7091\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 232.4904 - val_loss: 223.5920\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 207.8225 - val_loss: 209.6023\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 208.0278 - val_loss: 145.6505\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 234.5160 - val_loss: 235.0266\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 240.1806 - val_loss: 219.4897\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 213.1500 - val_loss: 214.6957\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 222.6279 - val_loss: 196.7197\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 223.7911 - val_loss: 219.3557\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 231.2869 - val_loss: 219.4753\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 210.6084 - val_loss: 207.7070\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 206.9789 - val_loss: 215.4901\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 205.1399 - val_loss: 239.3879\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 221.0729 - val_loss: 207.1262\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 219.3844 - val_loss: 231.0203\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 222.0520 - val_loss: 200.9881\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 212.3371 - val_loss: 206.4317\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 215.0225 - val_loss: 211.1696\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 232.7112 - val_loss: 194.1978\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 206.5660 - val_loss: 210.2366\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 215.2511 - val_loss: 220.9033\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 227.3649 - val_loss: 198.5679\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 224.3930 - val_loss: 222.4349\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 219.6731 - val_loss: 201.9604\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 223.6430 - val_loss: 231.1293\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 220.7306 - val_loss: 226.9052\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 212.6821 - val_loss: 224.9072\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 294.4576 - val_loss: 245.8119\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 203.3902 - val_loss: 220.0925\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 227.3061 - val_loss: 235.5479\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 207.8487 - val_loss: 240.1982\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 215.8616 - val_loss: 232.2929\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 232.7219 - val_loss: 160.0186\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 212.1872 - val_loss: 222.0913\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 214.2538 - val_loss: 230.9630\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 213.6063 - val_loss: 222.4292\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 215.6957 - val_loss: 219.1902\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 218.7361 - val_loss: 205.4568\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 220.8090 - val_loss: 204.8119\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 219.3579 - val_loss: 239.9448\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 214.7728 - val_loss: 248.4538\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 218.6960 - val_loss: 209.2366\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 224.3954 - val_loss: 227.4627\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 213.3606 - val_loss: 241.8372\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 208.0417 - val_loss: 242.6900\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 229.6222 - val_loss: 219.8101\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 200.3413 - val_loss: 230.5535\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 223.2429 - val_loss: 230.4660\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 197.2991 - val_loss: 230.7549\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 203.2470 - val_loss: 209.5835\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 212.6499 - val_loss: 220.3585\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 227.3355 - val_loss: 209.2910\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 172.3472 - val_loss: 95.4427\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 210.4097 - val_loss: 227.9016\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 203.1246 - val_loss: 230.5659\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 228.0736 - val_loss: 223.6906\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 211.1326 - val_loss: 204.1492\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 197.7480 - val_loss: 238.7050\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 207.1362 - val_loss: 208.5131\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 196.5773 - val_loss: 268.4466\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 230.5214 - val_loss: 236.1181\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 220.0049 - val_loss: 209.5184\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 204.5500 - val_loss: 212.1135\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 193.4506 - val_loss: 236.8284\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 200.9009 - val_loss: 209.4237\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 215.9068 - val_loss: 209.6514\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 233.8563 - val_loss: 233.2527\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 203.8493 - val_loss: 196.7872\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 209.3984 - val_loss: 203.4462\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 219.0417 - val_loss: 201.2176\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 187.1885 - val_loss: 230.1512\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 243.3389 - val_loss: 189.8335\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 214.0710 - val_loss: 172.2039\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 225.2268 - val_loss: 203.0870\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 201.4889 - val_loss: 224.0916\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 196.5275 - val_loss: 195.3030\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 210.0218 - val_loss: 208.0745\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 208.7833 - val_loss: 217.9921\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 216.1529 - val_loss: 190.4494\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 202.8502 - val_loss: 206.7672\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 205.4318 - val_loss: 209.0031\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 194.2822 - val_loss: 203.6186\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 187.8726 - val_loss: 212.3103\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 200.9233 - val_loss: 196.3443\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 186.8585 - val_loss: 200.4013\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 205.3962 - val_loss: 221.1115\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 201.9416 - val_loss: 209.6631\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 208.0610 - val_loss: 204.6981\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 190.2864 - val_loss: 182.8235\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 166.0641 - val_loss: 79.4227\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 127.8714 - val_loss: 200.3049\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 197.4124 - val_loss: 186.5074\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 191.6410 - val_loss: 224.6695\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 195.5493 - val_loss: 179.8148\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 188.2732 - val_loss: 195.4718\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 185.8920 - val_loss: 191.2232\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 200.0681 - val_loss: 206.1558\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 123.3241 - val_loss: 184.3456\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 179.6838 - val_loss: 190.7773\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 187.0267 - val_loss: 188.7950\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 199.3692 - val_loss: 168.3363\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 185.5111 - val_loss: 188.8996\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 186.3359 - val_loss: 182.0889\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 195.3808 - val_loss: 194.2341\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 200.2828 - val_loss: 186.6731\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 174.6995 - val_loss: 207.0695\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 205.0949 - val_loss: 195.8081\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 194.7972 - val_loss: 214.2292\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 206.4997 - val_loss: 185.1466\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 203.3452 - val_loss: 189.6931\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 192.8139 - val_loss: 179.2052\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 179.5776 - val_loss: 196.0989\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 182.3852 - val_loss: 216.6427\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 190.2273 - val_loss: 196.6009\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 199.4193 - val_loss: 178.8819\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 174.8709 - val_loss: 179.2274\n",
            "----step:  300  ----loss: [<tf.Tensor 'pos_vae/Mean_1:0' shape=() dtype=float32>]\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 207.5280 - val_loss: 199.2087\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 178.5570 - val_loss: 189.6366\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 206.6744 - val_loss: 194.9326\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 194.1107 - val_loss: 196.7486\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 203.5785 - val_loss: 175.2739\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 185.4745 - val_loss: 200.7014\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 179.3168 - val_loss: 189.2478\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 195.8293 - val_loss: 179.0831\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 191.5927 - val_loss: 165.7760\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 195.7115 - val_loss: 186.7187\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 181.5089 - val_loss: 186.3981\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 196.9482 - val_loss: 179.6967\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 176.4768 - val_loss: 184.8707\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 172.0606 - val_loss: 155.7568\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 193.7204 - val_loss: 171.7870\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 175.6411 - val_loss: 172.7740\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 178.8403 - val_loss: 175.3688\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 181.5027 - val_loss: 190.5642\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 185.3049 - val_loss: 179.1505\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 202.5350 - val_loss: 169.6360\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 164.7811 - val_loss: 179.6852\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 169.7687 - val_loss: 171.9382\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 169.9415 - val_loss: 165.0542\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 175.5303 - val_loss: 171.7593\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 174.7823 - val_loss: 184.7814\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 167.5441 - val_loss: 158.6422\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 185.4607 - val_loss: 153.9447\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 182.1522 - val_loss: 191.6589\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 188.1028 - val_loss: 170.9453\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 169.0417 - val_loss: 187.0729\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 178.5549 - val_loss: 180.0881\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 192.8281 - val_loss: 181.2352\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 184.7268 - val_loss: 181.1248\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 177.7127 - val_loss: 156.6874\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 167.7215 - val_loss: 173.5970\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 184.4231 - val_loss: 165.4363\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 189.4659 - val_loss: 95.3160\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 157.8610 - val_loss: 192.3820\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 201.4910 - val_loss: 183.3145\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 181.8006 - val_loss: 180.8783\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 161.8279 - val_loss: 180.3612\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 183.5663 - val_loss: 169.1955\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 171.3759 - val_loss: 168.5055\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 154.7764 - val_loss: 97.0147\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 197.4708 - val_loss: 185.7005\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 165.6954 - val_loss: 192.4668\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 191.4140 - val_loss: 175.2663\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 186.6123 - val_loss: 194.2428\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 163.3955 - val_loss: 198.3680\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 178.8660 - val_loss: 191.6091\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 171.6093 - val_loss: 181.6254\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 166.1300 - val_loss: 172.9270\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.6620 - val_loss: 170.3794\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 177.1021 - val_loss: 176.2819\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 172.4691 - val_loss: 167.9119\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 172.4992 - val_loss: 210.0484\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 190.5473 - val_loss: 187.4887\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 173.9418 - val_loss: 214.8732\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 184.0412 - val_loss: 167.0421\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 171.5925 - val_loss: 171.3708\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 185.4913 - val_loss: 195.1787\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 183.5187 - val_loss: 176.6130\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 162.5826 - val_loss: 189.1870\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 181.3354 - val_loss: 162.5665\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 191.7527 - val_loss: 186.5639\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 173.6057 - val_loss: 161.4720\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 181.4349 - val_loss: 153.3760\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 182.0297 - val_loss: 169.7967\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 187.7496 - val_loss: 203.8070\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 177.1593 - val_loss: 174.4646\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 173.8920 - val_loss: 156.2664\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 167.0645 - val_loss: 187.3461\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 174.1845 - val_loss: 188.2865\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 178.6979 - val_loss: 202.0525\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.3036 - val_loss: 170.0957\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 166.3308 - val_loss: 163.8115\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 155.6980 - val_loss: 159.4943\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 153.6731 - val_loss: 195.9654\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 192.0892 - val_loss: 157.7014\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 179.5878 - val_loss: 165.0186\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 180.0222 - val_loss: 181.4324\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 165.7114 - val_loss: 167.6278\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 187.6392 - val_loss: 160.3010\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 79.2662 - val_loss: 65.9792\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 80.1289 - val_loss: 177.2348\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.0766 - val_loss: 170.9788\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 161.6354 - val_loss: 151.4521\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 158.5013 - val_loss: 170.7665\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 172.6899 - val_loss: 193.5800\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 171.2045 - val_loss: 185.2274\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 182.1337 - val_loss: 167.6204\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 167.4805 - val_loss: 166.4595\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 177.9416 - val_loss: 203.1697\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 179.3569 - val_loss: 184.0690\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.5462 - val_loss: 177.6834\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 141.2638 - val_loss: 134.4872\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 172.1188 - val_loss: 162.8509\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 162.6377 - val_loss: 161.1407\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 177.0096 - val_loss: 172.6968\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 160.0821 - val_loss: 172.8989\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 160.4759 - val_loss: 150.9949\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 182.1975 - val_loss: 178.9440\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 183.6946 - val_loss: 163.9309\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 174.9718 - val_loss: 142.8083\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 169.5687 - val_loss: 163.6367\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 151.5704 - val_loss: 162.4425\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 169.8480 - val_loss: 163.4568\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 168.4752 - val_loss: 178.8226\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 189.1620 - val_loss: 166.1377\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 167.4008 - val_loss: 203.4312\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 164.5649 - val_loss: 172.6473\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 124.5387 - val_loss: 176.0032\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 176.6469 - val_loss: 163.8070\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 167.0883 - val_loss: 140.4656\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 178.3644 - val_loss: 191.6368\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 169.8131 - val_loss: 175.6901\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 177.7976 - val_loss: 196.2307\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 185.2104 - val_loss: 238.3897\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 168.7826 - val_loss: 171.7506\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 203.8684 - val_loss: 164.0967\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 157.5984 - val_loss: 189.8028\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 130.2313 - val_loss: 203.1824\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 172.0094 - val_loss: 188.6414\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 162.1351 - val_loss: 173.2756\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 177.2852 - val_loss: 177.1819\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 173.6773 - val_loss: 173.9936\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 170.4039 - val_loss: 164.5355\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 177.9040 - val_loss: 170.8080\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 180.8227 - val_loss: 167.6032\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 153.3844 - val_loss: 229.9420\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 172.1966 - val_loss: 183.7726\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 179.4977 - val_loss: 144.8826\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 160.5131 - val_loss: 178.6953\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 177.1768 - val_loss: 198.6356\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 170.6036 - val_loss: 176.2030\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 158.1489 - val_loss: 168.3185\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 161.9261 - val_loss: 167.3897\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 186.7197 - val_loss: 157.0015\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 175.1191 - val_loss: 181.1220\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 146.3546 - val_loss: 155.3676\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 144.9440 - val_loss: 175.6343\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 173.5549 - val_loss: 156.5355\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 162.6854 - val_loss: 172.9548\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 172.9969 - val_loss: 162.3581\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.8085 - val_loss: 182.3773\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 160.6325 - val_loss: 141.7251\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 157.4636 - val_loss: 162.9086\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 169.2643 - val_loss: 139.6219\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 164.1266 - val_loss: 153.4098\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 169.6165 - val_loss: 189.5555\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 158.0281 - val_loss: 156.7527\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 174.8354 - val_loss: 208.3715\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 173.7509 - val_loss: 153.1356\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 171.1534 - val_loss: 202.2926\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 183.9371 - val_loss: 181.5067\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 175.1226 - val_loss: 121.1377\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 156.8837 - val_loss: 160.4413\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 180.0017 - val_loss: 148.7721\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 169.6199 - val_loss: 168.0419\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 157.8367 - val_loss: 197.2507\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 181.1019 - val_loss: 173.3205\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 161.6210 - val_loss: 170.5937\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.4871 - val_loss: 129.6712\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 185.3547 - val_loss: 171.4151\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 192.8221 - val_loss: 161.0260\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 168.0695 - val_loss: 176.4883\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 164.5800 - val_loss: 166.3483\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.1642 - val_loss: 180.4143\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 172.9098 - val_loss: 174.9763\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 177.3374 - val_loss: 173.2812\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 180.0652 - val_loss: 186.3402\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 174.5966 - val_loss: 157.9891\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 167.9867 - val_loss: 212.3786\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.6819 - val_loss: 172.0559\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 158.1507 - val_loss: 171.2157\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 181.9693 - val_loss: 152.8721\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 173.5047 - val_loss: 165.6338\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 169.7804 - val_loss: 194.5528\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 185.6629 - val_loss: 167.8026\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 176.1873 - val_loss: 159.0470\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 165.7751 - val_loss: 174.7367\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 157.1456 - val_loss: 176.0131\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 184.2615 - val_loss: 180.9650\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 151.6355 - val_loss: 155.6301\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 171.1202 - val_loss: 121.3574\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 191.0697 - val_loss: 168.8597\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 165.2195 - val_loss: 188.3524\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 178.2083 - val_loss: 166.1269\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 166.0152 - val_loss: 154.3894\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 178.8718 - val_loss: 154.6555\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 149.4637 - val_loss: 175.2792\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 191.7883 - val_loss: 235.2336\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 163.2557 - val_loss: 184.1885\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 172.9373 - val_loss: 194.2462\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 166.2229 - val_loss: 169.1859\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 159.3447 - val_loss: 161.3883\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.5715 - val_loss: 255.4615\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 171.2055 - val_loss: 176.9313\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.8038 - val_loss: 187.8974\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 164.8253 - val_loss: 208.7504\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 160.4859 - val_loss: 177.9102\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 151.6404 - val_loss: 160.8769\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 175.1111 - val_loss: 175.5440\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 160.5953 - val_loss: 154.6355\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 181.0674 - val_loss: 168.8547\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 159.4754 - val_loss: 160.3459\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 180.4407 - val_loss: 170.6201\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 169.9735 - val_loss: 162.1485\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 170.6197 - val_loss: 153.1374\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 164.4313 - val_loss: 160.1764\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 196.7800 - val_loss: 160.6593\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 154.2354 - val_loss: 153.6480\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.2951 - val_loss: 182.0150\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 178.3269 - val_loss: 151.4241\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 156.4519 - val_loss: 141.7639\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 199.9715 - val_loss: 138.8736\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 175.4513 - val_loss: 147.4222\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 171.2133 - val_loss: 148.6778\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 158.3505 - val_loss: 189.7260\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 198.8697 - val_loss: 190.8322\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 174.6453 - val_loss: 152.6739\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 165.2988 - val_loss: 174.3869\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.7859 - val_loss: 180.6466\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 181.7595 - val_loss: 154.4032\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 169.7029 - val_loss: 160.5941\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 150.7432 - val_loss: 163.9898\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 168.4104 - val_loss: 184.1098\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 162.6182 - val_loss: 168.5720\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 120.1086 - val_loss: 184.3282\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 169.1655 - val_loss: 170.3167\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 166.5697 - val_loss: 158.0558\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 184.1216 - val_loss: 149.2744\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.0352 - val_loss: 166.4315\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 165.0600 - val_loss: 186.7978\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 167.1039 - val_loss: 146.6720\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 172.1672 - val_loss: 178.8157\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 167.9690 - val_loss: 167.4343\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.3647 - val_loss: 173.0269\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 168.0540 - val_loss: 152.8589\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 168.1206 - val_loss: 186.2433\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 164.6413 - val_loss: 170.5805\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 180.9987 - val_loss: 196.0827\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 188.0463 - val_loss: 189.4322\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 163.3275 - val_loss: 178.7854\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 166.5841 - val_loss: 176.1460\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 182.7255 - val_loss: 138.9637\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 158.3847 - val_loss: 188.0688\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 159.7026 - val_loss: 170.6463\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.5546 - val_loss: 178.1985\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 157.4404 - val_loss: 162.7854\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 163.3728 - val_loss: 169.7560\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 162.7879 - val_loss: 175.3171\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 143.4730 - val_loss: 167.8760\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 160.5072 - val_loss: 190.9535\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 177.1943 - val_loss: 180.2786\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 156.9926 - val_loss: 225.2954\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 135.2278 - val_loss: 176.4345\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 167.6573 - val_loss: 171.8319\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 173.3777 - val_loss: 173.7124\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 156.2526 - val_loss: 170.8730\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.5115 - val_loss: 152.7512\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 167.2999 - val_loss: 175.2281\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 131.5637 - val_loss: 49.8538\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 171.7136 - val_loss: 180.6967\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 179.3982 - val_loss: 182.2372\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.7086 - val_loss: 175.0561\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 136.8526 - val_loss: 169.3802\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.7195 - val_loss: 149.5582\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 179.0142 - val_loss: 156.2875\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 190.6980 - val_loss: 197.2551\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 143.5277 - val_loss: 166.3233\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 166.1755 - val_loss: 193.9346\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 200.5040 - val_loss: 177.9833\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 160.5692 - val_loss: 180.2229\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 167.8815 - val_loss: 172.4044\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.5733 - val_loss: 182.2107\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 165.0697 - val_loss: 170.8293\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 166.0351 - val_loss: 172.8142\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 162.0933 - val_loss: 170.2243\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 163.8203 - val_loss: 179.2498\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 165.7145 - val_loss: 151.7840\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.7737 - val_loss: 157.3436\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 163.6290 - val_loss: 150.1441\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 177.8647 - val_loss: 194.1427\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.2757 - val_loss: 176.7501\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 169.8393 - val_loss: 176.2969\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 180.9070 - val_loss: 179.2186\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.1874 - val_loss: 125.9510\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 172.9291 - val_loss: 162.1162\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 166.8742 - val_loss: 169.0769\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 168.1303 - val_loss: 152.6680\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 171.3792 - val_loss: 157.1672\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 187.7153 - val_loss: 160.3875\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 166.1690 - val_loss: 167.9494\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 171.5899 - val_loss: 143.9202\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 160.5628 - val_loss: 159.3935\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 167.1925 - val_loss: 166.7051\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 171.0416 - val_loss: 182.9422\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 163.2472 - val_loss: 163.6919\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 167.9052 - val_loss: 166.8091\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 162.2048 - val_loss: 172.2003\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 165.2766 - val_loss: 172.4090\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 181.9991 - val_loss: 163.8908\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 181.8344 - val_loss: 159.2451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 153.1650 - val_loss: 177.7548\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 176.0047 - val_loss: 149.0283\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 169.4473 - val_loss: 161.5229\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.7256 - val_loss: 137.8254\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 163.9327 - val_loss: 160.9850\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 165.5782 - val_loss: 158.5435\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 187.5423 - val_loss: 164.5866\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 162.5081 - val_loss: 148.7432\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.0024 - val_loss: 152.5681\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 177.9520 - val_loss: 191.9413\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 170.9597 - val_loss: 173.9554\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 156.6831 - val_loss: 190.6842\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 163.6461 - val_loss: 159.1654\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 187.9579 - val_loss: 172.8807\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 177.4083 - val_loss: 138.6339\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 191.2051 - val_loss: 156.6503\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 162.4455 - val_loss: 164.6700\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 160.3210 - val_loss: 194.2821\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 153.8625 - val_loss: 156.6601\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 150.0796 - val_loss: 202.9718\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 203.0673 - val_loss: 236.1830\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 179.1332 - val_loss: 183.4469\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 174.5612 - val_loss: 176.1328\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 160.7397 - val_loss: 156.2935\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 164.5068 - val_loss: 172.2107\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 169.3518 - val_loss: 201.3862\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 169.7306 - val_loss: 179.6006\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 168.1883 - val_loss: 165.5580\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.9186 - val_loss: 172.8322\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 158.2023 - val_loss: 85.2410\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 251.6656 - val_loss: 257.3576\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 172.7215 - val_loss: 161.1151\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 172.5335 - val_loss: 161.1392\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 167.7842 - val_loss: 169.5493\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 158.1513 - val_loss: 146.4058\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 169.8110 - val_loss: 154.1191\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 164.2658 - val_loss: 156.9649\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 176.0811 - val_loss: 179.3016\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 172.5278 - val_loss: 168.8980\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.4885 - val_loss: 175.4636\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 172.5442 - val_loss: 176.0549\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 163.2404 - val_loss: 160.2943\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 166.9734 - val_loss: 158.6447\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 134.6733 - val_loss: 188.4669\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 157.0928 - val_loss: 176.3248\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.5724 - val_loss: 172.3836\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.0418 - val_loss: 166.6855\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 151.6875 - val_loss: 194.3411\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 169.9689 - val_loss: 193.0704\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 146.0533 - val_loss: 183.4543\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 166.5854 - val_loss: 145.0663\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 157.9716 - val_loss: 172.6490\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 166.5891 - val_loss: 166.0316\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 189.9448 - val_loss: 184.1655\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 156.2587 - val_loss: 166.0872\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 179.8604 - val_loss: 174.6579\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 154.7424 - val_loss: 173.3421\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 168.7768 - val_loss: 150.9642\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 161.2131 - val_loss: 169.1415\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 171.4321 - val_loss: 145.7442\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 159.5589 - val_loss: 185.6894\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 167.3176 - val_loss: 181.6456\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 179.3296 - val_loss: 239.1884\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 241.6681 - val_loss: 165.8957\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 185.6382 - val_loss: 169.4001\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 200.9237 - val_loss: 157.9010\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 163.9904 - val_loss: 145.3593\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 168.9883 - val_loss: 177.9003\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 173.5688 - val_loss: 157.8363\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 191.9929 - val_loss: 155.5231\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.5494 - val_loss: 178.3842\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 174.2128 - val_loss: 134.1289\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 162.7015 - val_loss: 174.5342\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 155.2181 - val_loss: 148.5576\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 182.2926 - val_loss: 164.9292\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 157.2978 - val_loss: 179.0023\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 167.1727 - val_loss: 149.3239\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 178.1480 - val_loss: 166.1277\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 169.4423 - val_loss: 160.0500\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 172.1434 - val_loss: 172.3975\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 171.6490 - val_loss: 158.4214\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.9272 - val_loss: 141.3216\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 171.3117 - val_loss: 130.5138\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 163.1810 - val_loss: 158.7483\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 142.4973 - val_loss: 165.3699\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 152.9954 - val_loss: 168.7372\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 167.7165 - val_loss: 165.2556\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 176.5874 - val_loss: 147.5964\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.7008 - val_loss: 155.1270\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 162.5100 - val_loss: 164.7807\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 163.3173 - val_loss: 189.8753\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 179.2992 - val_loss: 160.7117\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 165.3376 - val_loss: 150.7632\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 168.7027 - val_loss: 180.8837\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 156.7170 - val_loss: 144.0471\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 165.1978 - val_loss: 170.9005\n",
            "----step:  700  ----loss: [<tf.Tensor 'pos_vae/Mean_1:0' shape=() dtype=float32>]\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 161.8643 - val_loss: 142.9359\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 157.5572 - val_loss: 194.0436\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 191.0559 - val_loss: 173.4961\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 96.9680 - val_loss: 79.2304\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 103.2625 - val_loss: 139.7480\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 173.0882 - val_loss: 168.9817\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.4233 - val_loss: 175.1192\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 176.4570 - val_loss: 165.5542\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 168.5907 - val_loss: 160.6935\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 183.5094 - val_loss: 163.0155\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 155.3272 - val_loss: 166.8203\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 156.0173 - val_loss: 157.5635\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 174.7803 - val_loss: 204.5669\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 185.0721 - val_loss: 177.9572\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 165.0477 - val_loss: 172.1636\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 172.5272 - val_loss: 171.4773\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 165.0361 - val_loss: 190.1123\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 162.0174 - val_loss: 180.0679\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 169.2541 - val_loss: 167.7919\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 172.0513 - val_loss: 164.5689\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 148.5315 - val_loss: 162.1782\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 171.6689 - val_loss: 189.8524\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.1944 - val_loss: 182.1983\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 171.8636 - val_loss: 162.7445\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 174.9837 - val_loss: 158.0689\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 157.8174 - val_loss: 164.0249\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 175.2987 - val_loss: 163.3554\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 174.1212 - val_loss: 177.6729\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.3209 - val_loss: 166.5560\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 173.1657 - val_loss: 159.1841\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 151.6809 - val_loss: 164.4637\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 188.3927 - val_loss: 187.9562\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 164.7484 - val_loss: 186.0756\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 163.2955 - val_loss: 179.4747\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 173.9332 - val_loss: 134.0568\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 163.7446 - val_loss: 148.3823\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 167.0238 - val_loss: 147.8992\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 169.9767 - val_loss: 169.9954\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 147.5147 - val_loss: 174.2294\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 170.5461 - val_loss: 162.1483\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 150.7053 - val_loss: 159.8670\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 178.5288 - val_loss: 144.5002\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 168.9781 - val_loss: 165.3194\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 161.8181 - val_loss: 182.4966\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 163.3337 - val_loss: 121.4493\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 162.3042 - val_loss: 168.3999\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 184.6916 - val_loss: 140.5580\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 167.4397 - val_loss: 181.3766\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 203.7462 - val_loss: 124.7845\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 163.9537 - val_loss: 155.7781\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 148.3077 - val_loss: 164.8348\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 161.6282 - val_loss: 184.8313\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 164.7103 - val_loss: 174.7144\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 148.0025 - val_loss: 143.1824\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 154.8414 - val_loss: 152.1322\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 164.8643 - val_loss: 170.6476\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 156.6138 - val_loss: 181.3010\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.7753 - val_loss: 153.8170\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 160.5476 - val_loss: 159.3194\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.9984 - val_loss: 141.9573\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 167.9794 - val_loss: 157.0000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 166.4109 - val_loss: 167.0455\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 170.3972 - val_loss: 151.0715\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 168.2122 - val_loss: 167.3744\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 151.8840 - val_loss: 166.6167\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 162.3097 - val_loss: 155.6783\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 179.7123 - val_loss: 140.5317\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 194.9653 - val_loss: 173.6658\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 243.9389 - val_loss: 180.8378\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 173.1891 - val_loss: 187.7132\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 169.5567 - val_loss: 160.5525\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 166.4412 - val_loss: 157.7875\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 168.3111 - val_loss: 172.4673\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 166.7385 - val_loss: 190.7950\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 165.0860 - val_loss: 183.7073\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 172.2512 - val_loss: 192.8248\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 168.7376 - val_loss: 164.7495\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 179.1917 - val_loss: 162.2096\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 173.0776 - val_loss: 161.1922\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 149.3598 - val_loss: 155.9406\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 152.1810 - val_loss: 175.3149\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 160.6123 - val_loss: 70.4897\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 161.4733 - val_loss: 146.2520\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 154.9685 - val_loss: 175.9481\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 170.1517 - val_loss: 184.9619\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 150.1887 - val_loss: 168.2167\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 169.4158 - val_loss: 170.7297\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 165.5595 - val_loss: 181.4854\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 162.5767 - val_loss: 178.9446\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 161.8082 - val_loss: 164.9500\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 166.6554 - val_loss: 157.1899\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.8255 - val_loss: 161.7850\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 146.7323 - val_loss: 170.4329\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 165.1456 - val_loss: 142.0338\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 168.7379 - val_loss: 150.6885\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 178.8990 - val_loss: 154.4845\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 172.5739 - val_loss: 195.0274\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 165.7811 - val_loss: 162.9491\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 206.0316 - val_loss: 150.3009\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 159.3240 - val_loss: 155.9986\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 168.9620 - val_loss: 180.7092\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 177.5002 - val_loss: 164.6921\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 155.1120 - val_loss: 166.9296\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 165.8601 - val_loss: 157.0421\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 170.1904 - val_loss: 186.7521\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 160.2993 - val_loss: 149.7536\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 165.7307 - val_loss: 159.2820\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 157.0573 - val_loss: 167.2214\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 165.0568 - val_loss: 160.6828\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 180.2732 - val_loss: 166.8289\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 162.9667 - val_loss: 184.5173\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.3715 - val_loss: 151.0189\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.1329 - val_loss: 168.6730\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 149.9533 - val_loss: 158.0579\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.5084 - val_loss: 168.6231\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 165.3783 - val_loss: 173.7342\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 162.4619 - val_loss: 178.6404\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 187.7003 - val_loss: 172.5580\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 162.5002 - val_loss: 167.0929\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 159.7472 - val_loss: 157.6096\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 170.5435 - val_loss: 151.1092\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 172.3170 - val_loss: 175.4546\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 180.8823 - val_loss: 179.0154\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 152.8854 - val_loss: 183.5683\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 167.9313 - val_loss: 183.9339\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 165.9221 - val_loss: 170.7963\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 143.4818 - val_loss: 185.0683\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 159.2401 - val_loss: 180.3024\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 164.9269 - val_loss: 174.5782\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 142.5108 - val_loss: 148.7097\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 153.2148 - val_loss: 178.7512\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 172.3793 - val_loss: 164.7134\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.6244 - val_loss: 164.1054\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 155.9766 - val_loss: 140.1619\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 131.9903 - val_loss: 159.1560\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 167.4899 - val_loss: 151.9661\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 155.5854 - val_loss: 147.9211\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 144.6841 - val_loss: 161.0890\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 172.1613 - val_loss: 206.5286\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.1540 - val_loss: 161.9984\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 184.8181 - val_loss: 158.8569\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 154.4634 - val_loss: 163.9973\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 166.4017 - val_loss: 169.7934\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 151.7246 - val_loss: 166.8106\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 162.3570 - val_loss: 183.1292\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 160.3565 - val_loss: 194.8288\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 199.2129 - val_loss: 158.4121\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 176.3356 - val_loss: 179.6803\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.4441 - val_loss: 162.3570\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 158.5541 - val_loss: 143.9927\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 159.3852 - val_loss: 142.8883\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 192.4582 - val_loss: 178.3638\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 161.5221 - val_loss: 169.6330\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 143.6002 - val_loss: 162.7106\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 188.8217 - val_loss: 157.6754\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.5515 - val_loss: 184.3664\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 162.0556 - val_loss: 139.6113\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 166.4442 - val_loss: 156.7931\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 156.9769 - val_loss: 155.0132\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 170.7509 - val_loss: 173.7359\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 165.8643 - val_loss: 170.0474\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 159.9821 - val_loss: 182.5831\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 159.8836 - val_loss: 173.4724\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 157.1415 - val_loss: 135.2555\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 172.1469 - val_loss: 168.6573\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 180.9311 - val_loss: 152.5201\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 165.6750 - val_loss: 156.1463\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 187.3525 - val_loss: 254.5136\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 163.4678 - val_loss: 162.4787\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 153.8510 - val_loss: 159.2259\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 169.1622 - val_loss: 166.6110\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 159.4135 - val_loss: 185.9927\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 165.3068 - val_loss: 170.4389\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 165.6108 - val_loss: 205.8448\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 150.9158 - val_loss: 137.3164\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 162.2244 - val_loss: 155.9332\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 160.9821 - val_loss: 186.0435\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 158.8362 - val_loss: 194.8764\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 173.9211 - val_loss: 158.7027\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 157.8730 - val_loss: 178.8611\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 152.1017 - val_loss: 190.0433\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 171.4759 - val_loss: 154.3658\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 168.2408 - val_loss: 163.9086\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 154.9526 - val_loss: 152.3041\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 154.6157 - val_loss: 180.2923\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 164.3488 - val_loss: 152.6693\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 171.7960 - val_loss: 154.2317\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 166.5915 - val_loss: 147.3979\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 164.9548 - val_loss: 188.6378\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 170.4325 - val_loss: 186.6186\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 161.1082 - val_loss: 172.5895\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 160.8237 - val_loss: 167.0830\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 168.7918 - val_loss: 158.1755\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 167.9842 - val_loss: 181.6570\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 163.7689 - val_loss: 180.6748\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 166.5594 - val_loss: 154.7443\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 179.5113 - val_loss: 175.2297\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 197.3041 - val_loss: 153.6372\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 170.1116 - val_loss: 174.3537\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 151.3009 - val_loss: 164.4233\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 160.5582 - val_loss: 158.7736\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 176.4249 - val_loss: 160.4466\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 167.8640 - val_loss: 192.3295\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 160.9595 - val_loss: 164.2622\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 163.5914 - val_loss: 162.8528\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 162.5195 - val_loss: 163.4328\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 138.1013 - val_loss: 156.3977\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 158.1529 - val_loss: 161.1870\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 174.5359 - val_loss: 163.9346\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 159.0349 - val_loss: 160.2125\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 164.2012 - val_loss: 216.0194\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 193.1124 - val_loss: 170.2026\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 167.8739 - val_loss: 168.6258\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 186.4113 - val_loss: 102.7949\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.5718 - val_loss: 146.8514\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 162.3082 - val_loss: 146.6004\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 164.6841 - val_loss: 201.6481\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 149.5359 - val_loss: 177.9867\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 170.6355 - val_loss: 165.8806\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 157.8219 - val_loss: 106.7796\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 142.8704 - val_loss: 165.0113\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 157.7498 - val_loss: 165.1775\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 174.9243 - val_loss: 163.3298\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 149.3090 - val_loss: 188.6464\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 166.0504 - val_loss: 119.8411\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 155.9809 - val_loss: 176.4765\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 170.6063 - val_loss: 185.2624\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 154.1413 - val_loss: 140.0499\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 157.2270 - val_loss: 182.1204\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 172.3922 - val_loss: 161.1999\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 157.5599 - val_loss: 178.4252\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 159.4258 - val_loss: 166.9038\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 161.1760 - val_loss: 160.3623\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 187.4051 - val_loss: 145.0750\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 189.2667 - val_loss: 140.8766\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 160.3291 - val_loss: 141.7697\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 162.3771 - val_loss: 151.2009\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 165.8809 - val_loss: 160.2348\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 155.9483 - val_loss: 177.6452\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 182.0510 - val_loss: 151.0842\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 158.3745 - val_loss: 166.7362\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.2824 - val_loss: 188.8577\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 163.1924 - val_loss: 164.0016\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 172.5575 - val_loss: 179.8730\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.6739 - val_loss: 114.0275\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 161.6928 - val_loss: 159.8446\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.7164 - val_loss: 159.2799\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 143.8944 - val_loss: 179.5909\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 160.5428 - val_loss: 164.6689\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 160.5283 - val_loss: 186.1597\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 163.8513 - val_loss: 170.6021\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 160.0979 - val_loss: 164.4480\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 170.9530 - val_loss: 174.1727\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 187.4744 - val_loss: 133.9494\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 162.3854 - val_loss: 187.8993\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 161.0400 - val_loss: 129.9876\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 160.5088 - val_loss: 151.6221\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 165.7178 - val_loss: 141.5487\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.2217 - val_loss: 167.9450\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 141.1421 - val_loss: 196.3294\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 153.4372 - val_loss: 167.5623\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 145.8647 - val_loss: 147.1167\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 148.8125 - val_loss: 155.9086\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 197.7540 - val_loss: 167.2005\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 171.8606 - val_loss: 157.8622\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 159.4001 - val_loss: 158.8539\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 156.1732 - val_loss: 165.2433\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 152.1942 - val_loss: 166.7690\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 165.2805 - val_loss: 159.3180\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.8713 - val_loss: 150.7438\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 156.0090 - val_loss: 164.5670\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 161.3875 - val_loss: 165.9039\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 165.3354 - val_loss: 180.9405\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 151.8777 - val_loss: 182.4240\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 159.2191 - val_loss: 175.8228\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 153.5967 - val_loss: 169.9956\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 141.8094 - val_loss: 170.9891\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 168.6802 - val_loss: 167.5405\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 185.1930 - val_loss: 144.8601\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 166.1689 - val_loss: 223.2156\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 162.2470 - val_loss: 172.3464\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 156.5410 - val_loss: 177.2897\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 164.2572 - val_loss: 183.1549\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 150.7253 - val_loss: 169.2189\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 160.0967 - val_loss: 141.3499\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 169.2084 - val_loss: 160.7175\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 164.2187 - val_loss: 157.6063\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.7514 - val_loss: 207.6717\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 158.2316 - val_loss: 122.8710\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 170.1667 - val_loss: 179.4104\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 170.4264 - val_loss: 195.2671\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 164.7433 - val_loss: 153.1384\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 164.5312 - val_loss: 193.7305\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 178.5114 - val_loss: 166.0285\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 157.0156 - val_loss: 150.2073\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 155.0757 - val_loss: 191.1422\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 128.7146 - val_loss: 163.7222\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 147.5281 - val_loss: 191.6810\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 168.2019 - val_loss: 174.4834\n",
            "----step:  999  ----loss: [<tf.Tensor 'pos_vae/Mean_1:0' shape=() dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwZL0omF8T67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3aa4bc6e-6a6d-4f47-9074-d05b6af9c70f"
      },
      "source": [
        "generator_neg = construct_model(emb_matrix_neg, name=\"neg_vae\")\n",
        "generator_neg_trained = train_model(generator_neg, negative_location, vocab_neg, \n",
        "                                    model_checkpoint_file_neg)"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 --------\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 735.5995 - val_loss: 731.2913\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 732.1522 - val_loss: 727.4366\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 728.3036 - val_loss: 720.9188\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 723.8104 - val_loss: 715.2971\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 717.9417 - val_loss: 710.3945\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 707.5708 - val_loss: 695.0495\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 692.7148 - val_loss: 681.9767\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 684.0609 - val_loss: 667.9986\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 663.7311 - val_loss: 656.1877\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 643.8498 - val_loss: 619.8660\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 632.1312 - val_loss: 605.0891\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 602.3452 - val_loss: 582.2066\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 582.9525 - val_loss: 558.9565\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 558.2261 - val_loss: 543.7061\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 545.9502 - val_loss: 506.6455\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 515.2245 - val_loss: 497.4078\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 486.7924 - val_loss: 475.1742\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 473.1094 - val_loss: 455.2549\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 432.3641 - val_loss: 413.4733\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 420.1352 - val_loss: 365.7641\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 399.6489 - val_loss: 376.2033\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 396.9989 - val_loss: 374.7440\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 359.7743 - val_loss: 370.4219\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 347.1582 - val_loss: 348.9942\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 332.5158 - val_loss: 322.5766\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 323.9026 - val_loss: 318.0909\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 325.9198 - val_loss: 317.3528\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 318.1445 - val_loss: 287.3068\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 313.1383 - val_loss: 296.4144\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 283.1905 - val_loss: 279.2249\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 334.3689 - val_loss: 362.5040\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 362.5024 - val_loss: 357.5439\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 290.5258 - val_loss: 268.1083\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 278.0832 - val_loss: 266.7285\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 266.6660 - val_loss: 259.6392\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 256.4618 - val_loss: 262.2464\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 265.8313 - val_loss: 269.8172\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 251.4203 - val_loss: 264.0316\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 255.5294 - val_loss: 270.4418\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 242.1721 - val_loss: 282.8745\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 272.6792 - val_loss: 248.6133\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 253.4271 - val_loss: 261.7352\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 263.2929 - val_loss: 249.3555\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 288.8836 - val_loss: 264.8747\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 261.8404 - val_loss: 282.9653\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 276.6002 - val_loss: 284.8627\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 268.3822 - val_loss: 310.0287\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 277.0550 - val_loss: 231.6510\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 254.5616 - val_loss: 269.4871\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 248.4538 - val_loss: 255.3114\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 258.7276 - val_loss: 269.1478\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 224.7961 - val_loss: 232.6934\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 213.2526 - val_loss: 248.4748\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 261.3987 - val_loss: 235.3842\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 252.1614 - val_loss: 241.5959\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 254.2198 - val_loss: 236.9599\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 285.1727 - val_loss: 258.0340\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 247.5270 - val_loss: 295.3552\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 243.0827 - val_loss: 252.0586\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 251.1485 - val_loss: 253.8326\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 226.9563 - val_loss: 240.4434\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 236.5551 - val_loss: 264.7653\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 255.9062 - val_loss: 263.1157\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 244.9033 - val_loss: 264.3462\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 234.3412 - val_loss: 229.2036\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 243.6090 - val_loss: 256.7674\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 231.8261 - val_loss: 258.6085\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 244.3807 - val_loss: 283.7194\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 249.9875 - val_loss: 218.7755\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 263.1590 - val_loss: 224.3074\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 257.3201 - val_loss: 245.1389\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 249.8724 - val_loss: 262.3294\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 247.6195 - val_loss: 238.4686\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 250.4102 - val_loss: 251.2912\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 246.0052 - val_loss: 261.1964\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 228.0786 - val_loss: 215.0716\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 249.9374 - val_loss: 230.3541\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 241.4811 - val_loss: 202.8751\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 215.3247 - val_loss: 245.7557\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 258.7317 - val_loss: 252.5698\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 242.1738 - val_loss: 207.6142\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 244.6917 - val_loss: 229.3217\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 251.7149 - val_loss: 254.0461\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 257.9567 - val_loss: 265.2350\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 253.1773 - val_loss: 239.5292\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 235.7853 - val_loss: 260.4744\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 210.1876 - val_loss: 198.5823\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 198.5828 - val_loss: 197.5235\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 197.5190 - val_loss: 195.9299\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 195.8798 - val_loss: 193.9726\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 193.9619 - val_loss: 191.8763\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 191.8832 - val_loss: 189.8246\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 189.8183 - val_loss: 187.8667\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 187.8848 - val_loss: 186.2147\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 186.1955 - val_loss: 184.7344\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 184.7597 - val_loss: 183.5636\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 183.6264 - val_loss: 182.7045\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 182.6724 - val_loss: 182.0116\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 182.0200 - val_loss: 181.5311\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 181.5631 - val_loss: 181.1891\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 181.1945 - val_loss: 180.8931\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 180.9279 - val_loss: 216.0024\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 272.3006 - val_loss: 276.5729\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 265.4933 - val_loss: 289.0073\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 283.0409 - val_loss: 276.9406\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 250.0938 - val_loss: 264.3833\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 261.2436 - val_loss: 308.3528\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 274.2747 - val_loss: 259.1162\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 266.0667 - val_loss: 245.5462\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 300.3167 - val_loss: 236.5289\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 259.5433 - val_loss: 266.1469\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 261.7095 - val_loss: 263.5372\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 271.8297 - val_loss: 258.8365\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 250.6217 - val_loss: 254.6393\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 253.5971 - val_loss: 242.6626\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 246.3722 - val_loss: 263.9837\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 256.3302 - val_loss: 241.4827\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 266.5273 - val_loss: 245.1152\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 239.9664 - val_loss: 252.0441\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 234.5958 - val_loss: 256.0604\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 250.1955 - val_loss: 219.0268\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 246.1276 - val_loss: 257.7881\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 242.3654 - val_loss: 243.7374\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 241.3052 - val_loss: 254.0046\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 241.5486 - val_loss: 237.2406\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 237.4597 - val_loss: 226.2813\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 241.5597 - val_loss: 223.5447\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 261.9969 - val_loss: 256.2072\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 233.1392 - val_loss: 241.6906\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 245.0326 - val_loss: 243.0714\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 249.6739 - val_loss: 244.1971\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 240.7372 - val_loss: 274.1016\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 235.3790 - val_loss: 244.4408\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 233.1037 - val_loss: 250.0350\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 254.0206 - val_loss: 260.3490\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 204.3782 - val_loss: 230.1342\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 233.7086 - val_loss: 234.9883\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 238.7371 - val_loss: 220.6358\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 240.2350 - val_loss: 247.9410\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 240.8746 - val_loss: 256.6166\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 240.0500 - val_loss: 233.0722\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 249.1486 - val_loss: 223.0186\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 263.3085 - val_loss: 253.2110\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 237.1277 - val_loss: 230.3583\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 241.4081 - val_loss: 238.9007\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 248.3978 - val_loss: 256.8594\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 225.3461 - val_loss: 244.9936\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 254.8787 - val_loss: 249.5213\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 269.0736 - val_loss: 237.3616\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 240.2549 - val_loss: 249.1542\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 240.5385 - val_loss: 267.1476\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 243.0760 - val_loss: 229.8867\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 281.3387 - val_loss: 252.5332\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 236.5341 - val_loss: 245.3070\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 242.7599 - val_loss: 229.3296\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 237.4251 - val_loss: 217.9436\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 239.8020 - val_loss: 239.7137\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 252.7172 - val_loss: 212.0325\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 244.8880 - val_loss: 248.5107\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 234.9014 - val_loss: 221.6071\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 248.0688 - val_loss: 244.8042\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 245.9836 - val_loss: 239.0824\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 232.3396 - val_loss: 276.6001\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 253.4021 - val_loss: 244.7328\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 238.5945 - val_loss: 259.2032\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 122.3100 - val_loss: 96.7903\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 220.3020 - val_loss: 204.8749\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 237.0404 - val_loss: 223.0299\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 236.3537 - val_loss: 193.9632\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 243.4327 - val_loss: 237.0284\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 236.5806 - val_loss: 221.1965\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 229.0236 - val_loss: 238.2892\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 243.2286 - val_loss: 229.6389\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 239.2488 - val_loss: 232.2545\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 250.2555 - val_loss: 244.1652\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 230.1124 - val_loss: 234.4498\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 238.6521 - val_loss: 284.9117\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 263.2175 - val_loss: 137.3726\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 235.7795 - val_loss: 234.9709\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 249.2199 - val_loss: 226.0606\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 226.8076 - val_loss: 241.6264\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 232.8132 - val_loss: 233.7459\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 227.7221 - val_loss: 231.0837\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 230.8829 - val_loss: 249.3389\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 249.7814 - val_loss: 217.9605\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 227.8058 - val_loss: 247.1604\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 249.6752 - val_loss: 238.5601\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 222.3474 - val_loss: 223.0842\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 222.9547 - val_loss: 156.2635\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 251.0704 - val_loss: 248.9790\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 255.7494 - val_loss: 237.5336\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 228.0581 - val_loss: 231.6345\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 239.0866 - val_loss: 208.5826\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 241.3373 - val_loss: 234.6403\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 246.6111 - val_loss: 233.8448\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 224.9167 - val_loss: 222.8626\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 222.6431 - val_loss: 230.7703\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 218.9446 - val_loss: 252.2658\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 235.2913 - val_loss: 220.5347\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 232.2805 - val_loss: 244.1350\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 237.3035 - val_loss: 216.0231\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 224.4713 - val_loss: 221.0732\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 227.3705 - val_loss: 223.3472\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 243.7286 - val_loss: 203.6588\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 217.5800 - val_loss: 223.8959\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 227.0158 - val_loss: 234.0423\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 241.3460 - val_loss: 210.4026\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 237.1923 - val_loss: 235.8013\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 232.5003 - val_loss: 214.2341\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 234.3745 - val_loss: 242.1383\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 231.9577 - val_loss: 238.2058\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 222.5794 - val_loss: 238.6756\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 305.4558 - val_loss: 255.9690\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 213.6968 - val_loss: 230.1488\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 237.9561 - val_loss: 245.7339\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 219.9486 - val_loss: 252.3485\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 225.9756 - val_loss: 241.9028\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 242.6944 - val_loss: 165.2935\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 220.5803 - val_loss: 231.0271\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 223.8353 - val_loss: 240.2938\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 223.3301 - val_loss: 233.6411\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 225.1619 - val_loss: 227.0042\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 228.2630 - val_loss: 215.9917\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 229.7061 - val_loss: 212.5859\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 225.6543 - val_loss: 248.6403\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 224.0594 - val_loss: 261.1337\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 225.0978 - val_loss: 214.8472\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 231.5618 - val_loss: 231.3876\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 217.9301 - val_loss: 247.9490\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 213.2126 - val_loss: 249.7453\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 233.5077 - val_loss: 221.1467\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 204.2395 - val_loss: 231.0925\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 226.0161 - val_loss: 232.2495\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 200.0011 - val_loss: 232.0591\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 204.1662 - val_loss: 213.2655\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 215.7889 - val_loss: 223.7977\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 229.2285 - val_loss: 208.1842\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 172.4612 - val_loss: 94.9433\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 211.2698 - val_loss: 225.6336\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 201.5102 - val_loss: 226.7430\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 226.1505 - val_loss: 220.1514\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 209.1692 - val_loss: 200.7981\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 195.8718 - val_loss: 235.0278\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 204.3312 - val_loss: 205.6297\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 192.2982 - val_loss: 261.9260\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 226.8918 - val_loss: 232.0511\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 217.8176 - val_loss: 211.2103\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 204.3812 - val_loss: 207.5991\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 189.0308 - val_loss: 238.7192\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 202.8693 - val_loss: 207.9612\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 213.9684 - val_loss: 213.0343\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 232.9335 - val_loss: 228.3655\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 197.7359 - val_loss: 195.9382\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 213.2686 - val_loss: 192.7384\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 215.9188 - val_loss: 203.4788\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 188.4846 - val_loss: 228.2765\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 240.4709 - val_loss: 183.0465\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 211.1138 - val_loss: 163.1509\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 219.8644 - val_loss: 199.5489\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 196.7608 - val_loss: 224.0595\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 194.4104 - val_loss: 188.1241\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 205.9949 - val_loss: 202.7058\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 208.3609 - val_loss: 214.6186\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 212.0217 - val_loss: 185.7147\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 198.9976 - val_loss: 203.2567\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 199.5108 - val_loss: 203.7937\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 187.1941 - val_loss: 199.4949\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 181.6553 - val_loss: 206.2792\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 195.3858 - val_loss: 191.6650\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.4466 - val_loss: 192.3495\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 200.0219 - val_loss: 220.5918\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 200.3067 - val_loss: 204.9478\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 205.9250 - val_loss: 201.7698\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 186.1896 - val_loss: 178.0469\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 161.3168 - val_loss: 73.1398\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 123.3854 - val_loss: 196.4796\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 198.1321 - val_loss: 179.8270\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 187.4163 - val_loss: 229.8413\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 194.5904 - val_loss: 175.2390\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 187.8526 - val_loss: 192.8222\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 182.4382 - val_loss: 195.5964\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 201.9210 - val_loss: 206.8621\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 115.3489 - val_loss: 181.8666\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.6290 - val_loss: 187.7544\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 186.5448 - val_loss: 188.3215\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 199.6377 - val_loss: 167.2012\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 183.4556 - val_loss: 188.9195\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.4014 - val_loss: 183.1866\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 197.0916 - val_loss: 192.1122\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 201.0307 - val_loss: 185.3718\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.1580 - val_loss: 209.5483\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 202.2861 - val_loss: 193.7909\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 195.2887 - val_loss: 213.3044\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 205.9382 - val_loss: 184.7019\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 203.0470 - val_loss: 193.7592\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 194.9810 - val_loss: 177.6919\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 181.6591 - val_loss: 194.6508\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 179.0905 - val_loss: 223.3031\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 193.3141 - val_loss: 200.2820\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 200.8125 - val_loss: 178.7046\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 176.3483 - val_loss: 181.3461\n",
            "----step:  300  ----loss: [<tf.Tensor 'neg_vae/Mean_1:0' shape=() dtype=float32>]\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 209.6155 - val_loss: 202.1713\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 178.7450 - val_loss: 194.2929\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 212.7797 - val_loss: 195.2439\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 190.3909 - val_loss: 199.4312\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 208.6236 - val_loss: 172.6344\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 185.3114 - val_loss: 209.1932\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.8623 - val_loss: 188.5657\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 195.3433 - val_loss: 181.9353\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 199.0727 - val_loss: 165.0692\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 196.6401 - val_loss: 193.5066\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 184.8378 - val_loss: 189.3033\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 199.1382 - val_loss: 178.0906\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.7883 - val_loss: 187.8951\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 172.6485 - val_loss: 158.7363\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 197.9429 - val_loss: 173.9859\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.3065 - val_loss: 171.6988\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.7175 - val_loss: 183.4444\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 185.9203 - val_loss: 201.4443\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 189.3960 - val_loss: 185.3593\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 210.2263 - val_loss: 170.9112\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.7766 - val_loss: 186.2568\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.2962 - val_loss: 178.4805\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.2452 - val_loss: 168.5016\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 181.5564 - val_loss: 176.0101\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 179.9821 - val_loss: 194.5985\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 170.4734 - val_loss: 166.0755\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 193.0693 - val_loss: 157.4034\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 189.2571 - val_loss: 200.9697\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 193.9322 - val_loss: 181.6516\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 175.5923 - val_loss: 192.6512\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 182.8105 - val_loss: 187.4865\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 200.6723 - val_loss: 186.7806\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 189.0501 - val_loss: 195.6012\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 188.2652 - val_loss: 158.4393\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 171.7871 - val_loss: 177.8556\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 190.9917 - val_loss: 172.1265\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 196.1502 - val_loss: 100.1149\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 162.6957 - val_loss: 199.1541\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 209.0704 - val_loss: 191.6283\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 189.5250 - val_loss: 190.7681\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 166.7477 - val_loss: 193.7534\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 192.7041 - val_loss: 176.0286\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.1247 - val_loss: 174.9107\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 158.0523 - val_loss: 99.6788\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 203.7430 - val_loss: 194.4927\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 169.3233 - val_loss: 200.1060\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 198.0354 - val_loss: 180.2675\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 191.5070 - val_loss: 201.5086\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.3653 - val_loss: 209.9939\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 185.2220 - val_loss: 200.4745\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 177.2219 - val_loss: 189.6396\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 172.8484 - val_loss: 178.0464\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 184.3384 - val_loss: 179.0911\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 184.4586 - val_loss: 184.6224\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 181.4679 - val_loss: 175.9035\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.9838 - val_loss: 216.8176\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 199.1466 - val_loss: 191.6807\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 177.0040 - val_loss: 224.6298\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 190.6599 - val_loss: 171.9684\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 174.0197 - val_loss: 181.3928\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 193.5176 - val_loss: 201.3586\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 186.5991 - val_loss: 185.1321\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 173.9719 - val_loss: 199.6941\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 184.8639 - val_loss: 178.5602\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 202.4795 - val_loss: 191.6563\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 179.8008 - val_loss: 164.5313\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 187.8025 - val_loss: 161.5002\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 189.8195 - val_loss: 178.3038\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 191.4580 - val_loss: 215.2553\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 185.8913 - val_loss: 174.8273\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 181.0355 - val_loss: 164.3133\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 175.2670 - val_loss: 200.0842\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.1168 - val_loss: 198.0857\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 186.2160 - val_loss: 213.5171\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 185.4728 - val_loss: 175.5638\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 173.8045 - val_loss: 177.9911\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 163.3733 - val_loss: 169.6825\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 157.4959 - val_loss: 207.0800\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 200.7134 - val_loss: 163.3938\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 186.9252 - val_loss: 174.5593\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 188.1698 - val_loss: 189.7755\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.0931 - val_loss: 177.2198\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 198.2026 - val_loss: 165.9131\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 81.1651 - val_loss: 93.1192\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 97.9527 - val_loss: 187.1041\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 184.1908 - val_loss: 185.7402\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.6519 - val_loss: 168.9680\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.6441 - val_loss: 186.1158\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 184.3917 - val_loss: 211.9800\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 187.0137 - val_loss: 201.2369\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 196.5504 - val_loss: 181.5160\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 180.6889 - val_loss: 180.1246\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 195.8740 - val_loss: 217.5411\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 192.4324 - val_loss: 199.4941\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 193.7658 - val_loss: 199.1333\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 152.6567 - val_loss: 143.0869\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 184.2267 - val_loss: 173.7498\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 172.0711 - val_loss: 173.0084\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 191.9540 - val_loss: 182.6445\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 169.2879 - val_loss: 189.7021\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 171.8369 - val_loss: 170.5472\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 197.0675 - val_loss: 189.1561\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 194.1171 - val_loss: 176.8955\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 192.2503 - val_loss: 150.1564\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 180.6814 - val_loss: 173.7855\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 160.4828 - val_loss: 175.3270\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 179.8007 - val_loss: 172.7479\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 179.2688 - val_loss: 188.1338\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 202.2962 - val_loss: 175.8008\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.1971 - val_loss: 219.0962\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.3644 - val_loss: 188.8786\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 131.7229 - val_loss: 185.5480\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 184.5272 - val_loss: 171.7479\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 177.3310 - val_loss: 147.0706\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 185.9027 - val_loss: 203.8516\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.2236 - val_loss: 178.4291\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 181.6957 - val_loss: 208.5144\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 194.8822 - val_loss: 258.2826\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.8963 - val_loss: 182.5187\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 213.0365 - val_loss: 169.9231\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.5032 - val_loss: 197.1866\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 137.1516 - val_loss: 215.2979\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 181.9494 - val_loss: 202.2844\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 172.5376 - val_loss: 181.6773\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 187.8559 - val_loss: 191.1418\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.9298 - val_loss: 187.9505\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 179.0476 - val_loss: 178.2222\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 189.7481 - val_loss: 183.3624\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 193.4104 - val_loss: 175.7926\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.8637 - val_loss: 245.7834\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 184.7632 - val_loss: 195.2983\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 189.5782 - val_loss: 157.5981\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 172.8921 - val_loss: 187.7950\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 187.8579 - val_loss: 212.0882\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.0189 - val_loss: 188.5716\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 167.3206 - val_loss: 182.7804\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 172.9223 - val_loss: 177.5627\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 197.4173 - val_loss: 165.0451\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 186.9120 - val_loss: 192.5458\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 152.5647 - val_loss: 167.2724\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 152.8434 - val_loss: 188.9435\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 185.4406 - val_loss: 166.1911\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 174.4299 - val_loss: 185.6518\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.1934 - val_loss: 171.4441\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 187.9491 - val_loss: 194.3177\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 170.7113 - val_loss: 150.2489\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.7556 - val_loss: 172.5973\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 179.7284 - val_loss: 146.6386\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 175.0405 - val_loss: 160.1283\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 180.4633 - val_loss: 203.1872\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 169.0420 - val_loss: 168.7434\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 184.2426 - val_loss: 231.5811\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 186.1693 - val_loss: 160.5530\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.6679 - val_loss: 211.9286\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 191.4326 - val_loss: 193.5305\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 184.7663 - val_loss: 129.4623\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 164.8533 - val_loss: 167.8293\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 189.4214 - val_loss: 159.9149\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 181.4062 - val_loss: 178.0703\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 166.5676 - val_loss: 210.8809\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 190.5791 - val_loss: 178.5263\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 168.0558 - val_loss: 180.5044\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 186.7861 - val_loss: 138.4177\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 197.2940 - val_loss: 185.1183\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 205.5928 - val_loss: 167.4764\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.0887 - val_loss: 186.1947\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.4874 - val_loss: 178.3167\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.5464 - val_loss: 193.5652\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 182.3629 - val_loss: 185.1514\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 189.5839 - val_loss: 183.1886\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 191.1990 - val_loss: 197.5270\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 187.3422 - val_loss: 168.7518\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 177.4092 - val_loss: 225.4402\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.6913 - val_loss: 181.6169\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 165.2734 - val_loss: 181.6954\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 193.2323 - val_loss: 164.2981\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 182.4901 - val_loss: 175.4373\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 180.0255 - val_loss: 211.7399\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 197.2735 - val_loss: 175.8466\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 185.0377 - val_loss: 168.7173\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 175.8793 - val_loss: 187.5388\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.8781 - val_loss: 185.0900\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 190.7094 - val_loss: 187.6219\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 161.6315 - val_loss: 166.4047\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 183.1297 - val_loss: 131.0783\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 204.6086 - val_loss: 180.9973\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.2060 - val_loss: 201.3161\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 189.0615 - val_loss: 178.4245\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.0539 - val_loss: 167.1313\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 189.9274 - val_loss: 165.1981\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 159.7659 - val_loss: 186.5095\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 205.0113 - val_loss: 246.4303\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 172.9067 - val_loss: 197.3263\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 182.3788 - val_loss: 205.6624\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.8603 - val_loss: 178.0194\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 171.7761 - val_loss: 170.7762\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 185.7416 - val_loss: 265.0033\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 183.9303 - val_loss: 188.5007\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.0499 - val_loss: 202.4151\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.2335 - val_loss: 223.0133\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 169.7656 - val_loss: 190.8718\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 160.0724 - val_loss: 171.9372\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 186.5184 - val_loss: 191.9566\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 169.9222 - val_loss: 164.9353\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 191.4474 - val_loss: 179.8891\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 171.9527 - val_loss: 172.5090\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 191.4760 - val_loss: 185.7545\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.1887 - val_loss: 172.2596\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 183.8641 - val_loss: 161.9805\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 176.2741 - val_loss: 169.9761\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 209.8140 - val_loss: 176.5929\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 165.3129 - val_loss: 164.1954\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 190.9966 - val_loss: 192.6033\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 189.9658 - val_loss: 160.7701\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 166.1068 - val_loss: 156.7076\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 214.4541 - val_loss: 151.5647\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 185.1113 - val_loss: 157.5656\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.9507 - val_loss: 156.0177\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 167.5133 - val_loss: 204.0723\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 213.9556 - val_loss: 209.7737\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 184.5999 - val_loss: 164.6385\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 176.6458 - val_loss: 179.3689\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 194.7024 - val_loss: 190.8778\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 195.9895 - val_loss: 170.8484\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 181.2466 - val_loss: 173.9281\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 158.7003 - val_loss: 175.5172\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 181.9720 - val_loss: 195.9066\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.4615 - val_loss: 184.6936\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 127.2031 - val_loss: 196.8983\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 181.9269 - val_loss: 182.5885\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.5803 - val_loss: 171.8480\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 196.7603 - val_loss: 167.2269\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 177.7471 - val_loss: 181.2852\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 177.5352 - val_loss: 200.4928\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 176.7434 - val_loss: 156.6378\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 184.8797 - val_loss: 186.3203\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 179.8189 - val_loss: 181.6774\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.3687 - val_loss: 188.4616\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.5118 - val_loss: 161.7184\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 180.5249 - val_loss: 197.9985\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 176.9544 - val_loss: 184.3216\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 191.2775 - val_loss: 210.8530\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 203.4399 - val_loss: 203.4775\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 172.3884 - val_loss: 194.7619\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.4304 - val_loss: 191.7659\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 193.2645 - val_loss: 144.4900\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 169.9012 - val_loss: 198.2845\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 168.8855 - val_loss: 187.4607\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 193.2428 - val_loss: 190.8974\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 165.4556 - val_loss: 172.2720\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 176.4965 - val_loss: 183.0724\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.4504 - val_loss: 195.2684\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 155.8340 - val_loss: 180.4540\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 172.1137 - val_loss: 202.2096\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 191.0168 - val_loss: 191.4410\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 167.2100 - val_loss: 238.4845\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 143.9662 - val_loss: 190.7906\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 180.6347 - val_loss: 182.5479\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 186.4395 - val_loss: 185.4505\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 168.6970 - val_loss: 185.9286\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 175.7037 - val_loss: 162.5003\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 180.0217 - val_loss: 180.5558\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 136.8490 - val_loss: 51.1214\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 181.3354 - val_loss: 194.5020\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 187.7505 - val_loss: 195.2958\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 178.8855 - val_loss: 184.0080\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 142.0819 - val_loss: 184.2667\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 194.5244 - val_loss: 158.9880\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 192.9309 - val_loss: 168.0827\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 203.9568 - val_loss: 209.2629\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 153.2236 - val_loss: 178.4068\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 175.0736 - val_loss: 203.5976\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 211.7746 - val_loss: 187.7723\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 171.5093 - val_loss: 190.2798\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 179.4057 - val_loss: 188.4931\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 178.7752 - val_loss: 198.0194\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 177.6494 - val_loss: 183.0134\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 177.4962 - val_loss: 183.6758\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.4772 - val_loss: 185.0387\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 176.2286 - val_loss: 189.8619\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 177.8108 - val_loss: 156.5630\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 186.4651 - val_loss: 166.2389\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 176.0382 - val_loss: 164.9951\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 188.2756 - val_loss: 209.6461\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.7328 - val_loss: 190.5855\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 182.7191 - val_loss: 189.3832\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 193.3296 - val_loss: 189.5605\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 188.4549 - val_loss: 136.7538\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 184.2331 - val_loss: 177.2714\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 179.3964 - val_loss: 185.7965\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 180.9211 - val_loss: 163.9748\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 180.2519 - val_loss: 170.2051\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 202.7664 - val_loss: 167.7261\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 175.6303 - val_loss: 185.7896\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.9157 - val_loss: 157.3690\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 166.4229 - val_loss: 172.8578\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 178.6700 - val_loss: 172.4174\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 179.8936 - val_loss: 193.4754\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 173.4425 - val_loss: 176.2461\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 177.7592 - val_loss: 181.3267\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 175.2597 - val_loss: 183.9517\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.6956 - val_loss: 180.5213\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 192.1935 - val_loss: 176.0092\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 195.3297 - val_loss: 172.8318\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 160.6383 - val_loss: 192.3878\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 187.9283 - val_loss: 158.8091\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 180.4372 - val_loss: 172.5413\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 191.9933 - val_loss: 147.4310\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 174.1829 - val_loss: 174.3213\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 178.3171 - val_loss: 168.6012\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 199.8196 - val_loss: 180.7255\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.8711 - val_loss: 162.1369\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 190.5295 - val_loss: 164.8374\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 189.7820 - val_loss: 204.3524\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 182.6157 - val_loss: 179.3410\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.4904 - val_loss: 205.7933\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.1167 - val_loss: 171.5512\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 198.5338 - val_loss: 183.8337\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 189.4721 - val_loss: 145.7317\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 200.1973 - val_loss: 166.5799\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 175.3849 - val_loss: 178.2971\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 171.0005 - val_loss: 205.3918\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 164.8474 - val_loss: 169.0859\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 159.4365 - val_loss: 219.3462\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 218.5755 - val_loss: 258.9444\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 189.3374 - val_loss: 190.9376\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 186.6017 - val_loss: 188.8515\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 173.7595 - val_loss: 167.1927\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 175.1264 - val_loss: 182.8931\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 179.2380 - val_loss: 214.9892\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 184.5076 - val_loss: 192.5114\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 180.8196 - val_loss: 175.9538\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 179.4597 - val_loss: 183.7301\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 168.5571 - val_loss: 88.5776\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 271.5255 - val_loss: 278.2689\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 185.1666 - val_loss: 171.3172\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 183.5568 - val_loss: 174.8420\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 179.9578 - val_loss: 181.1182\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 171.1202 - val_loss: 155.1707\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.9731 - val_loss: 167.6238\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 177.7350 - val_loss: 171.1365\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 188.7525 - val_loss: 202.5045\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 189.4020 - val_loss: 183.5073\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 184.8435 - val_loss: 187.5670\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 185.3292 - val_loss: 186.6167\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.6167 - val_loss: 177.4831\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 180.6787 - val_loss: 180.8807\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 144.2585 - val_loss: 201.4561\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 170.9536 - val_loss: 185.3674\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 188.2177 - val_loss: 185.2911\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 177.4720 - val_loss: 182.1905\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 162.1280 - val_loss: 209.7554\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 183.2717 - val_loss: 203.8906\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 153.5358 - val_loss: 199.7400\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 181.2490 - val_loss: 156.4896\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 166.6167 - val_loss: 184.1089\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.9075 - val_loss: 174.4571\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 202.8581 - val_loss: 198.5407\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 166.4555 - val_loss: 178.4036\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 192.1318 - val_loss: 187.8654\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 166.5985 - val_loss: 185.9631\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 181.6200 - val_loss: 164.3161\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 171.4960 - val_loss: 182.9421\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.1644 - val_loss: 155.9507\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 171.7249 - val_loss: 198.8560\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 177.5980 - val_loss: 197.3489\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 194.4038 - val_loss: 255.7405\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 257.5274 - val_loss: 176.8909\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 196.4740 - val_loss: 181.7084\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 213.6967 - val_loss: 164.5235\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 176.4620 - val_loss: 158.4657\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 181.4101 - val_loss: 191.7907\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 184.0912 - val_loss: 169.4466\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 203.9684 - val_loss: 165.0905\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 186.7351 - val_loss: 188.4556\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 185.7625 - val_loss: 145.7050\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 174.4320 - val_loss: 187.5683\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 166.9281 - val_loss: 161.1602\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 194.7519 - val_loss: 173.8859\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 168.5490 - val_loss: 189.8862\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 180.7396 - val_loss: 164.2525\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 189.5228 - val_loss: 180.0820\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 181.9958 - val_loss: 174.8692\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 185.2818 - val_loss: 185.0908\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 182.6454 - val_loss: 169.6854\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 191.5156 - val_loss: 153.6030\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 186.2098 - val_loss: 140.0434\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 175.0891 - val_loss: 169.0905\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 151.2731 - val_loss: 181.7604\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 169.5492 - val_loss: 180.6565\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.8227 - val_loss: 178.1416\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 192.5322 - val_loss: 163.9234\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 187.3990 - val_loss: 171.2068\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 175.2066 - val_loss: 171.7144\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 174.8219 - val_loss: 201.5149\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 192.3926 - val_loss: 174.6375\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 174.2543 - val_loss: 164.3453\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 182.4905 - val_loss: 188.7633\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 168.9205 - val_loss: 152.4798\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 179.1386 - val_loss: 185.1933\n",
            "----step:  700  ----loss: [<tf.Tensor 'neg_vae/Mean_1:0' shape=() dtype=float32>]\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 171.3736 - val_loss: 157.5749\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 168.5743 - val_loss: 210.6928\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 205.2606 - val_loss: 180.3904\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 93.9595 - val_loss: 71.8422\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 96.4963 - val_loss: 155.0614\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 184.9520 - val_loss: 181.4090\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 186.8455 - val_loss: 189.9641\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 189.1201 - val_loss: 179.7850\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 186.1522 - val_loss: 173.7519\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 193.6807 - val_loss: 173.0578\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 163.2272 - val_loss: 181.0529\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 165.9405 - val_loss: 162.5027\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 183.2469 - val_loss: 210.1418\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 194.1348 - val_loss: 189.9958\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 178.5173 - val_loss: 185.7144\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 184.9539 - val_loss: 187.7254\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 176.2283 - val_loss: 203.1595\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 173.3956 - val_loss: 187.2189\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 179.5350 - val_loss: 176.2856\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 184.0713 - val_loss: 177.7016\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 157.8955 - val_loss: 179.2589\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.9792 - val_loss: 203.7284\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 189.8652 - val_loss: 191.8988\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 183.5596 - val_loss: 168.5543\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 186.9628 - val_loss: 170.8945\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 170.7667 - val_loss: 177.7892\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 186.9612 - val_loss: 177.2604\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 188.9317 - val_loss: 191.6778\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.7607 - val_loss: 179.3367\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.4160 - val_loss: 171.8151\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 164.2421 - val_loss: 178.4591\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 200.5118 - val_loss: 201.7852\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 177.1529 - val_loss: 200.9487\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.4357 - val_loss: 192.6065\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 186.2116 - val_loss: 144.6404\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.7764 - val_loss: 163.1448\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 179.7492 - val_loss: 159.9146\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 180.9290 - val_loss: 180.8023\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 155.8244 - val_loss: 185.1405\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 184.1843 - val_loss: 175.9447\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 162.8934 - val_loss: 176.6197\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 193.0638 - val_loss: 160.1057\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 180.2160 - val_loss: 175.7111\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 171.6005 - val_loss: 197.8489\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.7636 - val_loss: 134.6815\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.9742 - val_loss: 183.3774\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 197.7864 - val_loss: 150.4435\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 179.3111 - val_loss: 190.1474\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 216.9667 - val_loss: 134.5662\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 177.3630 - val_loss: 170.6438\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 159.6531 - val_loss: 180.2776\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 177.0374 - val_loss: 196.9788\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 178.5161 - val_loss: 186.4059\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 160.3050 - val_loss: 153.4217\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 165.4656 - val_loss: 167.4566\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.9583 - val_loss: 183.0053\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 168.0011 - val_loss: 187.9039\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 186.4547 - val_loss: 164.6355\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 172.8692 - val_loss: 172.9510\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 189.0401 - val_loss: 155.1077\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 177.6326 - val_loss: 168.2906\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 178.2619 - val_loss: 177.7651\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 184.1314 - val_loss: 160.0408\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 182.2691 - val_loss: 181.1066\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 161.3637 - val_loss: 180.8327\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 174.3737 - val_loss: 167.0242\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 191.3853 - val_loss: 146.0480\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 206.5992 - val_loss: 184.1282\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 255.7727 - val_loss: 197.0643\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 186.7952 - val_loss: 204.4284\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 181.5656 - val_loss: 173.6540\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 177.3846 - val_loss: 167.4919\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 180.4483 - val_loss: 188.6984\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 178.5193 - val_loss: 205.8134\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.5021 - val_loss: 199.3616\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 186.5601 - val_loss: 204.9317\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 180.3105 - val_loss: 176.7430\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 194.6069 - val_loss: 179.5881\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.2796 - val_loss: 173.0840\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 162.2923 - val_loss: 168.7063\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 163.2859 - val_loss: 187.8595\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 172.5746 - val_loss: 86.9491\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.0753 - val_loss: 159.7661\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 165.0885 - val_loss: 189.4125\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.1410 - val_loss: 198.6519\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 160.3609 - val_loss: 181.4909\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 183.3217 - val_loss: 186.2410\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 179.7347 - val_loss: 190.9256\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 174.9254 - val_loss: 190.3382\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 172.5173 - val_loss: 180.0377\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.7956 - val_loss: 173.8340\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 190.9468 - val_loss: 172.8281\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 157.1758 - val_loss: 176.6426\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 175.7353 - val_loss: 153.1948\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 181.9939 - val_loss: 167.8609\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 188.7210 - val_loss: 168.2095\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 181.8185 - val_loss: 203.5109\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 175.9060 - val_loss: 169.3172\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 219.2688 - val_loss: 162.9254\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 171.9982 - val_loss: 164.9863\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 182.9010 - val_loss: 193.1875\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 188.0968 - val_loss: 178.2310\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 166.7709 - val_loss: 177.7485\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 176.0561 - val_loss: 166.7609\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.4016 - val_loss: 200.7064\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 172.1546 - val_loss: 163.4658\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 177.6797 - val_loss: 172.8347\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 169.7647 - val_loss: 177.1792\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 177.8347 - val_loss: 175.0350\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 194.8136 - val_loss: 182.9864\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 174.9589 - val_loss: 196.9099\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 196.6918 - val_loss: 160.2670\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 187.3857 - val_loss: 181.1771\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 164.1236 - val_loss: 175.5530\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 195.2268 - val_loss: 180.6731\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 178.3066 - val_loss: 184.9803\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 175.1278 - val_loss: 189.6946\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 200.3649 - val_loss: 182.1927\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 173.7775 - val_loss: 182.5971\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 172.5263 - val_loss: 173.9593\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 181.1711 - val_loss: 166.2229\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 184.9678 - val_loss: 184.9315\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 192.9368 - val_loss: 190.3512\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 165.3720 - val_loss: 196.1875\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 179.4855 - val_loss: 200.3947\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 176.5869 - val_loss: 184.0490\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 153.7370 - val_loss: 199.2406\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 170.8149 - val_loss: 196.3481\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 177.7877 - val_loss: 182.7818\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 152.7181 - val_loss: 164.3244\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 164.6081 - val_loss: 195.9643\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 182.1176 - val_loss: 175.9211\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 184.7108 - val_loss: 173.5130\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 166.5950 - val_loss: 149.2085\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 139.6730 - val_loss: 173.4331\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 181.2454 - val_loss: 159.8356\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 165.7432 - val_loss: 161.8953\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 154.1117 - val_loss: 178.6911\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 188.3130 - val_loss: 224.4187\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 196.9304 - val_loss: 175.2522\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 201.2336 - val_loss: 175.2799\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 166.7709 - val_loss: 176.9995\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 178.5910 - val_loss: 188.1758\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 165.9498 - val_loss: 178.4769\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.5666 - val_loss: 199.8689\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 173.5814 - val_loss: 213.2145\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 217.9645 - val_loss: 180.3407\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 190.8064 - val_loss: 195.4907\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 189.8067 - val_loss: 175.9155\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 172.9895 - val_loss: 158.2361\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 172.5085 - val_loss: 155.6431\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 206.8237 - val_loss: 191.0488\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 173.5086 - val_loss: 182.9079\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 155.4109 - val_loss: 177.3571\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 203.1948 - val_loss: 167.9504\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 191.7962 - val_loss: 198.2506\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.6041 - val_loss: 149.0759\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.7661 - val_loss: 169.0082\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 168.7270 - val_loss: 170.1604\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 184.4357 - val_loss: 185.2661\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 177.0106 - val_loss: 183.0564\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 171.2636 - val_loss: 199.9436\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 173.4656 - val_loss: 189.5042\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 167.9954 - val_loss: 145.3625\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 185.9023 - val_loss: 175.8924\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 191.1327 - val_loss: 165.6600\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 178.1637 - val_loss: 169.8900\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 199.4546 - val_loss: 263.6967\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 174.5427 - val_loss: 172.8985\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 165.9256 - val_loss: 167.7434\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 181.5666 - val_loss: 179.4435\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 171.4560 - val_loss: 199.4206\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 177.5942 - val_loss: 185.6353\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 178.2286 - val_loss: 221.8440\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 163.3667 - val_loss: 145.7530\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 172.7508 - val_loss: 168.5108\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 171.8909 - val_loss: 202.7145\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 170.9070 - val_loss: 211.8266\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 187.2372 - val_loss: 169.0222\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 170.3098 - val_loss: 190.5276\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 164.0046 - val_loss: 203.6209\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 186.6472 - val_loss: 171.3020\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 182.2402 - val_loss: 181.2081\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 166.2384 - val_loss: 157.4661\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 166.3378 - val_loss: 191.9292\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 177.9364 - val_loss: 165.1355\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 182.5575 - val_loss: 168.1525\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 179.5489 - val_loss: 158.8708\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 177.1166 - val_loss: 202.5773\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 183.2048 - val_loss: 200.0153\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.4516 - val_loss: 188.2449\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 173.5642 - val_loss: 181.5064\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 181.8348 - val_loss: 165.5067\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 179.1469 - val_loss: 199.9950\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.2388 - val_loss: 197.5718\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 179.7706 - val_loss: 165.4509\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 193.6709 - val_loss: 188.2945\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 209.7109 - val_loss: 163.4453\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.7909 - val_loss: 187.6271\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 163.3189 - val_loss: 176.5575\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 171.9514 - val_loss: 172.4518\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 190.4814 - val_loss: 171.8547\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 182.2891 - val_loss: 209.7737\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.5240 - val_loss: 177.2386\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.0265 - val_loss: 176.2160\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 175.3785 - val_loss: 176.8803\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 150.1100 - val_loss: 168.5753\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 172.1243 - val_loss: 169.4967\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 186.7111 - val_loss: 175.1839\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 173.4654 - val_loss: 174.8983\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 176.2073 - val_loss: 228.0700\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 205.0959 - val_loss: 186.4541\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 180.0753 - val_loss: 180.4206\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 201.3759 - val_loss: 110.1195\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 194.6624 - val_loss: 156.3957\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 175.0334 - val_loss: 160.3072\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 176.5270 - val_loss: 216.5917\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 161.3873 - val_loss: 192.1232\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 185.0991 - val_loss: 177.7380\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 172.0400 - val_loss: 115.8235\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 153.8761 - val_loss: 177.4742\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 169.8357 - val_loss: 179.6834\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 188.2440 - val_loss: 176.9998\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 159.3650 - val_loss: 205.1401\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 180.5567 - val_loss: 130.8826\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 168.4271 - val_loss: 193.0639\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 182.5187 - val_loss: 200.2502\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 163.7230 - val_loss: 150.0955\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 170.7591 - val_loss: 196.9350\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 185.5318 - val_loss: 173.5751\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 170.4268 - val_loss: 195.1745\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 172.0717 - val_loss: 176.8652\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 172.8146 - val_loss: 173.6404\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 201.2304 - val_loss: 159.0962\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 202.9747 - val_loss: 150.6456\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 171.4480 - val_loss: 151.9246\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 174.0629 - val_loss: 167.7268\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 180.8445 - val_loss: 173.4318\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 167.9600 - val_loss: 190.4049\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 196.3531 - val_loss: 161.6266\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 170.7534 - val_loss: 181.2710\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 178.7397 - val_loss: 207.5260\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 174.3065 - val_loss: 175.1285\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 184.9262 - val_loss: 195.0579\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 188.1514 - val_loss: 124.3114\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 174.8568 - val_loss: 172.0579\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 187.8820 - val_loss: 173.4233\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 154.8810 - val_loss: 192.2414\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 171.4396 - val_loss: 178.2360\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 173.9310 - val_loss: 201.2303\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.2482 - val_loss: 181.0138\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 172.5099 - val_loss: 174.1057\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 182.5072 - val_loss: 190.9001\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 200.2017 - val_loss: 150.3275\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 173.2119 - val_loss: 199.9512\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 173.6781 - val_loss: 137.3492\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 173.9831 - val_loss: 160.9556\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 178.3606 - val_loss: 155.6984\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 185.5473 - val_loss: 181.1225\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 151.2906 - val_loss: 206.8744\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 162.6104 - val_loss: 179.0062\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 156.4420 - val_loss: 165.0329\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 158.9373 - val_loss: 169.1825\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 218.1803 - val_loss: 177.1820\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 184.4362 - val_loss: 167.6419\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 171.7022 - val_loss: 177.8264\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 171.1765 - val_loss: 181.5294\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 163.8544 - val_loss: 177.7161\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.3204 - val_loss: 171.6828\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 193.8637 - val_loss: 169.0239\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 167.8280 - val_loss: 182.2785\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 175.1892 - val_loss: 176.8391\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 176.5647 - val_loss: 193.6278\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 164.4688 - val_loss: 198.9322\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 172.1718 - val_loss: 196.7377\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 165.6722 - val_loss: 180.6030\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 153.6148 - val_loss: 182.6598\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 183.3958 - val_loss: 180.4384\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 200.3309 - val_loss: 161.9312\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 178.0772 - val_loss: 239.4522\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 174.8169 - val_loss: 185.6580\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 168.6220 - val_loss: 192.4992\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 175.6271 - val_loss: 200.5487\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 162.3485 - val_loss: 182.4963\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 170.8554 - val_loss: 151.6708\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 183.7537 - val_loss: 173.7884\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 175.1241 - val_loss: 169.4752\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 187.7390 - val_loss: 224.2522\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 171.7136 - val_loss: 134.9837\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 181.6036 - val_loss: 189.6600\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 186.1599 - val_loss: 208.9496\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 177.5954 - val_loss: 165.0454\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 175.9687 - val_loss: 212.5664\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 192.1339 - val_loss: 178.0706\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 169.0727 - val_loss: 160.2883\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 168.8444 - val_loss: 205.7846\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 138.3154 - val_loss: 174.3651\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 160.5919 - val_loss: 205.4201\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 180.1057 - val_loss: 193.0202\n",
            "----step:  999  ----loss: [<tf.Tensor 'neg_vae/Mean_1:0' shape=() dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXVbeBo_xB1G",
        "colab_type": "text"
      },
      "source": [
        "# Tests \n",
        "We are now going to use the trained model to do sentence interpolation, but we need a few helper functions to do so "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sd277FwyN7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word_pos = {v:k for k,v in vocab_pos.items()}\n",
        "index2word_neg = {v:k for k,v in vocab_neg.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDr6It-xFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_sampled_sentence(sentence_vector, model, index2word, latent_dim=LATENT_DIM, max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Uses the trained model\n",
        "  @:param: sentence vector: latent space representation of a sentence\n",
        "      (this should be the output of predict_latent_mean)\n",
        "\n",
        "  \"\"\"\n",
        "  N = len(index2word.keys())+1\n",
        "  sentence_vector = np.reshape(sentence_vector,[1,latent_dim]) # reshaping into 1 x latent space, where 1 is the batch size\n",
        "  generated = tf.keras.activation.softmax(model.predict_sentence(sentence_vector))\n",
        "  generated = np.reshape(generated,[max_len,N]) # reshaping into sequence length x vocabulary words \n",
        "  generated_indices = np.apply_along_axis(np.argmax, 1, generated)\n",
        "  word_list = list(np.vectorize(index2word.get)(generated_indices))\n",
        "  w_list = [w for w in word_list if w] # filtering out the words\n",
        "  print(' '.join(w_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0oG1Ll0XgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shortest_homologies(point1,point2,n):\n",
        "  \"\"\"\n",
        "  Discover n mid-way  points in the path between point1 and point2.\n",
        "  The name of the functions is due to the fact that the points are in the \n",
        "  latent space.\n",
        "  \"\"\"\n",
        "  dist_vec = point2 - point1\n",
        "  sample = np.linspace(0, 1, n, endpoint = True)\n",
        "  samples = []\n",
        "  for s in sample:\n",
        "      samples.append(point1 + s * dist_vec)\n",
        "  return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQU2-HVzIOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_interpolation(sentence1, sentence2, n, \n",
        "                            vocab, model, index2word, \n",
        "                            max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Interpolating between the two given sentences in n steps.\n",
        "  \"\"\"\n",
        "  sequence1 = sentence_to_sequence(sentence1, vocab)\n",
        "  sequence1 = pad_sequences(sequence1, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  sequence2 = sentence_to_sequence(sentence2, vocab)\n",
        "  sequence2 = pad_sequences(sequence2, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  latent1 = model.predict_latent_mean(sequence1)\n",
        "  latent2 = model.predict_latent_mean(sequence2)\n",
        "  homologies = shortest_homologies(latent1, latent2, n)\n",
        "  for latent_sentence in homologies:\n",
        "    print_sampled_sentence(latent_sentence, model, index2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYhD3Q_8oeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e2e4507d-4fd8-4855-8e46-21420879d464"
      },
      "source": [
        "sentence1 = [\"I think machine learning is great\"]\n",
        "sentence2 = [\"Do you want a new book\"]\n",
        "print(\"Positive sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_pos,generator_pos, index2word_pos)\n",
        "print(\"Negative sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_neg,generator_neg, index2word_neg)"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive sentence interpolation\n",
            "tf.Tensor([], shape=(0, 64), dtype=float32)\n",
            "[<tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>, <tf.Tensor: shape=(0, 64), dtype=float32, numpy=array([], shape=(0, 64), dtype=float32)>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-374-c0086a651488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Positive sentence interpolation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences_interpolation(sentence1,sentence2,10,\n\u001b[0;32m----> 5\u001b[0;31m                         vocab_pos,generator_pos,index2word_pos)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative sentence interpolation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m sentences_interpolation(sentence1,sentence2,10,\n",
            "\u001b[0;32m<ipython-input-373-0144f4b86e56>\u001b[0m in \u001b[0;36msentences_interpolation\u001b[0;34m(sentence1, sentence2, n, vocab, model, index2word, max_len)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlatent_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhomologies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomologies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint_sampled_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-363-c421b990cafe>\u001b[0m in \u001b[0;36mprint_sampled_sentence\u001b[0;34m(sentence_vector, model, index2word, latent_dim, max_len)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[1;32m      8\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0msentence_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshaping into 1 x latent space, where 1 is the batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshaping into sequence length x vocabulary words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diUSya8k9XyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
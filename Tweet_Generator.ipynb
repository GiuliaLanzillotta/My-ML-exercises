{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Generator.ipynb",
      "provenance": [],
      "mount_file_id": "12MHNh7F8wxZkVWHEuDkvARP1SNU4bpcX",
      "authorship_tag": "ABX9TyNbPQ9cwxS1k2r7Q5Y9qprO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Tweet_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMqVaBwSPA",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of **Generating sentences from a continuous space paper**\n",
        "Here's a link to the [paper](https://arxiv.org/pdf/1511.06349v4.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCPxyfX9r53",
        "colab_type": "text"
      },
      "source": [
        "> ### The goal \n",
        " What I want to explore here is the expression of sentiment in generative models. <br>\n",
        " The dataset consists of two different samples of tweets, one with positive sentiment and one with negative sentiment. <br>\n",
        " The goal is to train two generators on the two sets separetly and analyse the qualitative differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwkLCb83wngS",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSehtnM7dGh",
        "colab_type": "code",
        "outputId": "97349fce-fcc6-4c53-9916-f99d9696bd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3kXO6vvwm9f",
        "colab_type": "code",
        "outputId": "8fbe3e6d-88e2-4990-b04e-3334b681eb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RnhXHWxKGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxiSqdJQxPvj",
        "colab_type": "text"
      },
      "source": [
        "An example of the raw data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR4lq8bJxKNx",
        "colab_type": "code",
        "outputId": "36269647-3f16-4103-b36c-adad34198108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW_iOL4O78mD",
        "colab_type": "code",
        "outputId": "efeef391-465a-43fc-c815-ef6d9b5e1a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzumvsqowr8M",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di8j0afxw3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJp435CzVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from collections import Counter\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StWI7_EYwbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Transforms the specified files in tokens using the Twitter tokenizer.\n",
        "    @:params: str\n",
        "        Input text to tokenize\n",
        "    @:return: list(str)\n",
        "        Returns the tokens as a list of strings.\n",
        "    \"\"\"\n",
        "    tokenizer = TweetTokenizer()\n",
        "    # tokenizing the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [w.lower() for w in tokens]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2SswBFb7ut3",
        "colab_type": "text"
      },
      "source": [
        "Now we build a vocabulary with the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Gwr0Ex7zeP",
        "colab_type": "code",
        "outputId": "885c063e-49c1-4e95-dbb3-665bdd5c1109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "frequency_treshold = 100000*0.0001 # 0.1% of the tweets should contain the \"frequent words\"\n",
        "frequency_treshold"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-eecHHzzQJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(file_name, output_file_name):\n",
        "  \"\"\"\n",
        "  Builds the vocabulary for the specified file.\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  print(\"Reading \",file_name)\n",
        "  raw = open(file_name,  \"r\").read()\n",
        "  more_words = tokenize_text(raw)\n",
        "  words.extend(more_words)\n",
        "  # counting the words\n",
        "  counter = Counter(words)\n",
        "  words_count = dict(counter)\n",
        "  # filtering \n",
        "  filtered_words = [k for k, v in words_count.items() if v >= frequency_treshold]\n",
        "  # building voabulary \n",
        "  # index starts at 1 so that the 0 can be left for the empty spaces\n",
        "  vocab = {k:i+1 for i,k in enumerate(filtered_words)}\n",
        "  # saving the vocabulary \n",
        "  with open(output_file_name, 'wb') as f: \n",
        "      pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2XSsFiVECtj",
        "colab_type": "code",
        "outputId": "17bc81f7-a70b-4c18-ed7f-4a03ec91f145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_pos = build_vocabulary(positive_location, \"vocab_pos.pkl\")\n",
        "vocab_neg = build_vocabulary(negative_location, \"vocab_neg.pkl\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading  twitter-datasets/train_pos.txt\n",
            "Reading  twitter-datasets/train_neg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bpgXYLg9IkZ",
        "colab_type": "code",
        "outputId": "ccdea7c8-03d1-49f7-8ee8-4fb221cc3922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_pos.keys())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8Sh0RXESLv",
        "colab_type": "code",
        "outputId": "331c5822-56f7-4869-fd1e-77bac2b60dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_neg.keys())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgAWLg6-4o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_sentence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  Filters the given sentence with the given vocabulary. \n",
        "    The words that are not in the vocabulary will be filtered out \n",
        "    of the sentence. \n",
        "  \"\"\"\n",
        "  return [word for word in sentence if word in vocab.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kRaA30GKkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_sequence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  (Filtering included)\n",
        "  Transforms the given sentence into a sequence of ints, \n",
        "  corresponding to the index of the word.\n",
        "  \"\"\"\n",
        "  filtered_sentence = filter_sentence(sentence, vocab)\n",
        "  return [vocab[word] for word in filtered_sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc5EBhCqwtuW",
        "colab_type": "text"
      },
      "source": [
        "# Sentences handler\n",
        "During training we want to be able to load the tweets from the file sequentially for memory efficiency. <br>\n",
        "Here I implement the helper functions that will do that\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKxJMIYlio29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_new_chunk(start, chunksize, file):\n",
        "  \"\"\"\n",
        "  Loads -chunksize- tweets from the specified file, starting from \n",
        "  -start- tweet (excluded). \n",
        "  \"\"\"\n",
        "  f=open(file)\n",
        "  lines=f.readlines()\n",
        "  selected = lines[start:start+chunksize]\n",
        "  return selected "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyDeGyj-q8D",
        "colab_type": "code",
        "outputId": "101ddf51-c2fe-405f-e264-d0ec58433b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "load_new_chunk(99997, 5, positive_location)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<user> <user> um gord ... i just read your profile . i'm not sure i can have lunch with a riders fan\\n\",\n",
              " \"<user> i'm so excited for tomorrow ! look out for two leprechauns ! xx\\n\",\n",
              " 'i always wondered what the job application is like at hooters .. do they just give you a bra and say , \" here fill this out \" .. ? \"\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dL5Zn1yG2Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5dsfNPguQfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_size = int(VALIDATION_SPLIT*100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9IcOlwA-3YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_generator(file, chunksize, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  The output of the generator must be a tuple (inputs, targets)\n",
        "  This tuple (a single output of the generator) makes a single batch\n",
        "  Different batches may have different sizes. For example, \n",
        "  the last batch of the epoch is commonly smaller than the others, \n",
        "  if the size of the dataset is not divisible by the batch size. \n",
        "  \"\"\"\n",
        "  start = 0 \n",
        "  while start < 100000 - validation_size:\n",
        "    sentences = load_new_chunk(start,chunksize,file)\n",
        "    sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "    start +=chunksize\n",
        "    yield padded_sequences, padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16vV70rNuPo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_validation_data(file, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  start = 100000 - validation_size\n",
        "  sentences = load_new_chunk(start,validation_size,file)\n",
        "  sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "  return padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUuiwa_eul94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_pos = get_validation_data(positive_location,MAX_SEQUENCE_LENGTH,vocab_pos)\n",
        "validation_neg = get_validation_data(negative_location,MAX_SEQUENCE_LENGTH,vocab_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NIhn9VUwzmP",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHrAfaCJmA",
        "colab_type": "text"
      },
      "source": [
        "To embed the words we are going to use a pre-trained GloVe model as a *prior* on the embedding that we'll be inserted as a first layer to the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTmWWpn7w1Jc",
        "colab_type": "code",
        "outputId": "862be35d-b3f1-4f95-8725-3c0bc90f61d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-17 15:36:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-17 15:36:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-17 15:36:01--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.08MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-17 15:42:31 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgHIKzsEbWG",
        "colab_type": "code",
        "outputId": "0bbfe70f-b968-4867-9e20-fcec84c9662d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ECvY5mFBSD",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the *200d* embedding file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxnFtKDoFAtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_location = 'glove.6B.200d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txw8HQPfFO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOa0_99hFAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove_location, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S10AVtAqAs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_emb_matrix(glove, vocab):\n",
        "  \"\"\"\n",
        "  Build the pre-trained GloVe embedding on the given vocabulary. \n",
        "  The GloVe embedding should be presented as a vocabulary. \n",
        "  \"\"\"\n",
        "  print(\"Building the GloVe embedding.\")\n",
        "  N = len(vocab.keys()) + 1 # number of words in the vocabulary \n",
        "  D = 200 # embedding dimension\n",
        "  emb_matrix = np.zeros((N,D))\n",
        "  for idx, word in enumerate(vocab.keys()):\n",
        "    embedding_vector = glove.get(word)\n",
        "    if embedding_vector is not None: emb_matrix[idx+1] = embedding_vector\n",
        "    else: emb_matrix[idx+1] = embeddings_index.get('unk')\n",
        "  return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8_oEHkGjm8",
        "colab_type": "code",
        "outputId": "f6e21004-270e-49cc-b676-1613deda9a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "emb_matrix_pos = build_emb_matrix(embeddings_index, vocab_pos)\n",
        "emb_matrix_neg = build_emb_matrix(embeddings_index, vocab_neg)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the GloVe embedding.\n",
            "Building the GloVe embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idGVoWe7G64B",
        "colab_type": "code",
        "outputId": "3a834f33-242f-4ccb-bb72-55c119fc3e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb_matrix_pos.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5712, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSIyMZstbdiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez(\"emb_matrix_pos\",emb_matrix_pos)\n",
        "np.savez(\"emb_matrix_neg\",emb_matrix_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXDvDo5I7Jab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_matrix_pos = np.load(\"emb_matrix_pos.npz\")[\"arr_0\"]\n",
        "emb_matrix_neg = np.load(\"emb_matrix_neg.npz\")[\"arr_0\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlfhxfAw19g",
        "colab_type": "text"
      },
      "source": [
        "# VAE model\n",
        "To make my life simpler I chose to use Keras to build the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv37tgi0HOYj",
        "colab_type": "code",
        "outputId": "4849a8df-0127-4b82-9df6-80c904ff232a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.layers.advanced_activations import ELU"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2vCT4_HCXN",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameters first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-eFaAIHx_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LATENT_DIM = 64\n",
        "HIDDEN_DIM = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYMmGPNd7a-i",
        "colab_type": "text"
      },
      "source": [
        "#### text VAE class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcN1Jf7v-px",
        "colab_type": "text"
      },
      "source": [
        "In the following cells we'll build a **Model** class and its **Layers**' classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGCQIt3u6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlzVlTJ_vi4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  \"\"\"\n",
        "  Sampling layer. \n",
        "  The sampling will be from the posterior on the latent vector, \n",
        "  whose prior is a standard Gaussian. \n",
        "  \"\"\"\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    # batch and latent dimensionality\n",
        "    batch_size = tf.shape(z_mean)[0] \n",
        "    latent_dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch_size, latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zREAGsAxkTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(layers.Layer):\n",
        "  \"\"\"\n",
        "  Embedding layer. \n",
        "  Responsible for training the embedding. \n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               emb_matrix= None, # pre-trained embeddings\n",
        "               name='embedding',\n",
        "               to_train= True, # wether to train the embeddings\n",
        "               **kwargs):\n",
        "    super(Embedding, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.embedding = layers.Embedding(N, D, weights=[emb_matrix],\n",
        "                                      input_length=seq_length, trainable=to_train)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyZfu7BwlTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Encoder layer. \n",
        "  This layer parametrizes the mapping from the input  \n",
        "  to a latent space distribution. \n",
        "  Concretely it will map sentences to the triple (z_mean, z_log_var, z).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               latent_dim=LATENT_DIM,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               name='encoder',\n",
        "               dropout_rate=0.2,\n",
        "               activation_fun='elu',\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.recurrent_layer = layers.Bidirectional(layers.LSTM(hidden_dim, \n",
        "                            return_sequences=False, recurrent_dropout=dropout_rate,\n",
        "                            input_shape=(None,seq_length,D)), merge_mode='concat')\n",
        "    self.dense_proj = layers.Dense(hidden_dim, activation=activation_fun)\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Input must be of shape (None, max_len)\n",
        "    h = self.recurrent_layer(inputs)\n",
        "    x = self.dense_proj(h)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8io-a-Xzy6S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Decoder layer. \n",
        "  This layer parametrizes the mapping from the latent space to the \n",
        "  output dimension. \n",
        "  Concretely it will map a z to a readable sentence.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               name='decoder',\n",
        "               dropout_rate=0.2,\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.latent2hidden = layers.Dense(hidden_dim,activation='linear',\n",
        "                                      input_shape=(None,latent_dim))\n",
        "    self.recurrent_layer = layers.LSTM(hidden_dim, return_sequences=True, \n",
        "                                       recurrent_dropout=dropout_rate, \n",
        "                                       input_shape=(None,seq_length,D))\n",
        "    self.mean = layers.TimeDistributed(layers.Dense(N, activation='linear'))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs[0]\n",
        "    z = inputs[1]\n",
        "    mask = inputs[2]\n",
        "    hidden = self.latent2hidden(z)\n",
        "    h_decoded = self.recurrent_layer(x, initial_state=[hidden,hidden], mask=mask)\n",
        "    x_decoded_mean = self.mean(h_decoded)\n",
        "    return h_decoded, x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPooKv_05bSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxln6VbuudUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textVAE(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  Model class for VAE.\n",
        "  Combines the embedding, encoder and decoder into an \n",
        "  end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               unk_embedding, # embedding for unknown word\n",
        "               emb_matrix=None, # pre-trained embeddings - if None, they will be trained\n",
        "               train_emb = True,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               word_dropout_rate=0.75,\n",
        "               dropout_rate=0.2,\n",
        "               name='textVAE',\n",
        "               **kwargs):\n",
        "    super(textVAE, self).__init__(name=name, **kwargs)\n",
        "    self.N, self.D = vocab_dim\n",
        "    self.unk_embedding = unk_embedding\n",
        "    self.seq_length = seq_length\n",
        "    self.word_dropout_rate = word_dropout_rate\n",
        "    self.embedding = Embedding(vocab_dim=vocab_dim,\n",
        "                               seq_length=seq_length,\n",
        "                               emb_matrix=emb_matrix,\n",
        "                               to_train=train_emb)\n",
        "    self.encoder = Encoder(vocab_dim=vocab_dim,\n",
        "                           seq_length=seq_length,\n",
        "                           latent_dim=latent_dim,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           dropout_rate=dropout_rate)\n",
        "    self.decoder = Decoder(vocab_dim=vocab_dim, \n",
        "                           seq_length=seq_length,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           latent_dim=latent_dim,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    \n",
        "  def get_mask(self, input_sequence):\n",
        "    \"\"\"\n",
        "    Automatically drops self.word_dropout_rate% of words \n",
        "    in the input. (To call before forwarding to the decoder.)\n",
        "    \"\"\"\n",
        "    #input shape = batch size x seq length x emb dimension\n",
        "    shape = K.shape(input_sequence)\n",
        "    prob = tf.random.uniform(shape,0,1,tf.float32)\n",
        "    mask = prob < self.word_dropout_rate \n",
        "    return mask\n",
        "\n",
        "  def compute_loss(self, z_mean, z_log_var, z, \n",
        "                   x, x_decoded_mean, target_weights):\n",
        "    \"\"\"\n",
        "    Computes the VAE loss. \n",
        "    \"\"\"\n",
        "    labels = tf.cast(x, tf.int32)\n",
        "    xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, \n",
        "                        labels, weights=target_weights, \n",
        "                        average_across_timesteps=False,\n",
        "                        average_across_batch=False), axis=-1)\n",
        "    kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    z_mean, z_log_var, z = self.encoder(x)\n",
        "    # Word dropout for the decoder, as described in the paper\n",
        "    mask = self.get_mask(inputs)\n",
        "    h_decoded, x_decoded_mean = self.decoder((x,z,mask))\n",
        "    # Add KL divergence regularization loss.\n",
        "    target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "    loss = self.compute_loss(z_mean, z_log_var, z, \n",
        "                             inputs, x_decoded_mean, target_weights) \n",
        "    self.add_loss(loss)\n",
        "    return h_decoded,x_decoded_mean  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAR_g9F7a5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGFY1dn7Den",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_model(emb_matrix, name):\n",
        "  \"\"\"\n",
        "  Constructing the text VAE model.\n",
        "  \"\"\"\n",
        "  N,D = emb_matrix.shape\n",
        "  vae = textVAE(vocab_dim=(N,D),\n",
        "                seq_length=MAX_SEQUENCE_LENGTH,\n",
        "                unk_embedding=embeddings_index.get('unk'), # from GloVe embedding index\n",
        "                emb_matrix=emb_matrix,\n",
        "                name=name)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  vae.compile(optimizer=optimizer)\n",
        "  return vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgNBXBecIw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusLhCTm8Zlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_pos = construct_model(emb_matrix_pos, name=\"pos_vae\")\n",
        "generator_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7JUB_cMj64-",
        "colab_type": "text"
      },
      "source": [
        "### text VAE with functional APIs\n",
        "Here I implement the same model as above, but using the Keras functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "commf2LjknYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV5M7rKrz1yP",
        "colab_type": "text"
      },
      "source": [
        "We first need to define the sampling function and the loss layer separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOYikcA1yKt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(z_mean, z_log_var):\n",
        "  epsilon = K.random_normal(shape=(BATCH_SIZE, LATENT_DIM))\n",
        "  return z_mean + tf.exp(0.5 * z_log_var) * epsilon  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ht0pfu0zyla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAEloss(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(VAEloss, self).__init__(**kwargs)\n",
        "\n",
        "    def get_vae_loss(self, z_mean, z_log_var, z, x, inputs, decoded_mean):\n",
        "      labels = tf.cast(inputs, tf.int32)\n",
        "      target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "      xent_loss = K.sum(tfa.seq2seq.sequence_loss(decoded_mean, \n",
        "                                                  labels,\n",
        "                                                  weights=target_weights,\n",
        "                                                  average_across_timesteps=False,\n",
        "                                                  average_across_batch=False), axis=-1)\n",
        "      kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "      return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      # unpack inputs\n",
        "      z_mean = inputs[0]\n",
        "      z_log_var = inputs[1]\n",
        "      z = inputs[2]\n",
        "      x = inputs[3]\n",
        "      inputs_ = inputs[4]\n",
        "      decoded_mean = inputs[5]\n",
        "      # add loss \n",
        "      loss = self.get_vae_loss(z_mean, z_log_var, z, x, inputs_, decoded_mean)\n",
        "      self.add_loss(loss, inputs=inputs)\n",
        "      \n",
        "      return K.ones_like(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VrO8POlyl49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgkPXHI5zaEo",
        "colab_type": "text"
      },
      "source": [
        "##### Positive model first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xJaL6bkEVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "e16084a3-a5af-4b34-a112-90ca1e1cf8bb"
      },
      "source": [
        "N,D = emb_matrix_pos.shape\n",
        "x_pos = Input(batch_shape=(None, MAX_SEQUENCE_LENGTH))\n",
        "# EMBEDDING LAYER -------------------------------\n",
        "x_embedded_pos = Embedding(N, D, weights=[emb_matrix_pos], \n",
        "                           input_length=MAX_SEQUENCE_LENGTH, trainable=True)(x_pos)\n",
        "# ENCODING LAYER --------------------------------\n",
        "hidden_pos = Bidirectional(LSTM(HIDDEN_DIM, \n",
        "                                return_sequences=False, \n",
        "                                recurrent_dropout=dropout_rate),\n",
        "                           merge_mode='concat')(x_embedded_pos)\n",
        "# dropping some of the input and output units, as suggested in the paper\n",
        "hidden_dropped_pos = Dropout(dropout_rate)(hidden_pos) \n",
        "hidden_pos2 = Dense(HIDDEN_DIM, activation='relu')(hidden_dropped_pos)\n",
        "hidden_dropped_pos2 = Dropout(dropout_rate)(hidden_pos2)\n",
        "# predicting the moments of the distribution on the latent space\n",
        "z_mean_pos = Dense(LATENT_DIM)(hidden_dropped_pos2)\n",
        "z_log_var_pos = Dense(LATENT_DIM)(hidden_dropped_pos2)\n",
        "# sampling\n",
        "z_pos = sample(z_mean_pos, z_log_var_pos)\n",
        "# DECODING LAYER -------------------------------\n",
        "repeated_context = RepeatVector(MAX_SEQUENCE_LENGTH)\n",
        "sequence_decoder_pos = LSTM(HIDDEN_DIM, \n",
        "                            return_sequences=True, \n",
        "                            recurrent_dropout=dropout_rate)\n",
        "sequence_decoder_mean_pos = TimeDistributed(Dense(N, activation='linear'))\n",
        "sequence_decoded_pos = sequence_decoder_pos(repeated_context(z_pos),initial_state=[hidden_pos2,hidden_pos2])\n",
        "sequence_decoded_mean_pos = sequence_decoder_mean_pos(sequence_decoded_pos)\n",
        "# LOSS LAYER -----------------------------------\n",
        "loss_layer_pos = VAEloss()([z_mean_pos, z_log_var_pos, z_pos, x_embedded_pos, x_pos, sequence_decoded_mean_pos])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoOmsHgTzeql",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8Os0N2zYYn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "bb704a5e-0bd1-473e-95f5-23c0a581d17b"
      },
      "source": [
        "VAE_pos = Model(inputs=x_pos, outputs=[loss_layer_pos], name=\"positiveVAE\") \n",
        "VAE_pos.compile(optimizer='adam')\n",
        "VAE_pos.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"positiveVAE\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 100, 200)     1142400     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 192)          228096      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 192)          0           bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 96)           18528       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 96)           0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 64)           6208        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_8 (TensorFlowOp [(None, 64)]         0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Exp_4 (TensorFlowOp [(None, 64)]         0           tf_op_layer_Mul_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 64)           6208        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_9 (TensorFlowOp [(1000, 64)]         0           tf_op_layer_Exp_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_4 (TensorFlow [(1000, 64)]         0           dense_17[0][0]                   \n",
            "                                                                 tf_op_layer_Mul_9[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_4 (RepeatVector)  (1000, 100, 64)      0           tf_op_layer_AddV2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (1000, 100, 96)      61824       repeat_vector_4[0][0]            \n",
            "                                                                 dense_16[0][0]                   \n",
            "                                                                 dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (1000, 100, 5712)    554064      lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "va_eloss_3 (VAEloss)            (None, 100, 200)     0           dense_17[0][0]                   \n",
            "                                                                 dense_18[0][0]                   \n",
            "                                                                 tf_op_layer_AddV2_4[0][0]        \n",
            "                                                                 embedding_4[0][0]                \n",
            "                                                                 input_8[0][0]                    \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 2,017,328\n",
            "Trainable params: 2,017,328\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1n4ugoard99",
        "colab_type": "text"
      },
      "source": [
        "#### Now negative tweets model (same as above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9c0GnFvkEt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LJcVQ3w5EB",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBkhZlNxBTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint_file_pos = \"textVAE_pos_checkpt.h5\"\n",
        "model_checkpoint_file_neg = \"textVAE_neg_checkpt.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC27n3V1A8r9",
        "colab_type": "code",
        "outputId": "97c3fec3-aa3e-48a5-98f0-9428bb17e592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "N_STEPS = int((1-VALIDATION_SPLIT)*100000/BATCH_SIZE) \n",
        "N_STEPS"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czHBc4u03qEI",
        "colab_type": "text"
      },
      "source": [
        "#### Positive model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cB4UmvBNEQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('-------epoch: ',epoch,'--------')\n",
        "  VAE_pos.fit(input_generator(positive_location, \n",
        "                            chunksize=BATCH_SIZE,\n",
        "                            max_seq_length=MAX_SEQUENCE_LENGTH, \n",
        "                            vocab=vocab_pos),\n",
        "                      epochs=1, steps_per_epoch=N_STEPS, \n",
        "                      validation_data = (validation_pos,validation_pos))\n",
        "    \n",
        "  print(\"Saving the model\")\n",
        "  VAE_pos.save_weights(model_checkpoint_file_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J-RomNy81Jna"
      },
      "source": [
        "#### Negative model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e674ba8f-1724-42d8-fddd-85ce5b5884ba",
        "id": "AUfWvwmE1Jnl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('-------epoch: ',epoch,'--------')\n",
        "  VAE_neg.fit(input_generator(negative_location, \n",
        "                            chunksize=BATCH_SIZE,\n",
        "                            max_seq_length=MAX_SEQUENCE_LENGTH, \n",
        "                            vocab=vocab_neg),\n",
        "                      epochs=1, steps_per_epoch=N_STEPS, \n",
        "                      validation_data = (validation_neg,validation_neg))\n",
        "    \n",
        "  print(\"Saving the model\")\n",
        "  VAE_neg.save_weights(model_checkpoint_file_neg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 --------\n",
            "31/80 [==========>...................] - ETA: 55s - loss: 669.0790"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl6Ljqd1KD5S",
        "colab_type": "text"
      },
      "source": [
        "Now we want to define a separate model for the Encoder and for the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni3sIoCDAvpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "N,D = emb_matrix_pos.shape\n",
        "encoder_pos = Model(inputs=x_pos, outputs=[z_pos, hidden_pos2], name=\"positiveEncoder\")\n",
        "#DECODER \n",
        "_z = Input(shape=(LATENT_DIM,))\n",
        "_hidden = Input(shape=(HIDDEN_DIM,))\n",
        "_decoding = sequence_decoder_pos(repeated_context(_z), initial_state=[_hidden,_hidden])\n",
        "_decoding_mean = sequence_decoder_mean_pos(_decoding)\n",
        "_decoding_mean = (Activation('softmax'))(_decoding_mean)\n",
        "decoder_pos = Model(inputs=[_z, _hidden], outputs=_decoding_mean, name=\"positiveDecoder\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXVbeBo_xB1G",
        "colab_type": "text"
      },
      "source": [
        "# Tests \n",
        "We are now going to use the trained model to do sentence interpolation, but we need a few helper functions to do so "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sd277FwyN7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word_pos = {v:k for k,v in vocab_pos.items()}\n",
        "index2word_neg = {v:k for k,v in vocab_neg.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBR5t4BI71T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_latent(encoder,inputs):\n",
        "  \"\"\"\n",
        "  Uses the encoder part of the trained generator to go \n",
        "  from the input space to the latent space.\n",
        "  \"\"\"\n",
        "  z,hidden = encoder.predict(inputs, batch_size=1)\n",
        "  return z,hidden\n",
        "\n",
        "def predict_sentence(decoder,z):\n",
        "  \"\"\"\n",
        "  Uses the decoder part of the trained generator to \n",
        "  go from the latent space to a new sentence.\n",
        "  Inputs must be the tuple (x,z)\n",
        "  \"\"\"\n",
        "  decoding_mean = decoder.predict(z, batch_size=1)\n",
        "  return decoding_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDr6It-xFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_sampled_sentence(sentence_original, sentence_vector, decoder, index2word, latent_dim=LATENT_DIM, max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Uses the trained model\n",
        "  @:param: sentence vector: latent space representation of a sentence\n",
        "      (this should be the output of predict_latent_mean)\n",
        "\n",
        "  \"\"\"\n",
        "  N = len(index2word.keys())+1\n",
        "  sentence_vector = np.reshape(sentence_vector,[1,latent_dim]) # reshaping into 1 x latent space, where 1 is the batch size\n",
        "  generated = tf.keras.activation.softmax(predict_sentence(decoder, sentence_vector))\n",
        "  generated = np.reshape(generated,[max_len,N]) # reshaping into sequence length x vocabulary words \n",
        "  generated_indices = np.apply_along_axis(np.argmax, 1, generated)\n",
        "  word_list = list(np.vectorize(index2word.get)(generated_indices))\n",
        "  w_list = [w for w in word_list if w] # filtering out the words\n",
        "  print(' '.join(w_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0oG1Ll0XgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shortest_homologies(point1,point2,n):\n",
        "  \"\"\"\n",
        "  Discover n mid-way  points in the path between point1 and point2.\n",
        "  The name of the functions is due to the fact that the points are in the \n",
        "  latent space.\n",
        "  \"\"\"\n",
        "  dist_vec = point2 - point1\n",
        "  sample = np.linspace(0, 1, n, endpoint = True)\n",
        "  samples = []\n",
        "  for s in sample:\n",
        "      samples.append(point1 + s * dist_vec)\n",
        "  return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQU2-HVzIOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_interpolation(sentence1, sentence2, n, \n",
        "                            vocab, encoder, decoder, index2word, \n",
        "                            max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Interpolating between the two given sentences in n steps.\n",
        "  \"\"\"\n",
        "  sequence1 = sentence_to_sequence(sentence1, vocab)\n",
        "  sequence1 = pad_sequences(sequence1, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  sequence2 = sentence_to_sequence(sentence2, vocab)\n",
        "  sequence2 = pad_sequences(sequence2, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  z1, _ = predict_latent(encoder,sequence1)\n",
        "  z2, _ = predict_latent(encoder,sequence2)\n",
        "  homologies = shortest_homologies(z1, z2, n)\n",
        "  for latent_sentence in homologies:\n",
        "    print_sampled_sentence(sentence1, latent_sentence, decoder, index2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wv3PHMpQs4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = [\"I think machine learning is great\"]\n",
        "sentence2 = [\"Do you want a new book\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYhD3Q_8oeI",
        "colab_type": "code",
        "outputId": "048b3db3-2d15-4910-aa4c-6f2eaf15425f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        }
      },
      "source": [
        "print(\"Positive sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_pos, \n",
        "                        encoder_pos, \n",
        "                        decoder_pos, \n",
        "                        index2word_pos)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive sentence interpolation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-67c7bf4c061f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mencoder_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mdecoder_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         index2word_pos)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-131b67f66a5d>\u001b[0m in \u001b[0;36msentences_interpolation\u001b[0;34m(sentence1, sentence2, n, vocab, encoder, decoder, index2word, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msequence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msequence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mhomologies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortest_homologies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-274d57d69e65>\u001b[0m in \u001b[0;36mpredict_latent\u001b[0;34m(encoder, inputs)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mspace\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'batch_outputs' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diUSya8k9XyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Negative sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_neg,generator_neg, index2word_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
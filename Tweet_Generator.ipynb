{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Generator.ipynb",
      "provenance": [],
      "mount_file_id": "12MHNh7F8wxZkVWHEuDkvARP1SNU4bpcX",
      "authorship_tag": "ABX9TyOxY6DUQgi72FiC0xLRXIoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Tweet_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMqVaBwSPA",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of **Generating sentences from a continuous space paper**\n",
        "Here's a link to the [paper](https://arxiv.org/pdf/1511.06349v4.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCPxyfX9r53",
        "colab_type": "text"
      },
      "source": [
        "> ### The goal \n",
        " What I want to explore here is the expression of sentiment in generative models. <br>\n",
        " The dataset consists of two different samples of tweets, one with positive sentiment and one with negative sentiment. <br>\n",
        " The goal is to train two generators on the two sets separetly and analyse the qualitative differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwkLCb83wngS",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSehtnM7dGh",
        "colab_type": "code",
        "outputId": "de1a528f-d4ce-424e-c4d6-c79f8ce30789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3kXO6vvwm9f",
        "colab_type": "code",
        "outputId": "995012b3-de43-47eb-b172-ca0b7355b29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RnhXHWxKGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxiSqdJQxPvj",
        "colab_type": "text"
      },
      "source": [
        "An example of the raw data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR4lq8bJxKNx",
        "colab_type": "code",
        "outputId": "45752d2d-9f2d-4c96-f56c-b64f98b22925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW_iOL4O78mD",
        "colab_type": "code",
        "outputId": "91b6ccdb-bfe4-491e-833a-4ccd7b3e5337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzumvsqowr8M",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di8j0afxw3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJp435CzVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from collections import Counter\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StWI7_EYwbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Transforms the specified files in tokens using the Twitter tokenizer.\n",
        "    @:params: str\n",
        "        Input text to tokenize\n",
        "    @:return: list(str)\n",
        "        Returns the tokens as a list of strings.\n",
        "    \"\"\"\n",
        "    tokenizer = TweetTokenizer()\n",
        "    # tokenizing the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [w.lower() for w in tokens]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2SswBFb7ut3",
        "colab_type": "text"
      },
      "source": [
        "Now we build a vocabulary with the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Gwr0Ex7zeP",
        "colab_type": "code",
        "outputId": "9d15d56d-0da7-4cf6-d837-a10181d1b734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "frequency_treshold = 100000*0.0001 # 0.1% of the tweets should contain the \"frequent words\"\n",
        "frequency_treshold"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-eecHHzzQJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(file_name, output_file_name):\n",
        "  \"\"\"\n",
        "  Builds the vocabulary for the specified file.\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  print(\"Reading \",file_name)\n",
        "  raw = open(file_name,  \"r\").read()\n",
        "  more_words = tokenize_text(raw)\n",
        "  words.extend(more_words)\n",
        "  # counting the words\n",
        "  counter = Counter(words)\n",
        "  words_count = dict(counter)\n",
        "  # filtering \n",
        "  filtered_words = [k for k, v in words_count.items() if v >= frequency_treshold]\n",
        "  # building voabulary \n",
        "  # index starts at 1 so that the 0 can be left for the empty spaces\n",
        "  vocab = {k:i+1 for i,k in enumerate(filtered_words)}\n",
        "  # saving the vocabulary \n",
        "  with open(output_file_name, 'wb') as f: \n",
        "      pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2XSsFiVECtj",
        "colab_type": "code",
        "outputId": "cb874e48-7de1-46bc-b2cc-42b46511d463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_pos = build_vocabulary(positive_location, \"vocab_pos.pkl\")\n",
        "vocab_neg = build_vocabulary(negative_location, \"vocab_neg.pkl\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading  twitter-datasets/train_pos.txt\n",
            "Reading  twitter-datasets/train_neg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bpgXYLg9IkZ",
        "colab_type": "code",
        "outputId": "36b52f28-1b6d-4d84-88e3-02c47f8466cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_pos.keys())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8Sh0RXESLv",
        "colab_type": "code",
        "outputId": "a88385cc-00b0-4344-8408-323033d476d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab_neg.keys())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgAWLg6-4o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_sentence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  Filters the given sentence with the given vocabulary. \n",
        "    The words that are not in the vocabulary will be filtered out \n",
        "    of the sentence. \n",
        "  \"\"\"\n",
        "  return [word for word in sentence if word in vocab.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kRaA30GKkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_sequence(sentence, vocab):\n",
        "  \"\"\"\n",
        "  (Filtering included)\n",
        "  Transforms the given sentence into a sequence of ints, \n",
        "  corresponding to the index of the word.\n",
        "  \"\"\"\n",
        "  filtered_sentence = filter_sentence(sentence, vocab)\n",
        "  return [vocab[word] for word in filtered_sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc5EBhCqwtuW",
        "colab_type": "text"
      },
      "source": [
        "# Sentences handler\n",
        "During training we want to be able to load the tweets from the file sequentially for memory efficiency. <br>\n",
        "Here I implement the helper functions that will do that\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKxJMIYlio29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_new_chunk(start, chunksize, file):\n",
        "  \"\"\"\n",
        "  Loads -chunksize- tweets from the specified file, starting from \n",
        "  -start- tweet (excluded). \n",
        "  \"\"\"\n",
        "  f=open(file)\n",
        "  lines=f.readlines()\n",
        "  selected = lines[start:start+chunksize]\n",
        "  return selected "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyDeGyj-q8D",
        "colab_type": "code",
        "outputId": "8d8c69df-668e-45ef-8c9d-2f57af0d14cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "load_new_chunk(99997, 5, positive_location)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<user> <user> um gord ... i just read your profile . i'm not sure i can have lunch with a riders fan\\n\",\n",
              " \"<user> i'm so excited for tomorrow ! look out for two leprechauns ! xx\\n\",\n",
              " 'i always wondered what the job application is like at hooters .. do they just give you a bra and say , \" here fill this out \" .. ? \"\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dL5Zn1yG2Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5dsfNPguQfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_size = int(VALIDATION_SPLIT*100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9IcOlwA-3YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_generator(file, chunksize, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  The output of the generator must be a tuple (inputs, targets)\n",
        "  This tuple (a single output of the generator) makes a single batch\n",
        "  Different batches may have different sizes. For example, \n",
        "  the last batch of the epoch is commonly smaller than the others, \n",
        "  if the size of the dataset is not divisible by the batch size. \n",
        "  \"\"\"\n",
        "  start = 0 \n",
        "  while start < 100000 - validation_size:\n",
        "    sentences = load_new_chunk(start,chunksize,file)\n",
        "    sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "    start +=chunksize\n",
        "    yield padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16vV70rNuPo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_validation_data(file, max_seq_length, vocab):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  start = 100000 - validation_size\n",
        "  sentences = load_new_chunk(start,validation_size,file)\n",
        "  sequences = [sentence_to_sequence(sentence,vocab) for sentence in sentences]\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", value=0)\n",
        "  return padded_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUuiwa_eul94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_pos = get_validation_data(positive_location,MAX_SEQUENCE_LENGTH,vocab_pos)\n",
        "validation_neg = get_validation_data(negative_location,MAX_SEQUENCE_LENGTH,vocab_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NIhn9VUwzmP",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHrAfaCJmA",
        "colab_type": "text"
      },
      "source": [
        "To embed the words we are going to use a pre-trained GloVe model as a *prior* on the embedding that we'll be inserted as a first layer to the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTmWWpn7w1Jc",
        "colab_type": "code",
        "outputId": "db9c2ac0-b826-4790-9cc5-b66cab69572d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-15 06:32:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-15 06:32:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-15 06:32:20--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.03MB/s    in 6m 27s  \n",
            "\n",
            "2020-04-15 06:38:48 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgHIKzsEbWG",
        "colab_type": "code",
        "outputId": "62d8a14a-fa50-4bcd-81df-276fa96e4e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ECvY5mFBSD",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the *200d* embedding file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxnFtKDoFAtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_location = 'glove.6B.200d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txw8HQPfFO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOa0_99hFAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove_location, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S10AVtAqAs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_emb_matrix(glove, vocab):\n",
        "  \"\"\"\n",
        "  Build the pre-trained GloVe embedding on the given vocabulary. \n",
        "  The GloVe embedding should be presented as a vocabulary. \n",
        "  \"\"\"\n",
        "  print(\"Building the GloVe embedding.\")\n",
        "  N = len(vocab.keys()) + 1 # number of words in the vocabulary \n",
        "  D = 200 # embedding dimension\n",
        "  emb_matrix = np.zeros((N,D))\n",
        "  for idx, word in enumerate(vocab.keys()):\n",
        "    embedding_vector = glove.get(word)\n",
        "    if embedding_vector is not None: emb_matrix[idx+1] = embedding_vector\n",
        "    else: emb_matrix[idx+1] = embeddings_index.get('unk')\n",
        "  return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8_oEHkGjm8",
        "colab_type": "code",
        "outputId": "66cd218f-ad91-428d-d838-8c6b5f80f150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "emb_matrix_pos = build_emb_matrix(embeddings_index, vocab_pos)\n",
        "emb_matrix_neg = build_emb_matrix(embeddings_index, vocab_neg)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the GloVe embedding.\n",
            "Building the GloVe embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idGVoWe7G64B",
        "colab_type": "code",
        "outputId": "fbc2e315-884f-4240-c612-76cfc4872a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb_matrix_pos.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5712, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSIyMZstbdiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez(\"emb_matrix_pos\",emb_matrix_pos)\n",
        "np.savez(\"emb_matrix_neg\",emb_matrix_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXDvDo5I7Jab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_matrix_pos = np.load(\"emb_matrix_pos.npz\")[\"arr_0\"]\n",
        "emb_matrix_neg = np.load(\"emb_matrix_neg.npz\")[\"arr_0\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlfhxfAw19g",
        "colab_type": "text"
      },
      "source": [
        "# VAE model\n",
        "To make my life simpler I chose to use Keras to build the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv37tgi0HOYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7e2008b-f956-4a5d-901b-fdfa4a7fa989"
      },
      "source": [
        "from keras.layers.advanced_activations import ELU"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2vCT4_HCXN",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameters first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-eFaAIHx_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LATENT_DIM = 64\n",
        "HIDDEN_DIM = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYMmGPNd7a-i",
        "colab_type": "text"
      },
      "source": [
        "#### text VAE class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcN1Jf7v-px",
        "colab_type": "text"
      },
      "source": [
        "In the following cells we'll build a **Model** class and its **Layers**' classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGCQIt3u6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlzVlTJ_vi4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  \"\"\"\n",
        "  Sampling layer. \n",
        "  The sampling will be from the posterior on the latent vector, \n",
        "  whose prior is a standard Gaussian. \n",
        "  \"\"\"\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    # batch and latent dimensionality\n",
        "    batch_size = tf.shape(z_mean)[0] \n",
        "    latent_dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch_size, latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zREAGsAxkTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(layers.Layer):\n",
        "  \"\"\"\n",
        "  Embedding layer. \n",
        "  Responsible for training the embedding. \n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               emb_matrix= None, # pre-trained embeddings\n",
        "               name='embedding',\n",
        "               to_train= True, # wether to train the embeddings\n",
        "               **kwargs):\n",
        "    super(Embedding, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.embedding = layers.Embedding(N, D, weights=[emb_matrix],\n",
        "                                      input_length=seq_length, trainable=to_train)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyZfu7BwlTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Encoder layer. \n",
        "  This layer parametrizes the mapping from the input  \n",
        "  to a latent space distribution. \n",
        "  Concretely it will map sentences to the triple (z_mean, z_log_var, z).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               latent_dim=LATENT_DIM,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               name='encoder',\n",
        "               dropout_rate=0.2,\n",
        "               activation_fun='elu',\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.recurrent_layer = layers.Bidirectional(layers.LSTM(hidden_dim, \n",
        "                            return_sequences=False, recurrent_dropout=dropout_rate,\n",
        "                            input_shape=(None,seq_length,D)), merge_mode='concat')\n",
        "    self.dense_proj = layers.Dense(hidden_dim, activation=activation_fun)\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Input must be of shape (None, max_len)\n",
        "    h = self.recurrent_layer(inputs)\n",
        "    x = self.dense_proj(h)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8io-a-Xzy6S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  \"\"\"\n",
        "  Decoder layer. \n",
        "  This layer parametrizes the mapping from the latent space to the \n",
        "  output dimension. \n",
        "  Concretely it will map a z to a readable sentence.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               name='decoder',\n",
        "               dropout_rate=0.2,\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    N, D = vocab_dim\n",
        "    self.latent2hidden = layers.Dense(hidden_dim,activation='linear',\n",
        "                                      input_shape=(None,latent_dim))\n",
        "    self.recurrent_layer = layers.LSTM(hidden_dim, return_sequences=True, \n",
        "                                       recurrent_dropout=dropout_rate, \n",
        "                                       input_shape=(None,seq_length,D))\n",
        "    self.mean = layers.TimeDistributed(layers.Dense(N, activation='linear'))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs[0]\n",
        "    z = inputs[1]\n",
        "    mask = inputs[2]\n",
        "    hidden = self.latent2hidden(z)\n",
        "    h_decoded = self.recurrent_layer(x, initial_state=[hidden,hidden], mask=mask)\n",
        "    x_decoded_mean = self.mean(h_decoded)\n",
        "    return h_decoded, x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPooKv_05bSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxln6VbuudUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textVAE(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  Model class for VAE.\n",
        "  Combines the embedding, encoder and decoder into an \n",
        "  end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_dim, # (number of words x embemdding dimension)\n",
        "               seq_length, #  maximum seq_length\n",
        "               unk_embedding, # embedding for unknown word\n",
        "               emb_matrix=None, # pre-trained embeddings - if None, they will be trained\n",
        "               train_emb = True,\n",
        "               hidden_dim=HIDDEN_DIM,\n",
        "               latent_dim=LATENT_DIM,\n",
        "               word_dropout_rate=0.75,\n",
        "               dropout_rate=0.2,\n",
        "               name='textVAE',\n",
        "               **kwargs):\n",
        "    super(textVAE, self).__init__(name=name, **kwargs)\n",
        "    self.N, self.D = vocab_dim\n",
        "    self.unk_embedding = unk_embedding\n",
        "    self.seq_length = seq_length\n",
        "    self.word_dropout_rate = word_dropout_rate\n",
        "    self.embedding = Embedding(vocab_dim=vocab_dim,\n",
        "                               seq_length=seq_length,\n",
        "                               emb_matrix=emb_matrix,\n",
        "                               to_train=train_emb)\n",
        "    self.encoder = Encoder(vocab_dim=vocab_dim,\n",
        "                           seq_length=seq_length,\n",
        "                           latent_dim=latent_dim,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           dropout_rate=dropout_rate)\n",
        "    self.decoder = Decoder(vocab_dim=vocab_dim, \n",
        "                           seq_length=seq_length,\n",
        "                           hidden_dim=hidden_dim, \n",
        "                           latent_dim=latent_dim,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    \n",
        "  def get_mask(self, input_sequence):\n",
        "    \"\"\"\n",
        "    Automatically drops self.word_dropout_rate% of words \n",
        "    in the input. (To call before forwarding to the decoder.)\n",
        "    \"\"\"\n",
        "    #input shape = batch size x seq length x emb dimension\n",
        "    shape = K.shape(input_sequence)\n",
        "    prob = tf.random.uniform(shape,0,1,tf.float32)\n",
        "    mask = prob < self.word_dropout_rate \n",
        "    return mask\n",
        "\n",
        "  def compute_loss(self, z_mean, z_log_var, z, \n",
        "                   x, x_decoded_mean, target_weights):\n",
        "    \"\"\"\n",
        "    Computes the VAE loss. \n",
        "    \"\"\"\n",
        "    labels = tf.cast(x, tf.int32)\n",
        "    xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, \n",
        "                        labels, weights=target_weights, \n",
        "                        average_across_timesteps=False,\n",
        "                        average_across_batch=False), axis=-1)\n",
        "    kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    z_mean, z_log_var, z = self.encoder(x)\n",
        "    # Word dropout for the decoder, as described in the paper\n",
        "    mask = self.get_mask(inputs)\n",
        "    h_decoded, x_decoded_mean = self.decoder((x,z,mask))\n",
        "    # Add KL divergence regularization loss.\n",
        "    target_weights=tf.ones_like(inputs, dtype=tf.float32)\n",
        "    loss = self.compute_loss(z_mean, z_log_var, z, \n",
        "                             inputs, x_decoded_mean, target_weights) \n",
        "    self.add_loss(loss)\n",
        "    return h_decoded,x_decoded_mean  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAR_g9F7a5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGFY1dn7Den",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_model(emb_matrix, name):\n",
        "  \"\"\"\n",
        "  Constructing the text VAE model.\n",
        "  \"\"\"\n",
        "  N,D = emb_matrix.shape\n",
        "  vae = textVAE(vocab_dim=(N,D),\n",
        "                seq_length=MAX_SEQUENCE_LENGTH,\n",
        "                unk_embedding=embeddings_index.get('unk'), # from GloVe embedding index\n",
        "                emb_matrix=emb_matrix,\n",
        "                name=name)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  vae.compile(optimizer=optimizer)\n",
        "  return vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgNBXBecIw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusLhCTm8Zlq",
        "colab_type": "code",
        "outputId": "8c2f6f2f-af80-4a89-ecda-42ab5d894b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "generator_pos = construct_model(emb_matrix_pos, name=\"pos_vae\")\n",
        "generator_pos"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.textVAE at 0x7fd7af9692e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LJcVQ3w5EB",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBkhZlNxBTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint_file_pos = \"textVAE_pos_checkpt\"\n",
        "model_checkpoint_file_neg = \"textVAE_neg_checkpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC27n3V1A8r9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bfcd723-d4bb-43a5-c641-121f01e0e155"
      },
      "source": [
        "EPOCHS = 1\n",
        "N_STEPS = int((1-VALIDATION_SPLIT)*100000/BATCH_SIZE) \n",
        "N_STEPS"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXS8bYgBoTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, input_file_location, vocab, model_location, \n",
        "                epochs=EPOCHS, n_steps=N_STEPS, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  Trains the given model for the given number of epochs, \n",
        "  making -n_steps- in each epoch. \n",
        "  \"\"\"\n",
        "  for epoch in range(EPOCHS):\n",
        "    print('-------epoch: ',epoch,'--------')\n",
        "    model.fit_generator(input_generator(positive_location, chunksize=BATCH_SIZE,\n",
        "                                      max_seq_length=MAX_SEQUENCE_LENGTH, vocab=vocab),\n",
        "                        epochs=1, steps_per_epoch=N_STEPS, \n",
        "                        validation_data = (validation_pos,validation_pos))\n",
        "  print(\"Saving the model\")\n",
        "  model.save_weights(model_location)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cB4UmvBNEQZ",
        "colab_type": "code",
        "outputId": "e9c6385b-4f09-44f8-b36e-012c287a1c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "generator_pos_trained = train_model(generator_pos, positive_location, vocab_pos, \n",
        "                                    model_checkpoint_file_pos)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 --------\n",
            "WARNING:tensorflow:From <ipython-input-52-4876294b0286>:12: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "80/80 [==============================] - 221s 3s/step - loss: 416.4609 - val_loss: 187.1423\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: textVAE_pos_checkpt/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwZL0omF8T67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_neg = construct_model(emb_matrix_neg, name=\"neg_vae\")\n",
        "generator_neg_trained = train_model(generator_neg, negative_location, vocab_neg, \n",
        "                                    model_checkpoint_file_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXVbeBo_xB1G",
        "colab_type": "text"
      },
      "source": [
        "# Tests \n",
        "We are now going to use the trained model to do sentence interpolation, but we need a few helper functions to do so "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sd277FwyN7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word_pos = {v:k for k,v in vocab_pos.items()}\n",
        "index2word_neg = {v:k for k,v in vocab_neg.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBR5t4BI71T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_latent(model,inputs):\n",
        "  \"\"\"\n",
        "  Uses the encoder part of the trained generator to go \n",
        "  from the input space to the latent space.\n",
        "  \"\"\"\n",
        "  x = model.embedding(inputs)\n",
        "  z_mean, _, z = model.encoder(x)\n",
        "  return z\n",
        "\n",
        "def predict_sentence(model,inputs):\n",
        "  \"\"\"\n",
        "  Uses the decoder part of the trained generator to \n",
        "  go from the latent space to a new sentence.\n",
        "  Inputs must be the tuple (x,z)\n",
        "  \"\"\"\n",
        "  x = model.embedding(inputs[0])\n",
        "  z = inputs[1]\n",
        "  _,x_decoded_mean = model.decoder((x,z,None)) # set Mask to None\n",
        "  return x_decoded_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDr6It-xFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_sampled_sentence(sentence_original, sentence_vector, model, index2word, latent_dim=LATENT_DIM, max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Uses the trained model\n",
        "  @:param: sentence vector: latent space representation of a sentence\n",
        "      (this should be the output of predict_latent_mean)\n",
        "\n",
        "  \"\"\"\n",
        "  N = len(index2word.keys())+1\n",
        "  sentence_vector = np.reshape(sentence_vector,[1,latent_dim]) # reshaping into 1 x latent space, where 1 is the batch size\n",
        "  generated = tf.keras.activation.softmax(predict_sentence(model, (sentence_original, sentence_vector)))\n",
        "  generated = np.reshape(generated,[max_len,N]) # reshaping into sequence length x vocabulary words \n",
        "  generated_indices = np.apply_along_axis(np.argmax, 1, generated)\n",
        "  word_list = list(np.vectorize(index2word.get)(generated_indices))\n",
        "  w_list = [w for w in word_list if w] # filtering out the words\n",
        "  print(' '.join(w_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0oG1Ll0XgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shortest_homologies(point1,point2,n):\n",
        "  \"\"\"\n",
        "  Discover n mid-way  points in the path between point1 and point2.\n",
        "  The name of the functions is due to the fact that the points are in the \n",
        "  latent space.\n",
        "  \"\"\"\n",
        "  dist_vec = point2 - point1\n",
        "  sample = np.linspace(0, 1, n, endpoint = True)\n",
        "  samples = []\n",
        "  for s in sample:\n",
        "      samples.append(point1 + s * dist_vec)\n",
        "  return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQU2-HVzIOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_interpolation(sentence1, sentence2, n, \n",
        "                            vocab, model, index2word, \n",
        "                            max_len=MAX_SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Interpolating between the two given sentences in n steps.\n",
        "  \"\"\"\n",
        "  sequence1 = sentence_to_sequence(sentence1, vocab)\n",
        "  sequence1 = pad_sequences(sequence1, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  sequence2 = sentence_to_sequence(sentence2, vocab)\n",
        "  sequence2 = pad_sequences(sequence2, maxlen=max_len, padding=\"pre\", value=0)\n",
        "  latent1 = predict_latent(model,sequence1)\n",
        "  latent2 = predict_latent(model,sequence2)\n",
        "  homologies = shortest_homologies(latent1, latent2, n)\n",
        "  for latent_sentence in homologies:\n",
        "    print_sampled_sentence(sentence1, latent_sentence, model, index2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wv3PHMpQs4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = [\"I think machine learning is great\"]\n",
        "sentence2 = [\"Do you want a new book\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYhD3Q_8oeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Positive sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_pos,generator_pos_trained, index2word_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diUSya8k9XyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Negative sentence interpolation\")\n",
        "sentences_interpolation(sentence1,sentence2,10,\n",
        "                        vocab_neg,generator_neg, index2word_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1l7PxGa0Z8dmU0UQtycWDd31o4L4dSy5Y",
      "authorship_tag": "ABX9TyMYoihIwUkfovmMq+k8yVSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy6mcm9B1rXg",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis of tweets\n",
        "This notebook has been used in the *Kaggle Competition* https://www.kaggle.com/c/cil-text-classification-2020/data.\n",
        "\n",
        "---\n",
        "\n",
        "### The pipeline \n",
        "1. Train the embeddings to obtain a representation of each word\n",
        "2. Compose the word embeddings to get a representation of each tweet\n",
        "3. Train a classifier to distinguish between positive and negative tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ol25AQz13Wp",
        "colab_type": "text"
      },
      "source": [
        "## Data gathering \n",
        "The dataset used comes from Twitter. \n",
        "<br>\n",
        "*Note: I have temporarly stored a copy  of the .zip file on my Drive to be able to access the data from here. <br>The following lines access my Drive folder.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85la27jf20oM",
        "colab_type": "code",
        "outputId": "9431d09c-9c9d-4261-9fde-fdf9dee35b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!unzip 'drive/My Drive/cil-text-classification-2020.zip'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/cil-text-classification-2020.zip\n",
            "  inflating: README.md               \n",
            "  inflating: build_vocab.sh          \n",
            "  inflating: cooc.py                 \n",
            "  inflating: cut_vocab.sh            \n",
            "  inflating: pickle_vocab.py         \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_data.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYotMKLN1lKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d58039c5-cea4-48b5-b8be-62e7b8099ca9"
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXN8w_dj2J53",
        "colab_type": "code",
        "outputId": "83c961a8-b3d5-48b0-e3ff-f245860258f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build_vocab.sh\tdrive\t\t sample_data\t\ttwitter-datasets\n",
            "cooc.py\t\tpickle_vocab.py  sample_submission.csv\n",
            "cut_vocab.sh\tREADME.md\t test_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF7HrTrf2SCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-MrnrZr5PKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "58513320-fcdd-4352-dd67-c6785b58f085"
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhnfxaqU5U2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7a3ffe00-e8bc-4554-9db7-e97a73b5418f"
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n",
        "!wc -l 'twitter-datasets/train_pos_full.txt'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n",
            "1250000 twitter-datasets/train_pos_full.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FdSnOBz6HFK",
        "colab_type": "text"
      },
      "source": [
        "Okay so our dataset consists of:<br>\n",
        "**10.000** entries for the little file<br>\n",
        "**1.250.000** entries for the full file -**x2** because we have a positive and negative file<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZV52d57Tn3a",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI0u8-jxTv4N",
        "colab_type": "text"
      },
      "source": [
        "### Co-occurrence matrix \n",
        "\n",
        "Since the dataset is this big we directly build the co-occurrence matrix from the text file, without loading it into a Python structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPxtb-V51bU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9a5584b-aa5b-4462-fbb8-2b96c3be02b8"
      },
      "source": [
        "# To build a co-occurrence matrix we have to do the following steps:\n",
        "# 1. build a vocabulary of words - you can work on this vocabulary as a pre-processing step of the pipeline\n",
        "# 2. match the words in each line to the vocabulary and build a co-occurence matrix based on a notion of window\n",
        "\n",
        "# Here we build the vocabulary text\n",
        "!cat 'twitter-datasets/train_pos.txt' 'twitter-datasets/train_neg.txt'  | sed \"s/ /\\n/g\" | grep -v \"^\\s*$\" | sort | uniq > vocab.txt\n",
        "!wc -l './vocab.txt'"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114427 ./vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxBJJt-GwBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's load the vocabulary into a Python dictionary and serialise it \n",
        "# To serialise a Python object we use the library pickle\n",
        "import pickle\n",
        "\n",
        "vocab = dict()\n",
        "with open('vocab.txt') as f:\n",
        "  for idx, line in enumerate(f):\n",
        "      vocab[line.strip()] = idx\n",
        "\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "  pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwDEb9HRE-zG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "064bbea2-f8eb-4c58-f167-3bd260d7a90c"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build_vocab.sh\tdrive\t\t sample_data\t\ttwitter-datasets\n",
            "cooc.py\t\tpickle_vocab.py  sample_submission.csv\tvocab.pkl\n",
            "cut_vocab.sh\tREADME.md\t test_data.txt\t\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMIVsJQ1H07O",
        "colab_type": "text"
      },
      "source": [
        "Now it is the time to build the **co-occurrence matrix**. <br>\n",
        "In this step we make a few important design choices, namely :\n",
        "- We define the size of the *co-occurrence window*, which implicitly defines our concept of *context*\n",
        "- We define a *weighting scheme*, which has implications on the type of meaning we are trying to medl (a more syntactical one, instead of a more semantical one) \n",
        "<br>\n",
        "\n",
        "\n",
        "```\n",
        "# Several choices can (and should be) be tried out here\n",
        "# I would make a distinction between two main kind of co-occurrence matrices: \n",
        "# the first one uses a large window (we can use the whole line) and no weighting, \n",
        "# while the second one uses a small window with a distance weighting. \n",
        "\n",
        "semantic_matrix = ... # large window, no weighting\n",
        "syntactic matrix = ... # small window, 1/n weighting\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI33AiH27wMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import *\n",
        "import numpy as np\n",
        "\n",
        "# Now let's define an helper function to build a co-occurrence matrix \n",
        "def build_cooc(vocab_location, original_files, \n",
        "               window_size=None, weighting=\"None\", output_location=\"./cooc.pkl\"):\n",
        "  \"\"\" \n",
        "  Parameters: \n",
        "  - vocab_location: a path to the .pkl file containing the vocabulary \n",
        "  - original_files: a list containing the paths to the input files\n",
        "  - window_size : if None, using the whole line as a window, otw a number is expected. \n",
        "    Note: the window size cannot be larger tha the line length.\n",
        "  - weighting: only 2 types supported for now, one of 'None' or 'Distance' \n",
        "  - output_location: a path to the .pkl file containing the co-occurrence matrix\n",
        "\n",
        "  By default the output (co-occurence matrix) will be saved in a .pkl file in the current\n",
        "  working directory.\n",
        "  \"\"\"\n",
        "  # load the vocabulary \n",
        "  with open(vocab_location, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "  data, row, col = [], [], []\n",
        "  counter = 1\n",
        "  # opening each file\n",
        "  for fn in original_files:\n",
        "      with open(fn) as f:\n",
        "        print(\"Working on \",fn)\n",
        "        # looking at each line\n",
        "        for line in f:\n",
        "          # Here we filter out the words that are not in the vocabulary \n",
        "          tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
        "          tokens = [t for t in tokens if t >= 0]\n",
        "          ll = len(tokens) # filtered line length\n",
        "          if window_size==None or window_size>=ll:\n",
        "            delta = ll \n",
        "          else: \n",
        "            delta = window_size \n",
        "          \n",
        "          for j in range(ll):\n",
        "              t1 = tokens[j]\n",
        "              for i in range(-1*delta,delta):\n",
        "                if j+i<0 or j+i>=ll or i==0:\n",
        "                  # Note: I exclude the self-co-occurrence \n",
        "                  # to save space in memory \n",
        "                  continue\n",
        "                t2 = tokens[j+i]\n",
        "                c = 1\n",
        "                if weighting == 'Distance':\n",
        "                  c = c/i\n",
        "                data.append(c)\n",
        "                row.append(t1)\n",
        "                col.append(t2)\n",
        "\n",
        "          if counter % 10000 == 0:\n",
        "              print(counter)\n",
        "          counter += 1\n",
        "            \n",
        "  # According to scipy documentation the duplicate indices \n",
        "  # entries are not summed automatically\n",
        "  cooc = coo_matrix((data, (row, col)))\n",
        "  print(\"summing duplicates (this can take a while)\")\n",
        "  cooc.sum_duplicates()\n",
        "\n",
        "  # Saving the output\n",
        "  with open(output_location, 'wb') as f:\n",
        "      pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ya26HdWMbkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_locations = [positive_location, negative_location]\n",
        "# First co-occurrence matrix \n",
        "build_cooc('./vocab.pkl', file_locations, output_location='./first_cooc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPt68WnX_m1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second co-occurrence matrix \n",
        "build_cooc('./vocab.pkl',file_locations, window_size=4, weighting='Distance', output_location='./second_cooc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLhdrkyKZvQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a02a5cb8-a485-45db-9ded-bead71166ea8"
      },
      "source": [
        "# Saving to Google Drive \n",
        "!cp vocab.pkl 'drive/My Drive/'\n",
        "!cp first_cooc.pkl 'drive/My Drive/' \n",
        "!cp second_cooc.pkl 'drive/My Drive/' "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!cp first_cooc.pkl 'drive/My Drive/' \\n!cp second_cooc.pkl 'drive/My Drive/' \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIsg1fa9VtJJ",
        "colab_type": "text"
      },
      "source": [
        "### GloVe training\n",
        "The GloVe objective is a **weighted least squares fit of log-counts**.<br>\n",
        "We'll train two word embeddings ($X$, $Y$) with stochastic gradient descent to maximise the GloVe objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiHMonUUAmF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "645c2164-9595-40b1-f02b-9475b13d78b3"
      },
      "source": [
        "# First we load the co-occurrence matrix we are training on \n",
        "cooc_location = 'first_cooc.pkl' # semantic cooc\n",
        "print(\"loading cooccurrence matrix\")\n",
        "with open(cooc_location, 'rb') as f:\n",
        "    cooc = pickle.load(f)\n",
        "print(\"{} nonzero entries\".format(cooc.nnz))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading cooccurrence matrix\n",
            "10121875 nonzero entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHW9eX_0Yfpk",
        "colab_type": "text"
      },
      "source": [
        "Here we have to define some **GloVe hyperparameters**. <br>This assignments should not come out of a rule-of-thumb, rather they should be based on the data as much as possible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCznd4JdH3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "176a0420-8741-4aa6-b848-5adceee6a812"
      },
      "source": [
        "# Now let's use a simple reasoning here. \n",
        "# Let's define  beta  as the frequency treshold, meaning that a frequent \n",
        "# pair of word should have a frequency  > beta.\n",
        "\n",
        "# Let's say a frequent word pair should appear in at least 5% of the tweets\n",
        "beta = 0.05 \n",
        "\n",
        "# We are working with 10.000 tweets for now\n",
        "ntweets = 10000\n",
        "\n",
        "# The hyperparameter MAX is then defined as : \n",
        "MAX = beta*ntweets\n",
        "print(\"MAX: \",MAX)\n",
        "\n",
        "# Other hyperparameters\n",
        "ALPHA = 3/4 # this is the discount factor to apply to the frequent words\n",
        "print(\"ALPHA: \", ALPHA)\n",
        "EMBEDDING_DIM = 100 # the number of dimensions has lots of implications, and \n",
        "print(\"EMBEDDING DIM: \", EMBEDDING_DIM)\n",
        "#there's no reasoning that can justify the right number "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX:  500.0\n",
            "ALPHA:  0.75\n",
            "EMBEDDING DIM:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLTdT5cpX-wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise the embeddings\n",
        "xs = np.random.normal(size=(cooc.shape[0], EMBEDDING_DIM))\n",
        "ys = np.random.normal(size=(cooc.shape[1], EMBEDDING_DIM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6AxCCHleaBl",
        "colab_type": "text"
      },
      "source": [
        "Here we implement **SGD** and we define some of its hyperparameters.\n",
        "\n",
        "---\n",
        "```\n",
        "#TODO : modify Glove objective to take into consideration different similarity\n",
        "# metrics between words (the standard one relies on the dot product)\n",
        "```\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdhZYzoeXzvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "c0a717fa-02b3-4489-e061-929edb88b5db"
      },
      "source": [
        "eta = 0.0001\n",
        "epochs = 10\n",
        "embedding_locations = 'embeddings1'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch {}\".format(epoch))\n",
        "  for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
        "    logn = np.log(n)\n",
        "    fn = min(1.0, (n / MAX) ** ALPHA)\n",
        "    x, y = xs[ix, :], ys[jy, :]\n",
        "    scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
        "    xs[ix, :] += scale * y\n",
        "    ys[jy, :] += scale * x\n",
        "np.savez(embedding_locations, xs, ys)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "epoch 1\n",
            "epoch 2\n",
            "epoch 3\n",
            "epoch 4\n",
            "epoch 5\n",
            "epoch 6\n",
            "epoch 7\n",
            "epoch 8\n",
            "epoch 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndc5HeRV9iCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp 'embeddings1.npz' 'drive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-wgCi2ulrqx",
        "colab_type": "text"
      },
      "source": [
        "Now that we have finished training the embeddings we want to have **a quick overview of what is the result**. <br>\n",
        "To be able to do that we first need to integrate our vocabulary with the embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdTU0SgtkpMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a83a0ef5-ff90-47eb-a8ac-ef8b6fb20264"
      },
      "source": [
        "# Re-load the embeddings\n",
        "npzfile = np.load('embeddings1.npz')\n",
        "npzfile.files"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr_0', 'arr_1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjocUedRouIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the vocabulary to extract the association index -> word \n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "      vocab = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-SoXIaRqHG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer the embeddings into a dataframe \n",
        "import pandas as pd\n",
        "# Now we need to substitute the indexes with the corresponding words, \n",
        "# as given in the vocabulary \n",
        "keys = vocab.keys()\n",
        "X_df = pd.DataFrame(npzfile['arr_0'], index=keys)\n",
        "Y_df = pd.DataFrame(npzfile['arr_1'], index=keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwEu4q65KUoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "b081db27-05d1-4539-aac2-961c259e8184"
      },
      "source": [
        "X_df.head()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>`</th>\n",
              "      <td>0.473936</td>\n",
              "      <td>-2.176287</td>\n",
              "      <td>-0.780994</td>\n",
              "      <td>-0.582022</td>\n",
              "      <td>1.096394</td>\n",
              "      <td>1.439894</td>\n",
              "      <td>1.213193</td>\n",
              "      <td>1.036713</td>\n",
              "      <td>0.507574</td>\n",
              "      <td>0.669754</td>\n",
              "      <td>-0.416431</td>\n",
              "      <td>0.205280</td>\n",
              "      <td>-1.505057</td>\n",
              "      <td>0.165941</td>\n",
              "      <td>0.520289</td>\n",
              "      <td>1.588283</td>\n",
              "      <td>-0.828317</td>\n",
              "      <td>-0.605844</td>\n",
              "      <td>0.107087</td>\n",
              "      <td>0.837186</td>\n",
              "      <td>-0.314274</td>\n",
              "      <td>-0.759787</td>\n",
              "      <td>-0.249638</td>\n",
              "      <td>-0.624133</td>\n",
              "      <td>1.134519</td>\n",
              "      <td>0.966436</td>\n",
              "      <td>0.642990</td>\n",
              "      <td>-1.702596</td>\n",
              "      <td>1.138297</td>\n",
              "      <td>-0.045424</td>\n",
              "      <td>-0.758173</td>\n",
              "      <td>0.265954</td>\n",
              "      <td>0.020887</td>\n",
              "      <td>1.687514</td>\n",
              "      <td>-0.771104</td>\n",
              "      <td>0.670245</td>\n",
              "      <td>0.110943</td>\n",
              "      <td>0.675248</td>\n",
              "      <td>-0.683348</td>\n",
              "      <td>-1.011031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.877079</td>\n",
              "      <td>0.289534</td>\n",
              "      <td>0.412858</td>\n",
              "      <td>-0.744307</td>\n",
              "      <td>0.413928</td>\n",
              "      <td>-1.624588</td>\n",
              "      <td>2.281203</td>\n",
              "      <td>0.015445</td>\n",
              "      <td>-1.730453</td>\n",
              "      <td>0.591234</td>\n",
              "      <td>-1.122728</td>\n",
              "      <td>1.195836</td>\n",
              "      <td>-1.553392</td>\n",
              "      <td>-0.054038</td>\n",
              "      <td>-0.664035</td>\n",
              "      <td>0.914970</td>\n",
              "      <td>-1.013892</td>\n",
              "      <td>-0.941261</td>\n",
              "      <td>-0.320293</td>\n",
              "      <td>1.809805</td>\n",
              "      <td>1.498340</td>\n",
              "      <td>1.463693</td>\n",
              "      <td>-0.840383</td>\n",
              "      <td>-0.474247</td>\n",
              "      <td>0.803659</td>\n",
              "      <td>-0.699898</td>\n",
              "      <td>0.426789</td>\n",
              "      <td>-1.616629</td>\n",
              "      <td>-0.934522</td>\n",
              "      <td>-2.384379</td>\n",
              "      <td>0.928403</td>\n",
              "      <td>-0.314667</td>\n",
              "      <td>0.480708</td>\n",
              "      <td>0.031845</td>\n",
              "      <td>1.227262</td>\n",
              "      <td>0.018915</td>\n",
              "      <td>-0.359059</td>\n",
              "      <td>1.110359</td>\n",
              "      <td>-0.564217</td>\n",
              "      <td>-0.118700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>^</th>\n",
              "      <td>0.289559</td>\n",
              "      <td>-1.868453</td>\n",
              "      <td>1.134751</td>\n",
              "      <td>0.502029</td>\n",
              "      <td>-1.257387</td>\n",
              "      <td>-1.146697</td>\n",
              "      <td>0.867599</td>\n",
              "      <td>0.929203</td>\n",
              "      <td>0.062321</td>\n",
              "      <td>-0.869012</td>\n",
              "      <td>0.513309</td>\n",
              "      <td>-0.585224</td>\n",
              "      <td>-0.994814</td>\n",
              "      <td>0.099877</td>\n",
              "      <td>0.650158</td>\n",
              "      <td>-0.668259</td>\n",
              "      <td>0.807061</td>\n",
              "      <td>0.116481</td>\n",
              "      <td>0.980497</td>\n",
              "      <td>0.619730</td>\n",
              "      <td>0.640612</td>\n",
              "      <td>-0.915360</td>\n",
              "      <td>0.649427</td>\n",
              "      <td>0.134223</td>\n",
              "      <td>-0.565159</td>\n",
              "      <td>1.135901</td>\n",
              "      <td>-1.197937</td>\n",
              "      <td>1.969857</td>\n",
              "      <td>0.317963</td>\n",
              "      <td>-0.638805</td>\n",
              "      <td>-0.861153</td>\n",
              "      <td>0.211646</td>\n",
              "      <td>-1.069356</td>\n",
              "      <td>0.934066</td>\n",
              "      <td>-0.860682</td>\n",
              "      <td>1.579404</td>\n",
              "      <td>0.656490</td>\n",
              "      <td>-0.765495</td>\n",
              "      <td>0.464613</td>\n",
              "      <td>1.104234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.910024</td>\n",
              "      <td>0.688463</td>\n",
              "      <td>-0.022465</td>\n",
              "      <td>-0.088749</td>\n",
              "      <td>0.247602</td>\n",
              "      <td>-0.437997</td>\n",
              "      <td>0.902696</td>\n",
              "      <td>-0.418634</td>\n",
              "      <td>0.081079</td>\n",
              "      <td>0.300992</td>\n",
              "      <td>1.104750</td>\n",
              "      <td>0.599431</td>\n",
              "      <td>-0.269272</td>\n",
              "      <td>0.604846</td>\n",
              "      <td>-0.362225</td>\n",
              "      <td>-0.216814</td>\n",
              "      <td>-0.090024</td>\n",
              "      <td>1.908220</td>\n",
              "      <td>0.208957</td>\n",
              "      <td>0.392682</td>\n",
              "      <td>-0.627514</td>\n",
              "      <td>1.258340</td>\n",
              "      <td>-1.698121</td>\n",
              "      <td>-0.219329</td>\n",
              "      <td>-0.373388</td>\n",
              "      <td>-0.758205</td>\n",
              "      <td>-1.151123</td>\n",
              "      <td>-0.398018</td>\n",
              "      <td>-0.009797</td>\n",
              "      <td>1.831295</td>\n",
              "      <td>-0.498855</td>\n",
              "      <td>0.699288</td>\n",
              "      <td>0.614120</td>\n",
              "      <td>0.055377</td>\n",
              "      <td>-0.855412</td>\n",
              "      <td>-0.760369</td>\n",
              "      <td>-1.444465</td>\n",
              "      <td>-2.255991</td>\n",
              "      <td>-0.154119</td>\n",
              "      <td>-0.492416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>~</th>\n",
              "      <td>1.178408</td>\n",
              "      <td>-0.672965</td>\n",
              "      <td>-0.497189</td>\n",
              "      <td>-0.810477</td>\n",
              "      <td>-0.895652</td>\n",
              "      <td>0.110635</td>\n",
              "      <td>-1.042897</td>\n",
              "      <td>-0.710021</td>\n",
              "      <td>0.074419</td>\n",
              "      <td>0.777404</td>\n",
              "      <td>-1.324407</td>\n",
              "      <td>0.452558</td>\n",
              "      <td>-0.207527</td>\n",
              "      <td>-0.090533</td>\n",
              "      <td>0.661557</td>\n",
              "      <td>1.362989</td>\n",
              "      <td>0.394902</td>\n",
              "      <td>-0.674951</td>\n",
              "      <td>-0.600683</td>\n",
              "      <td>1.871310</td>\n",
              "      <td>-1.050641</td>\n",
              "      <td>-0.072039</td>\n",
              "      <td>-0.413957</td>\n",
              "      <td>0.577257</td>\n",
              "      <td>-0.914736</td>\n",
              "      <td>-0.989615</td>\n",
              "      <td>-1.239333</td>\n",
              "      <td>-1.165745</td>\n",
              "      <td>0.995368</td>\n",
              "      <td>-1.944461</td>\n",
              "      <td>-0.439282</td>\n",
              "      <td>-1.657235</td>\n",
              "      <td>0.236989</td>\n",
              "      <td>0.857301</td>\n",
              "      <td>-0.006548</td>\n",
              "      <td>-0.234861</td>\n",
              "      <td>0.722585</td>\n",
              "      <td>-0.799353</td>\n",
              "      <td>-0.167051</td>\n",
              "      <td>-0.540751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.782462</td>\n",
              "      <td>1.443861</td>\n",
              "      <td>-0.886983</td>\n",
              "      <td>0.567317</td>\n",
              "      <td>-0.125966</td>\n",
              "      <td>-0.259720</td>\n",
              "      <td>-0.497193</td>\n",
              "      <td>0.811501</td>\n",
              "      <td>0.616308</td>\n",
              "      <td>0.583513</td>\n",
              "      <td>-0.440980</td>\n",
              "      <td>-1.857846</td>\n",
              "      <td>0.596675</td>\n",
              "      <td>-1.422876</td>\n",
              "      <td>-0.314646</td>\n",
              "      <td>-1.718516</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>1.321182</td>\n",
              "      <td>-0.879390</td>\n",
              "      <td>0.832289</td>\n",
              "      <td>-0.794130</td>\n",
              "      <td>-0.437417</td>\n",
              "      <td>0.658336</td>\n",
              "      <td>-1.995509</td>\n",
              "      <td>-0.197276</td>\n",
              "      <td>0.399674</td>\n",
              "      <td>1.666053</td>\n",
              "      <td>-1.372860</td>\n",
              "      <td>1.109184</td>\n",
              "      <td>-0.050761</td>\n",
              "      <td>-1.122224</td>\n",
              "      <td>0.166420</td>\n",
              "      <td>-1.211621</td>\n",
              "      <td>1.321194</td>\n",
              "      <td>-0.222103</td>\n",
              "      <td>0.559735</td>\n",
              "      <td>0.213144</td>\n",
              "      <td>0.973547</td>\n",
              "      <td>0.220861</td>\n",
              "      <td>1.067820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>×</th>\n",
              "      <td>-1.301938</td>\n",
              "      <td>-0.221869</td>\n",
              "      <td>-1.339770</td>\n",
              "      <td>-1.289122</td>\n",
              "      <td>0.351518</td>\n",
              "      <td>1.447174</td>\n",
              "      <td>-0.461569</td>\n",
              "      <td>1.964100</td>\n",
              "      <td>1.508901</td>\n",
              "      <td>-0.434748</td>\n",
              "      <td>-0.422564</td>\n",
              "      <td>-2.072223</td>\n",
              "      <td>0.034195</td>\n",
              "      <td>1.665451</td>\n",
              "      <td>-1.118646</td>\n",
              "      <td>1.426851</td>\n",
              "      <td>0.353533</td>\n",
              "      <td>-0.272413</td>\n",
              "      <td>-1.193303</td>\n",
              "      <td>0.113430</td>\n",
              "      <td>0.445754</td>\n",
              "      <td>-0.089027</td>\n",
              "      <td>-0.560299</td>\n",
              "      <td>-0.760512</td>\n",
              "      <td>-0.287211</td>\n",
              "      <td>0.734672</td>\n",
              "      <td>0.507651</td>\n",
              "      <td>-0.337162</td>\n",
              "      <td>-0.545904</td>\n",
              "      <td>-0.293745</td>\n",
              "      <td>1.277089</td>\n",
              "      <td>-0.379277</td>\n",
              "      <td>-0.620461</td>\n",
              "      <td>0.811449</td>\n",
              "      <td>1.470436</td>\n",
              "      <td>2.191121</td>\n",
              "      <td>-0.658830</td>\n",
              "      <td>0.864191</td>\n",
              "      <td>0.442519</td>\n",
              "      <td>-0.624450</td>\n",
              "      <td>...</td>\n",
              "      <td>0.169492</td>\n",
              "      <td>-0.855915</td>\n",
              "      <td>0.324772</td>\n",
              "      <td>0.530432</td>\n",
              "      <td>0.417808</td>\n",
              "      <td>-0.744394</td>\n",
              "      <td>1.148376</td>\n",
              "      <td>-0.970921</td>\n",
              "      <td>-2.058514</td>\n",
              "      <td>-0.675513</td>\n",
              "      <td>0.300095</td>\n",
              "      <td>-0.581709</td>\n",
              "      <td>0.534720</td>\n",
              "      <td>-1.116369</td>\n",
              "      <td>0.414033</td>\n",
              "      <td>0.418269</td>\n",
              "      <td>-0.735615</td>\n",
              "      <td>0.933798</td>\n",
              "      <td>1.361246</td>\n",
              "      <td>1.237684</td>\n",
              "      <td>-1.152001</td>\n",
              "      <td>0.092433</td>\n",
              "      <td>2.471461</td>\n",
              "      <td>-0.344984</td>\n",
              "      <td>0.411856</td>\n",
              "      <td>1.450843</td>\n",
              "      <td>-0.797478</td>\n",
              "      <td>-1.097178</td>\n",
              "      <td>0.180615</td>\n",
              "      <td>-0.432896</td>\n",
              "      <td>-0.896195</td>\n",
              "      <td>0.612991</td>\n",
              "      <td>-0.588402</td>\n",
              "      <td>-0.310230</td>\n",
              "      <td>1.227507</td>\n",
              "      <td>-0.296647</td>\n",
              "      <td>-1.146949</td>\n",
              "      <td>2.776162</td>\n",
              "      <td>-0.369591</td>\n",
              "      <td>1.293035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;</th>\n",
              "      <td>1.255780</td>\n",
              "      <td>0.268804</td>\n",
              "      <td>0.770104</td>\n",
              "      <td>-0.447349</td>\n",
              "      <td>1.008445</td>\n",
              "      <td>0.181766</td>\n",
              "      <td>-0.048524</td>\n",
              "      <td>-0.251425</td>\n",
              "      <td>-1.733353</td>\n",
              "      <td>0.628797</td>\n",
              "      <td>1.270918</td>\n",
              "      <td>-0.055002</td>\n",
              "      <td>0.012452</td>\n",
              "      <td>-1.005026</td>\n",
              "      <td>0.243341</td>\n",
              "      <td>1.691085</td>\n",
              "      <td>0.231184</td>\n",
              "      <td>-1.200860</td>\n",
              "      <td>0.163073</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.070585</td>\n",
              "      <td>0.143458</td>\n",
              "      <td>-0.414321</td>\n",
              "      <td>-0.128827</td>\n",
              "      <td>-0.877579</td>\n",
              "      <td>0.713582</td>\n",
              "      <td>-0.127578</td>\n",
              "      <td>-0.214490</td>\n",
              "      <td>0.019982</td>\n",
              "      <td>-0.103662</td>\n",
              "      <td>0.297082</td>\n",
              "      <td>-0.294494</td>\n",
              "      <td>0.226625</td>\n",
              "      <td>0.733855</td>\n",
              "      <td>0.824954</td>\n",
              "      <td>-1.148233</td>\n",
              "      <td>-2.012519</td>\n",
              "      <td>1.004830</td>\n",
              "      <td>-0.473306</td>\n",
              "      <td>-0.692251</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285825</td>\n",
              "      <td>1.763377</td>\n",
              "      <td>-0.674509</td>\n",
              "      <td>-0.263090</td>\n",
              "      <td>-0.267436</td>\n",
              "      <td>-1.987921</td>\n",
              "      <td>1.763837</td>\n",
              "      <td>-0.314442</td>\n",
              "      <td>0.642990</td>\n",
              "      <td>0.061467</td>\n",
              "      <td>0.640279</td>\n",
              "      <td>0.222577</td>\n",
              "      <td>-0.519906</td>\n",
              "      <td>-0.452409</td>\n",
              "      <td>-0.101849</td>\n",
              "      <td>0.593100</td>\n",
              "      <td>-0.537599</td>\n",
              "      <td>0.319871</td>\n",
              "      <td>1.779960</td>\n",
              "      <td>0.323549</td>\n",
              "      <td>-0.401870</td>\n",
              "      <td>0.206373</td>\n",
              "      <td>0.635499</td>\n",
              "      <td>0.305064</td>\n",
              "      <td>-0.721673</td>\n",
              "      <td>0.899752</td>\n",
              "      <td>0.832585</td>\n",
              "      <td>0.408215</td>\n",
              "      <td>0.224310</td>\n",
              "      <td>-0.487612</td>\n",
              "      <td>-1.157295</td>\n",
              "      <td>-1.112730</td>\n",
              "      <td>0.006064</td>\n",
              "      <td>0.589938</td>\n",
              "      <td>0.107328</td>\n",
              "      <td>0.287665</td>\n",
              "      <td>-0.362274</td>\n",
              "      <td>-0.441167</td>\n",
              "      <td>-0.584990</td>\n",
              "      <td>0.445047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "`  0.473936 -2.176287 -0.780994  ...  1.110359 -0.564217 -0.118700\n",
              "^  0.289559 -1.868453  1.134751  ... -2.255991 -0.154119 -0.492416\n",
              "~  1.178408 -0.672965 -0.497189  ...  0.973547  0.220861  1.067820\n",
              "× -1.301938 -0.221869 -1.339770  ...  2.776162 -0.369591  1.293035\n",
              "<  1.255780  0.268804  0.770104  ... -0.441167 -0.584990  0.445047\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1f2h8QxMU6m",
        "colab_type": "text"
      },
      "source": [
        "#### VSM utils\n",
        "*Note*: The following code is inspired by the ```Stanford University VSM package```, which is available at the goihub repository https://github.com/cgpotts/cs224u.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJsavMBhKZOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "import scipy.spatial.distance\n",
        "\n",
        "# We here define a function that, given a measure of similarity between words, \n",
        "# and the embedding, computes the \"neighbors\" of an input word\n",
        "def neighbors(word, df, distfunc, df2=None):\n",
        "    \"\"\"Tool for finding the nearest neighbors of `word` in `df` according\n",
        "    to `distfunc`. The comparisons are between row vectors.\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "        The anchor word. Assumed to be in `rownames`.\n",
        "    df : pd.DataFrame\n",
        "        The vector-space model.\n",
        "    distfunc : function mapping vector pairs to floats.\n",
        "        The measure of distance between vectors. Can also be `euclidean`,\n",
        "        `matching`, `jaccard`, as well as any other distance measure\n",
        "        between 1d vectors.\n",
        "    df2 : pd.DataFrame\n",
        "        The context embedding. \n",
        "        If df2 is not None, the distance function will be applied between the\n",
        "        df-embedding and the df2-embedding.\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If word is not in `df.index`.\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Ordered by closeness to `word`.\n",
        "    \"\"\"\n",
        "    if word not in df.index:\n",
        "        raise ValueError('{} is not in this VSM'.format(word))\n",
        "    w = df.loc[word]\n",
        "    if df2 is None:\n",
        "      df2 = df\n",
        "    dists = df2.apply(lambda x: distfunc(w, x), axis=1)\n",
        "    return dists.sort_values()[0:15]\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Here the code for a few measures of similarity \n",
        "def euclidean(u, v):\n",
        "    return scipy.spatial.distance.euclidean(u, v)\n",
        "\n",
        "def cosine(u, v):\n",
        "    return scipy.spatial.distance.cosine(u, v)\n",
        "\n",
        "# Note: the following measures should only be applied to count matrices \n",
        "def matching(u, v):\n",
        "    return np.sum(np.minimum(u, v))\n",
        "\n",
        "def jaccard(u, v):\n",
        "    return 1.0 - (matching(u, v) / np.sum(np.maximum(u, v)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK9fIGLxOdyH",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjkZGThKsf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "952759c3-1697-4711-b130-ffa80e79c4fc"
      },
      "source": [
        "index = np.random.randint(0,X_df.shape[0])\n",
        "word = X_df.index[index]\n",
        "neighbors(word,X_df,cosine)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quotes                  0.000000\n",
              "vinos                   0.557079\n",
              "ripenglish              0.588347\n",
              "overacting              0.602061\n",
              "gomawo                  0.605229\n",
              "#neverforget            0.613627\n",
              "ffsss                   0.619497\n",
              "bx-t                    0.620471\n",
              "ingat                   0.623194\n",
              "flotteste               0.631646\n",
              "camel                   0.633609\n",
              "#greatcomebackthough    0.636938\n",
              "libbys                  0.637234\n",
              "#partyrockingonidol     0.639461\n",
              "i360                    0.641337\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLm9sFkwPDpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fc64d7dd-5ba9-4a0d-c099-e082fbcabc0e"
      },
      "source": [
        "neighbors(\"quotes\",X_df,cosine,Y_df)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "duvelkeskermis    0.601453\n",
              "freemas           0.603692\n",
              "hall's            0.607701\n",
              "actively          0.625095\n",
              "signpost          0.627758\n",
              "jared             0.631085\n",
              "mam's             0.631448\n",
              "countri           0.631840\n",
              "anlaby            0.632246\n",
              "(214)             0.634015\n",
              "spinch            0.634625\n",
              "motivation        0.634958\n",
              "inamed            0.635423\n",
              "sxxt              0.643254\n",
              "funko             0.644584\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqJzl9W1POK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "227c2ed8-8911-48b9-c289-e83c03085159"
      },
      "source": [
        "index = np.random.randint(0,X_df.shape[0])\n",
        "word = X_df.index[index]\n",
        "neighbors(word,X_df,cosine)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mopped           0.000000\n",
              "hollywoo         0.594653\n",
              "#bestpartofit    0.604491\n",
              "s52c             0.609266\n",
              "suuucks          0.615372\n",
              "vivendi          0.617818\n",
              "pear             0.618398\n",
              "hubert           0.618561\n",
              "pheromones       0.618819\n",
              "bodily           0.618852\n",
              "cloves           0.623619\n",
              "ultramet         0.623902\n",
              "line's           0.626508\n",
              "aman             0.627809\n",
              "kismat           0.628252\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEqmTmhUROGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp-XeBQqRupt",
        "colab_type": "text"
      },
      "source": [
        "#### #*TODO*: \n",
        "- Improve preprocessing (hashtags, punctuation, ...) \n",
        "- Insert the embeddings in a classification pipeline"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1l7PxGa0Z8dmU0UQtycWDd31o4L4dSy5Y",
      "authorship_tag": "ABX9TyPaS6Nputbdw0nDgO5Kmd4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliaLanzillotta/exercises/blob/master/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy6mcm9B1rXg",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis of tweets\n",
        "This notebook has been used in the *Kaggle Competition* https://www.kaggle.com/c/cil-text-classification-2020/data.\n",
        "\n",
        "---\n",
        "\n",
        "### The pipeline \n",
        "1. Train the embeddings to obtain a representation of each word\n",
        "2. Compose the word embeddings to get a representation of each tweet\n",
        "3. Train a classifier to distinguish between positive and negative tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ol25AQz13Wp",
        "colab_type": "text"
      },
      "source": [
        "## Data gathering \n",
        "The dataset used comes from Twitter. \n",
        "<br>\n",
        "*Note: I have temporarly stored a copy  of the .zip file on my Drive to be able to access the data from here. <br>The following lines access my Drive folder.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85la27jf20oM",
        "colab_type": "code",
        "outputId": "290c032d-7453-441b-f685-e8767117bc15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!unzip 'drive/My Drive/cil-text-classification-2020.zip'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/cil-text-classification-2020.zip\n",
            "  inflating: README.md               \n",
            "  inflating: build_vocab.sh          \n",
            "  inflating: cooc.py                 \n",
            "  inflating: cut_vocab.sh            \n",
            "  inflating: pickle_vocab.py         \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_data.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYotMKLN1lKg",
        "colab_type": "code",
        "outputId": "41ca8ec1-3667-495a-914a-a44a60745b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!unzip 'drive/My Drive/twitter-datasets.zip'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/twitter-datasets.zip\n",
            "  inflating: twitter-datasets/sample_submission.csv  \n",
            "  inflating: twitter-datasets/test_data.txt  \n",
            "  inflating: twitter-datasets/train_neg_full.txt  \n",
            "  inflating: twitter-datasets/train_neg.txt  \n",
            "  inflating: twitter-datasets/train_pos_full.txt  \n",
            "  inflating: twitter-datasets/train_pos.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXN8w_dj2J53",
        "colab_type": "code",
        "outputId": "f02a78cc-4f76-4d81-bfc9-94a5f37b906e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build_vocab.sh\tembeddings1.npz  sample_data\t\ttwitter-datasets\n",
            "cooc.py\t\tfirst_cooc.pkl\t sample_submission.csv\tvocab.pkl\n",
            "cut_vocab.sh\tpickle_vocab.py  second_cooc.pkl\n",
            "drive\t\tREADME.md\t test_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF7HrTrf2SCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_location = 'twitter-datasets/train_pos.txt'\n",
        "negative_location = 'twitter-datasets/train_neg.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-MrnrZr5PKE",
        "colab_type": "code",
        "outputId": "58513320-fcdd-4352-dd67-c6785b58f085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!head -3 'twitter-datasets/train_pos.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
            "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
            "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhnfxaqU5U2Q",
        "colab_type": "code",
        "outputId": "7a3ffe00-e8bc-4554-9db7-e97a73b5418f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!wc -l 'twitter-datasets/train_pos.txt'\n",
        "!wc -l 'twitter-datasets/train_pos_full.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n",
            "1250000 twitter-datasets/train_pos_full.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FdSnOBz6HFK",
        "colab_type": "text"
      },
      "source": [
        "Okay so our dataset consists of:<br>\n",
        "**10.000** entries for the little file<br>\n",
        "**1.250.000** entries for the full file -**x2** because we have a positive and negative file<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZV52d57Tn3a",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI0u8-jxTv4N",
        "colab_type": "text"
      },
      "source": [
        "### Co-occurrence matrix \n",
        "\n",
        "Since the dataset is this big we directly build the co-occurrence matrix from the text file, without loading it into a Python structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPxtb-V51bU",
        "colab_type": "code",
        "outputId": "e9a5584b-aa5b-4462-fbb8-2b96c3be02b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# To build a co-occurrence matrix we have to do the following steps:\n",
        "# 1. build a vocabulary of words - you can work on this vocabulary as a pre-processing step of the pipeline\n",
        "# 2. match the words in each line to the vocabulary and build a co-occurence matrix based on a notion of window\n",
        "\n",
        "# Here we build the vocabulary text\n",
        "!cat 'twitter-datasets/train_pos.txt' 'twitter-datasets/train_neg.txt'  | sed \"s/ /\\n/g\" | grep -v \"^\\s*$\" | sort | uniq > vocab.txt\n",
        "!wc -l './vocab.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114427 ./vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxBJJt-GwBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's load the vocabulary into a Python dictionary and serialise it \n",
        "# To serialise a Python object we use the library pickle\n",
        "import pickle\n",
        "\n",
        "vocab = dict()\n",
        "with open('vocab.txt') as f:\n",
        "  for idx, line in enumerate(f):\n",
        "      vocab[line.strip()] = idx\n",
        "\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "  pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwDEb9HRE-zG",
        "colab_type": "code",
        "outputId": "1df9cdb7-ef6e-41a5-c62b-af19ea367bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMIVsJQ1H07O",
        "colab_type": "text"
      },
      "source": [
        "Now it is the time to build the **co-occurrence matrix**. <br>\n",
        "In this step we make a few important design choices, namely :\n",
        "- We define the size of the *co-occurrence window*, which implicitly defines our concept of *context*\n",
        "- We define a *weighting scheme*, which has implications on the type of meaning we are trying to medl (a more syntactical one, instead of a more semantical one) \n",
        "<br>\n",
        "\n",
        "\n",
        "```\n",
        "# Several choices can (and should be) be tried out here\n",
        "# I would make a distinction between two main kind of co-occurrence matrices: \n",
        "# the first one uses a large window (we can use the whole line) and no weighting, \n",
        "# while the second one uses a small window with a distance weighting. \n",
        "\n",
        "semantic_matrix = ... # large window, no weighting\n",
        "syntactic matrix = ... # small window, 1/n weighting\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI33AiH27wMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import *\n",
        "import numpy as np\n",
        "\n",
        "# Now let's define an helper function to build a co-occurrence matrix \n",
        "def build_cooc(vocab_location, original_files, \n",
        "               window_size=None, weighting=\"None\", output_location=\"./cooc.pkl\"):\n",
        "  \"\"\" \n",
        "  Parameters: \n",
        "  - vocab_location: a path to the .pkl file containing the vocabulary \n",
        "  - original_files: a list containing the paths to the input files\n",
        "  - window_size : if None, using the whole line as a window, otw a number is expected. \n",
        "    Note: the window size cannot be larger tha the line length.\n",
        "  - weighting: only 2 types supported for now, one of 'None' or 'Distance' \n",
        "  - output_location: a path to the .pkl file containing the co-occurrence matrix\n",
        "\n",
        "  By default the output (co-occurence matrix) will be saved in a .pkl file in the current\n",
        "  working directory.\n",
        "  \"\"\"\n",
        "  # load the vocabulary \n",
        "  with open(vocab_location, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "  data, row, col = [], [], []\n",
        "  counter = 1\n",
        "  # opening each file\n",
        "  for fn in original_files:\n",
        "      with open(fn) as f:\n",
        "        print(\"Working on \",fn)\n",
        "        # looking at each line\n",
        "        for line in f:\n",
        "          # Here we filter out the words that are not in the vocabulary \n",
        "          tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
        "          tokens = [t for t in tokens if t >= 0]\n",
        "          ll = len(tokens) # filtered line length\n",
        "          if window_size==None or window_size>=ll:\n",
        "            delta = ll \n",
        "          else: \n",
        "            delta = window_size \n",
        "          \n",
        "          for j in range(ll):\n",
        "              t1 = tokens[j]\n",
        "              for i in range(-1*delta,delta):\n",
        "                if j+i<0 or j+i>=ll or i==0:\n",
        "                  # Note: I exclude the self-co-occurrence \n",
        "                  # to save space in memory \n",
        "                  continue\n",
        "                t2 = tokens[j+i]\n",
        "                c = 1\n",
        "                if weighting == 'Distance':\n",
        "                  c = c/i\n",
        "                data.append(c)\n",
        "                row.append(t1)\n",
        "                col.append(t2)\n",
        "\n",
        "          if counter % 10000 == 0:\n",
        "              print(counter)\n",
        "          counter += 1\n",
        "            \n",
        "  # According to scipy documentation the duplicate indices \n",
        "  # entries are not summed automatically\n",
        "  cooc = coo_matrix((data, (row, col)))\n",
        "  print(\"summing duplicates (this can take a while)\")\n",
        "  cooc.sum_duplicates()\n",
        "\n",
        "  # Saving the output\n",
        "  with open(output_location, 'wb') as f:\n",
        "      pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ya26HdWMbkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_locations = [positive_location, negative_location]\n",
        "# First co-occurrence matrix \n",
        "build_cooc('./vocab.pkl', file_locations, output_location='./first_cooc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPt68WnX_m1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second co-occurrence matrix \n",
        "build_cooc('./vocab.pkl',file_locations, window_size=4, weighting='Distance', output_location='./second_cooc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLhdrkyKZvQY",
        "colab_type": "code",
        "outputId": "a02a5cb8-a485-45db-9ded-bead71166ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Saving to Google Drive \n",
        "!cp vocab.pkl 'drive/My Drive/'\n",
        "!cp first_cooc.pkl 'drive/My Drive/' \n",
        "!cp second_cooc.pkl 'drive/My Drive/' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!cp first_cooc.pkl 'drive/My Drive/' \\n!cp second_cooc.pkl 'drive/My Drive/' \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-1I5snAaSAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading command \n",
        "! cp 'drive/My Drive/vocab.pkl' '.'\n",
        "! cp 'drive/My Drive/first_cooc.pkl' '.'\n",
        "! cp 'drive/My Drive/second_cooc.pkl' '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIsg1fa9VtJJ",
        "colab_type": "text"
      },
      "source": [
        "### GloVe training\n",
        "The GloVe objective is a **weighted least squares fit of log-counts**.<br>\n",
        "We'll train two word embeddings ($X$, $Y$) with stochastic gradient descent to maximise the GloVe objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiHMonUUAmF4",
        "colab_type": "code",
        "outputId": "645c2164-9595-40b1-f02b-9475b13d78b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# First we load the co-occurrence matrix we are training on \n",
        "cooc_location = 'first_cooc.pkl' # semantic cooc\n",
        "print(\"loading cooccurrence matrix\")\n",
        "with open(cooc_location, 'rb') as f:\n",
        "    cooc = pickle.load(f)\n",
        "print(\"{} nonzero entries\".format(cooc.nnz))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading cooccurrence matrix\n",
            "10121875 nonzero entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHW9eX_0Yfpk",
        "colab_type": "text"
      },
      "source": [
        "Here we have to define some **GloVe hyperparameters**. <br>This assignments should not come out of a rule-of-thumb, rather they should be based on the data as much as possible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCznd4JdH3P",
        "colab_type": "code",
        "outputId": "9b06ac01-d96c-473d-dd0c-a953b5ec6f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Now let's use a simple reasoning here. \n",
        "# Let's define  beta  as the frequency treshold, meaning that a frequent \n",
        "# pair of word should have a frequency  > beta.\n",
        "\n",
        "# Let's say a frequent word pair should appear in at least 5% of the tweets\n",
        "beta = 0.05 \n",
        "\n",
        "# We are working with 10.000 tweets for now\n",
        "ntweets = 10000\n",
        "\n",
        "# The hyperparameter MAX is then defined as : \n",
        "MAX = beta*ntweets\n",
        "print(\"MAX: \",MAX)\n",
        "\n",
        "# Other hyperparameters\n",
        "ALPHA = 3/4 # this is the discount factor to apply to the frequent words\n",
        "print(\"ALPHA: \", ALPHA)\n",
        "EMBEDDING_DIM = 100 # the number of dimensions has lots of implications, and \n",
        "print(\"EMBEDDING DIM: \", EMBEDDING_DIM)\n",
        "#there's no reasoning that can justify the right number "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX:  500.0\n",
            "ALPHA:  0.75\n",
            "EMBEDDING DIM:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLTdT5cpX-wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise the embeddings\n",
        "xs = np.random.normal(size=(cooc.shape[0], EMBEDDING_DIM))\n",
        "ys = np.random.normal(size=(cooc.shape[1], EMBEDDING_DIM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6AxCCHleaBl",
        "colab_type": "text"
      },
      "source": [
        "Here we implement **SGD** and we define some of its hyperparameters.\n",
        "\n",
        "---\n",
        "```\n",
        "#TODO : modify Glove objective to take into consideration different similarity\n",
        "# metrics between words (the standard one relies on the dot product)\n",
        "```\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdhZYzoeXzvW",
        "colab_type": "code",
        "outputId": "c0a717fa-02b3-4489-e061-929edb88b5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "eta = 0.0001\n",
        "epochs = 10\n",
        "embedding_locations = 'embeddings1'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch {}\".format(epoch))\n",
        "  for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
        "    logn = np.log(n)\n",
        "    fn = min(1.0, (n / MAX) ** ALPHA)\n",
        "    x, y = xs[ix, :], ys[jy, :]\n",
        "    scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
        "    xs[ix, :] += scale * y\n",
        "    ys[jy, :] += scale * x\n",
        "np.savez(embedding_locations, xs, ys)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "epoch 1\n",
            "epoch 2\n",
            "epoch 3\n",
            "epoch 4\n",
            "epoch 5\n",
            "epoch 6\n",
            "epoch 7\n",
            "epoch 8\n",
            "epoch 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndc5HeRV9iCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp 'embeddings1.npz' 'drive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lcs4nnEaEdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading command \n",
        "!cp 'drive/My Drive/embeddings1.npz' '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-wgCi2ulrqx",
        "colab_type": "text"
      },
      "source": [
        "Now that we have finished training the embeddings we want to have **a quick overview of what is the result**. <br>\n",
        "To be able to do that we first need to integrate our vocabulary with the embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdTU0SgtkpMi",
        "colab_type": "code",
        "outputId": "9dfb87f6-39d3-4471-9d25-e821e9154223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Re-load the embeddings\n",
        "npzfile = np.load('embeddings1.npz')\n",
        "npzfile.files"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr_0', 'arr_1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjocUedRouIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the vocabulary to extract the association index -> word \n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "      vocab = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-SoXIaRqHG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer the embeddings into a dataframe \n",
        "import pandas as pd\n",
        "# Now we need to substitute the indexes with the corresponding words, \n",
        "# as given in the vocabulary \n",
        "keys = vocab.keys()\n",
        "X_df = pd.DataFrame(npzfile['arr_0'], index=keys)\n",
        "Y_df = pd.DataFrame(npzfile['arr_1'], index=keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwEu4q65KUoD",
        "colab_type": "code",
        "outputId": "80422220-2cd3-4e71-9395-7eec04ad8597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "X_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>`</th>\n",
              "      <td>0.473936</td>\n",
              "      <td>-2.176287</td>\n",
              "      <td>-0.780994</td>\n",
              "      <td>-0.582022</td>\n",
              "      <td>1.096394</td>\n",
              "      <td>1.439894</td>\n",
              "      <td>1.213193</td>\n",
              "      <td>1.036713</td>\n",
              "      <td>0.507574</td>\n",
              "      <td>0.669754</td>\n",
              "      <td>-0.416431</td>\n",
              "      <td>0.205280</td>\n",
              "      <td>-1.505057</td>\n",
              "      <td>0.165941</td>\n",
              "      <td>0.520289</td>\n",
              "      <td>1.588283</td>\n",
              "      <td>-0.828317</td>\n",
              "      <td>-0.605844</td>\n",
              "      <td>0.107087</td>\n",
              "      <td>0.837186</td>\n",
              "      <td>-0.314274</td>\n",
              "      <td>-0.759787</td>\n",
              "      <td>-0.249638</td>\n",
              "      <td>-0.624133</td>\n",
              "      <td>1.134519</td>\n",
              "      <td>0.966436</td>\n",
              "      <td>0.642990</td>\n",
              "      <td>-1.702596</td>\n",
              "      <td>1.138297</td>\n",
              "      <td>-0.045424</td>\n",
              "      <td>-0.758173</td>\n",
              "      <td>0.265954</td>\n",
              "      <td>0.020887</td>\n",
              "      <td>1.687514</td>\n",
              "      <td>-0.771104</td>\n",
              "      <td>0.670245</td>\n",
              "      <td>0.110943</td>\n",
              "      <td>0.675248</td>\n",
              "      <td>-0.683348</td>\n",
              "      <td>-1.011031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.877079</td>\n",
              "      <td>0.289534</td>\n",
              "      <td>0.412858</td>\n",
              "      <td>-0.744307</td>\n",
              "      <td>0.413928</td>\n",
              "      <td>-1.624588</td>\n",
              "      <td>2.281203</td>\n",
              "      <td>0.015445</td>\n",
              "      <td>-1.730453</td>\n",
              "      <td>0.591234</td>\n",
              "      <td>-1.122728</td>\n",
              "      <td>1.195836</td>\n",
              "      <td>-1.553392</td>\n",
              "      <td>-0.054038</td>\n",
              "      <td>-0.664035</td>\n",
              "      <td>0.914970</td>\n",
              "      <td>-1.013892</td>\n",
              "      <td>-0.941261</td>\n",
              "      <td>-0.320293</td>\n",
              "      <td>1.809805</td>\n",
              "      <td>1.498340</td>\n",
              "      <td>1.463693</td>\n",
              "      <td>-0.840383</td>\n",
              "      <td>-0.474247</td>\n",
              "      <td>0.803659</td>\n",
              "      <td>-0.699898</td>\n",
              "      <td>0.426789</td>\n",
              "      <td>-1.616629</td>\n",
              "      <td>-0.934522</td>\n",
              "      <td>-2.384379</td>\n",
              "      <td>0.928403</td>\n",
              "      <td>-0.314667</td>\n",
              "      <td>0.480708</td>\n",
              "      <td>0.031845</td>\n",
              "      <td>1.227262</td>\n",
              "      <td>0.018915</td>\n",
              "      <td>-0.359059</td>\n",
              "      <td>1.110359</td>\n",
              "      <td>-0.564217</td>\n",
              "      <td>-0.118700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>^</th>\n",
              "      <td>0.289559</td>\n",
              "      <td>-1.868453</td>\n",
              "      <td>1.134751</td>\n",
              "      <td>0.502029</td>\n",
              "      <td>-1.257387</td>\n",
              "      <td>-1.146697</td>\n",
              "      <td>0.867599</td>\n",
              "      <td>0.929203</td>\n",
              "      <td>0.062321</td>\n",
              "      <td>-0.869012</td>\n",
              "      <td>0.513309</td>\n",
              "      <td>-0.585224</td>\n",
              "      <td>-0.994814</td>\n",
              "      <td>0.099877</td>\n",
              "      <td>0.650158</td>\n",
              "      <td>-0.668259</td>\n",
              "      <td>0.807061</td>\n",
              "      <td>0.116481</td>\n",
              "      <td>0.980497</td>\n",
              "      <td>0.619730</td>\n",
              "      <td>0.640612</td>\n",
              "      <td>-0.915360</td>\n",
              "      <td>0.649427</td>\n",
              "      <td>0.134223</td>\n",
              "      <td>-0.565159</td>\n",
              "      <td>1.135901</td>\n",
              "      <td>-1.197937</td>\n",
              "      <td>1.969857</td>\n",
              "      <td>0.317963</td>\n",
              "      <td>-0.638805</td>\n",
              "      <td>-0.861153</td>\n",
              "      <td>0.211646</td>\n",
              "      <td>-1.069356</td>\n",
              "      <td>0.934066</td>\n",
              "      <td>-0.860682</td>\n",
              "      <td>1.579404</td>\n",
              "      <td>0.656490</td>\n",
              "      <td>-0.765495</td>\n",
              "      <td>0.464613</td>\n",
              "      <td>1.104234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.910024</td>\n",
              "      <td>0.688463</td>\n",
              "      <td>-0.022465</td>\n",
              "      <td>-0.088749</td>\n",
              "      <td>0.247602</td>\n",
              "      <td>-0.437997</td>\n",
              "      <td>0.902696</td>\n",
              "      <td>-0.418634</td>\n",
              "      <td>0.081079</td>\n",
              "      <td>0.300992</td>\n",
              "      <td>1.104750</td>\n",
              "      <td>0.599431</td>\n",
              "      <td>-0.269272</td>\n",
              "      <td>0.604846</td>\n",
              "      <td>-0.362225</td>\n",
              "      <td>-0.216814</td>\n",
              "      <td>-0.090024</td>\n",
              "      <td>1.908220</td>\n",
              "      <td>0.208957</td>\n",
              "      <td>0.392682</td>\n",
              "      <td>-0.627514</td>\n",
              "      <td>1.258340</td>\n",
              "      <td>-1.698121</td>\n",
              "      <td>-0.219329</td>\n",
              "      <td>-0.373388</td>\n",
              "      <td>-0.758205</td>\n",
              "      <td>-1.151123</td>\n",
              "      <td>-0.398018</td>\n",
              "      <td>-0.009797</td>\n",
              "      <td>1.831295</td>\n",
              "      <td>-0.498855</td>\n",
              "      <td>0.699288</td>\n",
              "      <td>0.614120</td>\n",
              "      <td>0.055377</td>\n",
              "      <td>-0.855412</td>\n",
              "      <td>-0.760369</td>\n",
              "      <td>-1.444465</td>\n",
              "      <td>-2.255991</td>\n",
              "      <td>-0.154119</td>\n",
              "      <td>-0.492416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>~</th>\n",
              "      <td>1.178408</td>\n",
              "      <td>-0.672965</td>\n",
              "      <td>-0.497189</td>\n",
              "      <td>-0.810477</td>\n",
              "      <td>-0.895652</td>\n",
              "      <td>0.110635</td>\n",
              "      <td>-1.042897</td>\n",
              "      <td>-0.710021</td>\n",
              "      <td>0.074419</td>\n",
              "      <td>0.777404</td>\n",
              "      <td>-1.324407</td>\n",
              "      <td>0.452558</td>\n",
              "      <td>-0.207527</td>\n",
              "      <td>-0.090533</td>\n",
              "      <td>0.661557</td>\n",
              "      <td>1.362989</td>\n",
              "      <td>0.394902</td>\n",
              "      <td>-0.674951</td>\n",
              "      <td>-0.600683</td>\n",
              "      <td>1.871310</td>\n",
              "      <td>-1.050641</td>\n",
              "      <td>-0.072039</td>\n",
              "      <td>-0.413957</td>\n",
              "      <td>0.577257</td>\n",
              "      <td>-0.914736</td>\n",
              "      <td>-0.989615</td>\n",
              "      <td>-1.239333</td>\n",
              "      <td>-1.165745</td>\n",
              "      <td>0.995368</td>\n",
              "      <td>-1.944461</td>\n",
              "      <td>-0.439282</td>\n",
              "      <td>-1.657235</td>\n",
              "      <td>0.236989</td>\n",
              "      <td>0.857301</td>\n",
              "      <td>-0.006548</td>\n",
              "      <td>-0.234861</td>\n",
              "      <td>0.722585</td>\n",
              "      <td>-0.799353</td>\n",
              "      <td>-0.167051</td>\n",
              "      <td>-0.540751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.782462</td>\n",
              "      <td>1.443861</td>\n",
              "      <td>-0.886983</td>\n",
              "      <td>0.567317</td>\n",
              "      <td>-0.125966</td>\n",
              "      <td>-0.259720</td>\n",
              "      <td>-0.497193</td>\n",
              "      <td>0.811501</td>\n",
              "      <td>0.616308</td>\n",
              "      <td>0.583513</td>\n",
              "      <td>-0.440980</td>\n",
              "      <td>-1.857846</td>\n",
              "      <td>0.596675</td>\n",
              "      <td>-1.422876</td>\n",
              "      <td>-0.314646</td>\n",
              "      <td>-1.718516</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>1.321182</td>\n",
              "      <td>-0.879390</td>\n",
              "      <td>0.832289</td>\n",
              "      <td>-0.794130</td>\n",
              "      <td>-0.437417</td>\n",
              "      <td>0.658336</td>\n",
              "      <td>-1.995509</td>\n",
              "      <td>-0.197276</td>\n",
              "      <td>0.399674</td>\n",
              "      <td>1.666053</td>\n",
              "      <td>-1.372860</td>\n",
              "      <td>1.109184</td>\n",
              "      <td>-0.050761</td>\n",
              "      <td>-1.122224</td>\n",
              "      <td>0.166420</td>\n",
              "      <td>-1.211621</td>\n",
              "      <td>1.321194</td>\n",
              "      <td>-0.222103</td>\n",
              "      <td>0.559735</td>\n",
              "      <td>0.213144</td>\n",
              "      <td>0.973547</td>\n",
              "      <td>0.220861</td>\n",
              "      <td>1.067820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>×</th>\n",
              "      <td>-1.301938</td>\n",
              "      <td>-0.221869</td>\n",
              "      <td>-1.339770</td>\n",
              "      <td>-1.289122</td>\n",
              "      <td>0.351518</td>\n",
              "      <td>1.447174</td>\n",
              "      <td>-0.461569</td>\n",
              "      <td>1.964100</td>\n",
              "      <td>1.508901</td>\n",
              "      <td>-0.434748</td>\n",
              "      <td>-0.422564</td>\n",
              "      <td>-2.072223</td>\n",
              "      <td>0.034195</td>\n",
              "      <td>1.665451</td>\n",
              "      <td>-1.118646</td>\n",
              "      <td>1.426851</td>\n",
              "      <td>0.353533</td>\n",
              "      <td>-0.272413</td>\n",
              "      <td>-1.193303</td>\n",
              "      <td>0.113430</td>\n",
              "      <td>0.445754</td>\n",
              "      <td>-0.089027</td>\n",
              "      <td>-0.560299</td>\n",
              "      <td>-0.760512</td>\n",
              "      <td>-0.287211</td>\n",
              "      <td>0.734672</td>\n",
              "      <td>0.507651</td>\n",
              "      <td>-0.337162</td>\n",
              "      <td>-0.545904</td>\n",
              "      <td>-0.293745</td>\n",
              "      <td>1.277089</td>\n",
              "      <td>-0.379277</td>\n",
              "      <td>-0.620461</td>\n",
              "      <td>0.811449</td>\n",
              "      <td>1.470436</td>\n",
              "      <td>2.191121</td>\n",
              "      <td>-0.658830</td>\n",
              "      <td>0.864191</td>\n",
              "      <td>0.442519</td>\n",
              "      <td>-0.624450</td>\n",
              "      <td>...</td>\n",
              "      <td>0.169492</td>\n",
              "      <td>-0.855915</td>\n",
              "      <td>0.324772</td>\n",
              "      <td>0.530432</td>\n",
              "      <td>0.417808</td>\n",
              "      <td>-0.744394</td>\n",
              "      <td>1.148376</td>\n",
              "      <td>-0.970921</td>\n",
              "      <td>-2.058514</td>\n",
              "      <td>-0.675513</td>\n",
              "      <td>0.300095</td>\n",
              "      <td>-0.581709</td>\n",
              "      <td>0.534720</td>\n",
              "      <td>-1.116369</td>\n",
              "      <td>0.414033</td>\n",
              "      <td>0.418269</td>\n",
              "      <td>-0.735615</td>\n",
              "      <td>0.933798</td>\n",
              "      <td>1.361246</td>\n",
              "      <td>1.237684</td>\n",
              "      <td>-1.152001</td>\n",
              "      <td>0.092433</td>\n",
              "      <td>2.471461</td>\n",
              "      <td>-0.344984</td>\n",
              "      <td>0.411856</td>\n",
              "      <td>1.450843</td>\n",
              "      <td>-0.797478</td>\n",
              "      <td>-1.097178</td>\n",
              "      <td>0.180615</td>\n",
              "      <td>-0.432896</td>\n",
              "      <td>-0.896195</td>\n",
              "      <td>0.612991</td>\n",
              "      <td>-0.588402</td>\n",
              "      <td>-0.310230</td>\n",
              "      <td>1.227507</td>\n",
              "      <td>-0.296647</td>\n",
              "      <td>-1.146949</td>\n",
              "      <td>2.776162</td>\n",
              "      <td>-0.369591</td>\n",
              "      <td>1.293035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;</th>\n",
              "      <td>1.255780</td>\n",
              "      <td>0.268804</td>\n",
              "      <td>0.770104</td>\n",
              "      <td>-0.447349</td>\n",
              "      <td>1.008445</td>\n",
              "      <td>0.181766</td>\n",
              "      <td>-0.048524</td>\n",
              "      <td>-0.251425</td>\n",
              "      <td>-1.733353</td>\n",
              "      <td>0.628797</td>\n",
              "      <td>1.270918</td>\n",
              "      <td>-0.055002</td>\n",
              "      <td>0.012452</td>\n",
              "      <td>-1.005026</td>\n",
              "      <td>0.243341</td>\n",
              "      <td>1.691085</td>\n",
              "      <td>0.231184</td>\n",
              "      <td>-1.200860</td>\n",
              "      <td>0.163073</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.070585</td>\n",
              "      <td>0.143458</td>\n",
              "      <td>-0.414321</td>\n",
              "      <td>-0.128827</td>\n",
              "      <td>-0.877579</td>\n",
              "      <td>0.713582</td>\n",
              "      <td>-0.127578</td>\n",
              "      <td>-0.214490</td>\n",
              "      <td>0.019982</td>\n",
              "      <td>-0.103662</td>\n",
              "      <td>0.297082</td>\n",
              "      <td>-0.294494</td>\n",
              "      <td>0.226625</td>\n",
              "      <td>0.733855</td>\n",
              "      <td>0.824954</td>\n",
              "      <td>-1.148233</td>\n",
              "      <td>-2.012519</td>\n",
              "      <td>1.004830</td>\n",
              "      <td>-0.473306</td>\n",
              "      <td>-0.692251</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285825</td>\n",
              "      <td>1.763377</td>\n",
              "      <td>-0.674509</td>\n",
              "      <td>-0.263090</td>\n",
              "      <td>-0.267436</td>\n",
              "      <td>-1.987921</td>\n",
              "      <td>1.763837</td>\n",
              "      <td>-0.314442</td>\n",
              "      <td>0.642990</td>\n",
              "      <td>0.061467</td>\n",
              "      <td>0.640279</td>\n",
              "      <td>0.222577</td>\n",
              "      <td>-0.519906</td>\n",
              "      <td>-0.452409</td>\n",
              "      <td>-0.101849</td>\n",
              "      <td>0.593100</td>\n",
              "      <td>-0.537599</td>\n",
              "      <td>0.319871</td>\n",
              "      <td>1.779960</td>\n",
              "      <td>0.323549</td>\n",
              "      <td>-0.401870</td>\n",
              "      <td>0.206373</td>\n",
              "      <td>0.635499</td>\n",
              "      <td>0.305064</td>\n",
              "      <td>-0.721673</td>\n",
              "      <td>0.899752</td>\n",
              "      <td>0.832585</td>\n",
              "      <td>0.408215</td>\n",
              "      <td>0.224310</td>\n",
              "      <td>-0.487612</td>\n",
              "      <td>-1.157295</td>\n",
              "      <td>-1.112730</td>\n",
              "      <td>0.006064</td>\n",
              "      <td>0.589938</td>\n",
              "      <td>0.107328</td>\n",
              "      <td>0.287665</td>\n",
              "      <td>-0.362274</td>\n",
              "      <td>-0.441167</td>\n",
              "      <td>-0.584990</td>\n",
              "      <td>0.445047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "`  0.473936 -2.176287 -0.780994  ...  1.110359 -0.564217 -0.118700\n",
              "^  0.289559 -1.868453  1.134751  ... -2.255991 -0.154119 -0.492416\n",
              "~  1.178408 -0.672965 -0.497189  ...  0.973547  0.220861  1.067820\n",
              "× -1.301938 -0.221869 -1.339770  ...  2.776162 -0.369591  1.293035\n",
              "<  1.255780  0.268804  0.770104  ... -0.441167 -0.584990  0.445047\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1f2h8QxMU6m",
        "colab_type": "text"
      },
      "source": [
        "#### VSM utils\n",
        "*Note*: The following code is inspired by the ```Stanford University VSM package```, which is available at the goihub repository https://github.com/cgpotts/cs224u.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJsavMBhKZOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "import scipy.spatial.distance\n",
        "\n",
        "# We here define a function that, given a measure of similarity between words, \n",
        "# and the embedding, computes the \"neighbors\" of an input word\n",
        "def neighbors(word, df, distfunc, df2=None):\n",
        "    \"\"\"Tool for finding the nearest neighbors of `word` in `df` according\n",
        "    to `distfunc`. The comparisons are between row vectors.\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "        The anchor word. Assumed to be in `rownames`.\n",
        "    df : pd.DataFrame\n",
        "        The vector-space model.\n",
        "    distfunc : function mapping vector pairs to floats.\n",
        "        The measure of distance between vectors. Can also be `euclidean`,\n",
        "        `matching`, `jaccard`, as well as any other distance measure\n",
        "        between 1d vectors.\n",
        "    df2 : pd.DataFrame\n",
        "        The context embedding. \n",
        "        If df2 is not None, the distance function will be applied between the\n",
        "        df-embedding and the df2-embedding.\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If word is not in `df.index`.\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Ordered by closeness to `word`.\n",
        "    \"\"\"\n",
        "    if word not in df.index:\n",
        "        raise ValueError('{} is not in this VSM'.format(word))\n",
        "    w = df.loc[word]\n",
        "    if df2 is None:\n",
        "      df2 = df\n",
        "    dists = df2.apply(lambda x: distfunc(w, x), axis=1)\n",
        "    return dists.sort_values()[0:15]\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Here the code for a few measures of similarity \n",
        "def euclidean(u, v):\n",
        "    return scipy.spatial.distance.euclidean(u, v)\n",
        "\n",
        "def cosine(u, v):\n",
        "    return scipy.spatial.distance.cosine(u, v)\n",
        "\n",
        "# Note: the following measures should only be applied to count matrices \n",
        "def matching(u, v):\n",
        "    return np.sum(np.minimum(u, v))\n",
        "\n",
        "def jaccard(u, v):\n",
        "    return 1.0 - (matching(u, v) / np.sum(np.maximum(u, v)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK9fIGLxOdyH",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjkZGThKsf6",
        "colab_type": "code",
        "outputId": "952759c3-1697-4711-b130-ffa80e79c4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "index = np.random.randint(0,X_df.shape[0])\n",
        "word = X_df.index[index]\n",
        "neighbors(word,X_df,cosine)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quotes                  0.000000\n",
              "vinos                   0.557079\n",
              "ripenglish              0.588347\n",
              "overacting              0.602061\n",
              "gomawo                  0.605229\n",
              "#neverforget            0.613627\n",
              "ffsss                   0.619497\n",
              "bx-t                    0.620471\n",
              "ingat                   0.623194\n",
              "flotteste               0.631646\n",
              "camel                   0.633609\n",
              "#greatcomebackthough    0.636938\n",
              "libbys                  0.637234\n",
              "#partyrockingonidol     0.639461\n",
              "i360                    0.641337\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLm9sFkwPDpO",
        "colab_type": "code",
        "outputId": "fc64d7dd-5ba9-4a0d-c099-e082fbcabc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "neighbors(\"quotes\",X_df,cosine,Y_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "duvelkeskermis    0.601453\n",
              "freemas           0.603692\n",
              "hall's            0.607701\n",
              "actively          0.625095\n",
              "signpost          0.627758\n",
              "jared             0.631085\n",
              "mam's             0.631448\n",
              "countri           0.631840\n",
              "anlaby            0.632246\n",
              "(214)             0.634015\n",
              "spinch            0.634625\n",
              "motivation        0.634958\n",
              "inamed            0.635423\n",
              "sxxt              0.643254\n",
              "funko             0.644584\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8c_p2BVapzt",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment classification \n",
        "We'll now train a classification algorithm to predict *positive* or *negative* sentiment by looking at a tweet. <br>\n",
        "Here are the next steps: \n",
        "1. Build a representation of the tweet from the representation of the single words in the tweet \n",
        "2. Build a classifier that takes in a vector representing the tweet and that produces a classification of the input into two categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbTU50QFboIR",
        "colab_type": "text"
      },
      "source": [
        "### Tweet embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO9s-Im5ayYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function that will builds the tweet embedding starting from the \n",
        "# single words embeddings\n",
        "def get_tweet_emb(words, D, words2=None):\n",
        "  \"\"\"\n",
        "  Process a Tweet as a sequence of words and produce a vector representation of the tweet. \n",
        "  In this version of the function we just sum the words.\n",
        "  Parameters: \n",
        "  -----------\n",
        "  - words : list of numpy arrays\n",
        "      The list of vectors containing the embeddings of the words in the tweet. \n",
        "  - D : int\n",
        "      The embedding dimension.\n",
        "  - words2 : list of numpy arrays\n",
        "      The list of vectors containing a second embedding of the words in the tweet. \n",
        "  Returns: \n",
        "  ----------\n",
        "  np.array \n",
        "      A vector representation of the tweet\n",
        "  \"\"\"\n",
        "  tweet = np.zeros(D)\n",
        "  for word in words: \n",
        "    tweet += word \n",
        "  if words2 is not None:\n",
        "    for word in words2: \n",
        "      tweet += word \n",
        "  return tweet\n",
        "\n",
        "# Main function: scans the positive and negative files and builds a matrix that will contain the \n",
        "# representtion of each tweet along with its label (1 for positive, 0 for negative)\n",
        "def build_tweets_emb(files, labels, N, D, embeddingX, \n",
        "                     embeddingY=None, output_name=\"final_emb\"):\n",
        "  \"\"\" \n",
        "  Parameters: \n",
        "  ----------\n",
        "  - files : list(str)\n",
        "      The list of paths to the input files containing the tweets\n",
        "  - labels : list({1,0})\n",
        "      The list of labels to the respective input files (the order matters).\n",
        "      If labels is None, the output matrix will not contain the labels column (to\n",
        "      use in test pipeline).\n",
        "  - N : int \n",
        "      The total number of tweets (#positive tweets + #negative tweets).\n",
        "  - D : int \n",
        "      The embedding dimension.\n",
        "  - embeddingX : pd.DataFrame\n",
        "      The output embedding matrix \n",
        "  - embeddingY: pd.DataFrame\n",
        "      The context embedding matrix. \n",
        "      If it is not None a combination of the two embeddings will produce the \n",
        "      embedding for each word.\n",
        "  - output_location : str\n",
        "      The name of file where the final embedding will be stored.\n",
        "      By default the output will be saved in a .npz file in the \n",
        "      current working directory.\n",
        "  \n",
        "  Returns\n",
        "  --------\n",
        "  np.array \n",
        "      The output matrix containing the tweets embedding + labels if present\n",
        "  \"\"\"\n",
        "  # We initialise the output matrix\n",
        "  if labels is None: out_dim1 = D\n",
        "  else: out_dim1 = D+1\n",
        "  output = np.zeros((N,out_dim1))\n",
        "  \n",
        "  counter = 0\n",
        "\n",
        "  for i, file in enumerate(files):\n",
        "    if labels is not None: label = labels[i]\n",
        "    with open(file) as f:\n",
        "      print(\"Working on \",file)\n",
        "      # look at each tweet\n",
        "      for l, line in enumerate(f):\n",
        "        # Here we filter out the words that are not in the vocabulary \n",
        "        tokens = [t for t in line.strip().split()]\n",
        "        vocab = embeddingX.index\n",
        "        tokens = [t for t in tokens if t in vocab]\n",
        "        # Get the tweet embedding\n",
        "        words = [embeddingX.loc[t].to_numpy() for t in tokens]\n",
        "        words2 = None \n",
        "        if embeddingY is not None: \n",
        "          words2 = [embeddingY.loc[t].to_numpy() for t in tokens]\n",
        "        tweet = get_tweet_emb(words, D, words2).reshape(1,-1) # making sure it is a row vector\n",
        "        # Save the tweet in the output matrix\n",
        "        if labels is None: output[counter,:] = tweet\n",
        "        else: output[counter,:] = np.column_stack((tweet,label))\n",
        "        if l % 10000 == 0:\n",
        "          print(l)\n",
        "        counter +=1\n",
        "            \n",
        "  # Saving the output\n",
        "  np.savez(output_name, output)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOeCdvsi7XSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp 'final_emb.npz' 'drive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIWZkGJcoMW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9a353f8b-5ff1-4bd3-9ac8-2dd1f348d4a3"
      },
      "source": [
        "# get the dimensions\n",
        "!wc -l 'twitter-datasets/train_pos.txt'\n",
        "!wc -l 'twitter-datasets/train_neg.txt'\n",
        "# total dim = 100.000 + 100.000 "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 twitter-datasets/train_pos.txt\n",
            "100000 twitter-datasets/train_neg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBsvX3ghmd0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "c4fdb3f4-9e96-43f5-f2ba-2ee75aafaac4"
      },
      "source": [
        "# Okay now we'll use the cose above to produce the input to the next step\n",
        "tweets_matrix = build_tweets_emb(files=[positive_location,negative_location],labels=[1,0],\n",
        "                                 N=200000, D=EMBEDDING_DIM, embeddingX=X_df)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on  twitter-datasets/train_pos.txt\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "Working on  twitter-datasets/train_neg.txt\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJc84dK4su8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "062662eb-21c4-4f06-8ab8-aa05a5aeb630"
      },
      "source": [
        "tweets_matrix[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.94293148e+00, -5.96480667e+00,  1.56619580e+00, -1.13903590e+00,\n",
              "       -8.08579120e-01,  3.35587898e+00,  8.24847265e-01,  3.26281687e+00,\n",
              "       -3.09925591e+00, -3.32048297e+00, -3.57959889e+00, -6.02754057e+00,\n",
              "        5.73164383e+00, -1.18852600e-02, -5.46592463e-01, -7.19458015e+00,\n",
              "        1.04901091e+00, -1.03213249e+00,  1.55800959e+00,  3.14066298e+00,\n",
              "       -6.35999976e-01,  4.46153206e+00,  1.21392054e+00, -5.60984788e-01,\n",
              "        1.77762892e+00,  1.98305065e+00, -7.55785213e-01, -4.16361119e+00,\n",
              "        3.92223738e+00,  1.65557587e+00, -2.36909513e-01,  8.86969156e+00,\n",
              "        5.18694602e+00,  2.78832038e+00, -3.62039783e+00, -7.33978692e+00,\n",
              "       -1.19092158e+01, -2.35741167e+00,  5.92825464e+00,  2.82874155e+00,\n",
              "        1.45691849e+00, -1.02214718e+00,  1.82172351e+00, -3.82068367e+00,\n",
              "        5.66453663e-01,  1.44222663e+00, -1.28627767e+00, -1.48822140e+00,\n",
              "        2.85913986e+00, -4.44944262e+00, -4.11216076e+00,  1.40830878e+00,\n",
              "       -1.62718003e+00,  8.46338717e+00,  4.53941679e+00, -2.27049509e+00,\n",
              "       -3.73590269e-02,  2.82262294e+00, -2.29081834e+00, -4.98319989e+00,\n",
              "        2.22813132e+00,  5.00737151e+00, -6.64720730e+00,  3.93491535e-01,\n",
              "       -2.41618606e+00,  1.83010755e-02, -2.76664993e+00, -1.44362621e+00,\n",
              "        3.75136054e+00, -3.13450631e+00,  5.50623045e+00,  7.08009318e+00,\n",
              "        7.61066028e+00,  8.86498537e-01, -1.99510248e+00, -3.46807419e+00,\n",
              "       -5.41109629e+00, -9.04708290e-01,  5.50812597e+00,  1.17504003e+00,\n",
              "       -2.23871922e+00, -2.70317293e-04, -2.77677927e-02,  1.52927157e+00,\n",
              "        1.49167156e+00, -2.33164331e+00, -3.81169107e+00,  3.95730137e-01,\n",
              "       -1.40075770e+00,  4.21416808e+00, -1.37524814e-01,  2.54315972e-01,\n",
              "        7.17208327e+00,  8.79898233e-01,  3.42105001e-02, -3.38148095e+00,\n",
              "       -1.31591294e-01, -1.93007543e+00, -2.95353547e-01, -2.33187940e+00,\n",
              "        0.00000000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGn_rZwkbqjg",
        "colab_type": "text"
      },
      "source": [
        "### Classification task\n",
        "Now we build a simple classifier and asses its performance with a validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DRQZ_Vt-xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D = tweets_matrix.shape\n",
        "x_train = tweets_matrix[:,0:-1]\n",
        "y_train = tweets_matrix[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g6Ifojubsqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Out choice of classifier is a vanilla NN \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=D-1, name=\"Dense1\"))\n",
        "model.add(Dense(units=64, activation='relu', input_dim=64, name=\"Dense2\"))\n",
        "model.add(Dense(units=2, activation='softmax',name=\"Dense3\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Sj-LmhvlkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters \n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9 \n",
        "BATCH_SIZE = 32 # We use batches of 32 tweets per update \n",
        "EPOCHS = 5 # Training for 5 epochs at a time\n",
        "VAL_SPLIT = 0.3 # We retain 30% of the tweets for validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slSvLNVJufro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "9e3d9b8f-2104-43ab-a352-f09ee8d57de4"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Dense1 (Dense)               (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "Dense2 (Dense)               (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "Dense3 (Dense)               (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 10,754\n",
            "Trainable params: 10,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxlTeDuE0_C0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SJvRnq_uu_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "b54b5f4e-2b1f-47a4-c23b-7f2e5ae4df94"
      },
      "source": [
        "# Training loop \n",
        "history = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
        "          validation_split=VAL_SPLIT, shuffle=True)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140000 samples, validate on 60000 samples\n",
            "Epoch 1/5\n",
            "140000/140000 [==============================] - 17s 121us/step - loss: 0.4522 - acc: 0.7922 - val_loss: 1.1340 - val_acc: 0.3077\n",
            "Epoch 2/5\n",
            "140000/140000 [==============================] - 17s 118us/step - loss: 0.4469 - acc: 0.7960 - val_loss: 1.0712 - val_acc: 0.3371\n",
            "Epoch 3/5\n",
            "140000/140000 [==============================] - 17s 119us/step - loss: 0.4418 - acc: 0.7983 - val_loss: 1.1469 - val_acc: 0.3245\n",
            "Epoch 4/5\n",
            "140000/140000 [==============================] - 16s 116us/step - loss: 0.4377 - acc: 0.8005 - val_loss: 1.0552 - val_acc: 0.3755\n",
            "Epoch 5/5\n",
            "140000/140000 [==============================] - 17s 118us/step - loss: 0.4339 - acc: 0.8021 - val_loss: 1.0338 - val_acc: 0.3648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuwaINww106K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b660129b-f292-4729-dfad-21a9b4424414"
      },
      "source": [
        "# Let's plot because plots make us happy \n",
        "# We should have saved the training data in the history callback\n",
        "print(history.history.keys())"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2wTDQb319ij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "68642a26-a162-453e-aca5-657b534048b1"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngkBEJOEUBCIiCacgEBFJuP8GOOkqEKkfZ8YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFnCQejs2W1e",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the plots above something is reeeeally wrond with this data. Either we messed up somewhere or the embeddings we learned suck (could be both actually). In order to learn more about this we could make some analysis on the dataset we provide. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAdRH4E2yKo8",
        "colab_type": "text"
      },
      "source": [
        "### Prediction pipeline \n",
        "In this section we build the submission file. The steps are the following: \n",
        "1. Pre-process the test data (in the same way the train data was pre-processed!) \n",
        "2. Build a vectorised representation of each tweet in the test data using the embedding previously trained and the combining techniques used for the train dataset\n",
        "3. Pass the test data through the trained model to get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FSDVxbvyPVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing\n",
        "# Note: there's no pre-processing to do for now (because we haven't done any before)\n",
        "# but there is a difference between the test and the train files, namely: the test tweets are \n",
        "# numbered. \n",
        "# We will assume that the numeration follows the sequence of natural numbers and we will get \n",
        "# rid of this numbers in the preprocessing step.\n",
        "!cat 'twitter-datasets/test_data.txt'|sed \"s/^[[:digit:]]*,//g\" | grep -v \"^\\s*$\" > test_data_new.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0AOMoUP4VBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4cd03035-7827-4870-bef6-bc6d88d4d503"
      },
      "source": [
        "!wc -l 'test_data_new.txt'"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 test_data_new.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWt0a5hq4fRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cb8eb406-f2ce-4afe-c2c6-d553ef8afab1"
      },
      "source": [
        "# Okay now we'll produce the test input to the classifier\n",
        "test_matrix = build_tweets_emb(files=['test_data_new.txt'],labels=None, N=10000, D=EMBEDDING_DIM, embeddingX=X_df)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on  test_data_new.txt\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chlTc1AQ4xkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb14c891-eec7-4aac-a60f-cc3ac9bc7038"
      },
      "source": [
        "test_matrix.shape"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ir0XtP4_FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = test_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEHsIxTf45_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's build this predictions! \n",
        "classes = model.predict(x_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuq6unHX5BvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd76cd97-810f-4f69-f17b-7842e2b9d931"
      },
      "source": [
        "classes.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5YZZq4u5b6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0b0b598-9a2f-4f97-bc4d-2631400a7337"
      },
      "source": [
        "predictions = 1- np.argmax(classes,axis=1)\n",
        "predictions"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NYSOzKg6kSJ",
        "colab_type": "text"
      },
      "source": [
        "#####  A quick look at the result\n",
        "Our prediction for the first line is **1**, meaning that our classifier believes it has positive sentiment. <br>\n",
        "Out prediction for the second line is **0**, meaning that our classifier believes it has negative sentiment. <br>\n",
        "The first two lines are the following: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTL6D3nk6b8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "753465f2-40e2-46a9-b246-804922e3db11"
      },
      "source": [
        "!cat 'twitter-datasets/test_data.txt'| head -2"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n",
            "2,<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVvCEnyl7Hvz",
        "colab_type": "text"
      },
      "source": [
        "Doesn't seem that bad ... <br>\n",
        "But let's continue building the submission file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5PdXKMo5QPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "40ba4f2f-e8c6-408c-85e1-6be5bd9d2ce7"
      },
      "source": [
        "# Build submission matrix \n",
        "submission = pd.DataFrame(predictions, index=range(1,len(predictions)+1),columns=[\"Prediction\"])\n",
        "submission.head()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Prediction\n",
              "1           1\n",
              "2           0\n",
              "3           0\n",
              "4           0\n",
              "5           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHJAJH56E1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the submission is a .csv file\n",
        "submission.to_csv(\"sumbission.csv\",index_label=\"Id\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp-XeBQqRupt",
        "colab_type": "text"
      },
      "source": [
        "#### #*TODO*: \n",
        "- Improve preprocessing (hashtags, punctuation, ...) \n",
        "- Improve classifier and model assessment\n",
        "- Analyse the training data (using a simple linear model, which words would have more predictive power? ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn6aUdihwnMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}